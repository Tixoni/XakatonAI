[Файл: c:\Users\kolyb\Identification\XakatonAI\Copy_Files_POSSIBLE_DANGER_RUN_ONLY_HERE.py]
[Размер: 5202 байт]
[Дата изменения: 2025-11-29 16:51:31.190897]

import os
import sys
import glob
from pathlib import Path
from datetime import datetime

def main():
    # Установка корневой директории
    if len(sys.argv) > 1:
        root_dir = sys.argv[1]
    else:
        root_dir = os.path.dirname(os.path.abspath(__file__))
    
    # Проверка существования директории
    if not os.path.exists(root_dir):
        print(f'Ошибка: Директория "{root_dir}" не существует!')
        input('Нажмите Enter для выхода...')
        return 1
    
    # Получение абсолютного пути
    root_dir = os.path.abspath(root_dir)
    
    print(f'Рабочая директория: {root_dir}')
    print()
    
    # Удалить существующий файл результатов
    if os.path.exists('Project_Files.txt'):
        os.remove('Project_Files.txt')
    
    # Список разрешенных расширений для исходного кода
    allowed_extensions = [
        '*.py'
    ]

    
    # Исключаемые директории
    exclude_dirs = ['.git', 'datasets', '__pycache__']
    
    # Максимальный размер файла (1 МБ)
    max_size = 1048576
    
    print('Начало обработки файлов...')
    print()
    
    processed_files = 0
    skipped_files = 0
    
    # Рекурсивный поиск файлов
    for extension in allowed_extensions:
        pattern = os.path.join(root_dir, '**', extension)
        
        for file_path in glob.glob(pattern, recursive=True):
            # Проверка на исключаемые директории
            skip_file = False
            for exclude_dir in exclude_dirs:
                if exclude_dir in file_path.split(os.sep):
                    skip_file = True
                    break
            
            if skip_file:
                continue
            
            # Получение информации о файле
            try:
                file_size = os.path.getsize(file_path)
                mod_time = datetime.fromtimestamp(os.path.getmtime(file_path))
                
                # Проверка размера файла
                if file_size < max_size:
                    print(f'Обработка: {file_path}')
                    
                    with open('Project_Files.txt', 'a', encoding='utf-8') as result_file:
                        result_file.write(f'[Файл: {file_path}]\n')
                        result_file.write(f'[Размер: {file_size} байт]\n')
                        result_file.write(f'[Дата изменения: {mod_time}]\n')
                        result_file.write('\n')
                        
                        # Попытка чтения файла
                        try:
                            with open(file_path, 'r', encoding='utf-8') as src_file:
                                content = src_file.read()
                                result_file.write(content)
                        except UnicodeDecodeError:
                            # Попробуем другие кодировки
                            try:
                                with open(file_path, 'r', encoding='cp1251') as src_file:
                                    content = src_file.read()
                                    result_file.write(content)
                            except:
                                result_file.write('[Ошибка чтения файла - возможно бинарный файл]\n')
                        except Exception as e:
                            result_file.write(f'[Ошибка чтения файла: {str(e)}]\n')
                        
                        result_file.write('\n')
                        result_file.write('-----\n')
                        result_file.write('\n')
                    
                    processed_files += 1
                else:
                    print(f'Пропуск большого файла: {file_path} ({file_size} байт)')
                    
                    with open('Project_Files.txt', 'a', encoding='utf-8') as result_file:
                        result_file.write(f'[Файл: {file_path} - ПРОПУЩЕН (слишком большой: {file_size} байт)]\n')
                    
                    skipped_files += 1
                    
            except Exception as e:
                print(f'Ошибка при обработке файла {file_path}: {e}')
    
    print()

    print(f'Готово! Результат сохранен в Project_Files.txt')
    print(f'Обработано файлов: {processed_files}, пропущено: {skipped_files}')
    print(f'Обработана директория: {root_dir}')
    
    # Пауза в конце
    input('Нажмите Enter для выхода...')
    return 0

if __name__ == '__main__':
    sys.exit(main())

-----

[Файл: c:\Users\kolyb\Identification\XakatonAI\main.py]
[Размер: 2548 байт]
[Дата изменения: 2025-11-29 15:51:38.837323]

import sys
import os
import warnings
from pathlib import Path

# Подавляем предупреждение о pin_memory при использовании CPU
warnings.filterwarnings("ignore", message=".*pin_memory.*", category=UserWarning)

# Импортируем наши модули
from src.config_loader import load_config
from src.detector import YOLODetector
from src.processors import process_image, process_video
from src.attribute_models import AttributeModels

def main():
    print("=" * 60)
    print("Детекция людей и поездов (YOLO11) - Modular Version")
    print("=" * 60)
    
    # 1. Загрузка конфига
    config = load_config("config.json")
    
    # 2. Проверка входного файла
    input_file = config.get("input_file", "")
    if not input_file:
        print("Ошибка: не указан input_file в конфиге")
        sys.exit(1)
        
    input_file = os.path.normpath(input_file.strip('"\'' ))
    if not os.path.exists(input_file):
        print(f"Файл не найден: {input_file}")
        sys.exit(1)

    # 3. Инициализация детектора
    yolo_cfg = config.get("yolo", {})
    detector = YOLODetector(
        model_path=yolo_cfg.get("model", "yolo11m.pt"),
        conf_threshold=config.get("detection", {}).get("confidence_threshold", 0.5),
        device=yolo_cfg.get("device", "cpu"),
        custom_colors=config.get("colors", {}),
        half_precision=config.get("processing", {}).get("half_precision", False),
        ppe_model=None,
        clothing_model=None
    )

    # 4. AttributeModels (ppe/clothes)
    attr_cfg = config.get("attributes", {})
    attribute_models = AttributeModels(
        ppe_model_path=attr_cfg.get("ppe_model"),
        clothes_model_path=attr_cfg.get("clothes_model"),
        device=attr_cfg.get("device", yolo_cfg.get("device", "cpu")),
        conf=attr_cfg.get("confidence", 0.25)
    )

    # 5. Запуск обработки
    ext = Path(input_file).suffix.lower()
    image_exts = {'.jpg', '.jpeg', '.png', '.bmp', '.webp'}
    video_exts = {'.mp4', '.avi', '.mov', '.mkv'}

    if ext in image_exts:
        process_image(input_file, detector, attribute_models, config)
    elif ext in video_exts:
        process_video(input_file, detector, attribute_models, config)
    else:
        print(f"Неизвестный формат файла: {ext}")

if __name__ == "__main__":
    main()

-----

[Файл: c:\Users\kolyb\Identification\XakatonAI\src\attribute_models.py]
[Размер: 1194 байт]
[Дата изменения: 2025-11-29 09:44:37.940652]

# src/attribute_models.py
from ultralytics import YOLO
import cv2
import numpy as np

class AttributeModels:
    def __init__(self, ppe_model_path=None, clothes_model_path=None, device="cpu", conf=0.25):
        self.device = device
        self.conf = conf

        self.ppe = YOLO(ppe_model_path) if ppe_model_path else None
        self.clothes = YOLO(clothes_model_path) if clothes_model_path else None

    def _infer(self, model, crop):
        if model is None:
            return []
        # ultralytics.predict -> results[0].boxes
        results = model(crop, device=self.device, conf=self.conf, verbose=False)
        if len(results) == 0:
            return []
        res = results[0]
        out = []
        for box in res.boxes:
            cls = int(box.cls[0].cpu().numpy()) if hasattr(box.cls[0], 'cpu') else int(box.cls[0])
            conf = float(box.conf[0].cpu().numpy()) if hasattr(box.conf[0], 'cpu') else float(box.conf[0])
            out.append((cls, conf))
        return out

    def run_ppe(self, crop):
        return self._infer(self.ppe, crop)

    def run_clothes(self, crop):
        return self._infer(self.clothes, crop)

-----

[Файл: c:\Users\kolyb\Identification\XakatonAI\src\config_loader.py]
[Размер: 1226 байт]
[Дата изменения: 2025-11-29 09:41:57.478675]

"""
Модуль для загрузки конфигурации
"""

import os
import sys
import json


def load_config(config_path="config.json"):
    """Загрузка конфигурации"""
    # Получаем абсолютный путь к конфигу
    if not os.path.isabs(config_path):
        script_dir = os.path.dirname(os.path.abspath(__file__))
        # Поднимаемся на уровень выше (из src в корень)
        root_dir = os.path.dirname(script_dir)
        config_path = os.path.join(root_dir, config_path)
    
    if not os.path.exists(config_path):
        print(f"Ошибка: файл config.json не найден!")
        print(f"Ожидался путь: {config_path}")
        sys.exit(1)
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except json.JSONDecodeError as e:
        print(f"Ошибка при парсинге config.json: {e}")
        print(f"Проверьте синтаксис JSON файла")
        sys.exit(1)
    except Exception as e:
        print(f"Ошибка при загрузке config.json: {e}")
        sys.exit(1)

-----

[Файл: c:\Users\kolyb\Identification\XakatonAI\src\detector.py]
[Размер: 11414 байт]
[Дата изменения: 2025-11-29 15:53:16.023489]

import cv2
import sys
from ultralytics import YOLO


class YOLODetector:
    """Класс для детекции объектов с использованием YOLO11"""
    
    CLASSES = {
        0: "person",
        6: "train"
    }
    
    DEFAULT_COLORS = {
        0: (0, 255, 0),  # Зеленый
        6: (255, 0, 0),  # Синий
    }
    
    def __init__(self, model_path="yolo11m.pt", conf_threshold=0.5, device="cpu",
                 custom_colors=None, half_precision=False,
                 ppe_model=None, clothing_model=None):         
        """
        Расширено: добавлены ppe_model и clothing_model
        """
        self.conf_threshold = conf_threshold
        self.device = device
        self.half_precision = half_precision
        
        print(f"Загрузка модели YOLO11: {model_path}")
        
        try:
            self.model = YOLO(model_path)
            print(f"Модель загружена успешно!")
        except Exception as e:
            print(f"Ошибка при загрузке модели: {e}")
            sys.exit(1)

        self.ppe_model = YOLO(ppe_model) if ppe_model else None
        self.clothing_model = YOLO(clothing_model) if clothing_model else None
        print(f"PPE модель: {'OK' if self.ppe_model else 'нет'}")
        print(f"Clothing модель: {'OK' if self.clothing_model else 'нет'}")
        
        self.colors = dict(self.DEFAULT_COLORS)
        if custom_colors:
            self._apply_custom_colors(custom_colors)

        self.colors = dict(self.DEFAULT_COLORS)
        if custom_colors:
            self._apply_custom_colors(custom_colors)
    
    def _apply_custom_colors(self, colors_config):
        for key, value in colors_config.items():
            class_id = self._resolve_class_id(key)
            color_tuple = self._parse_color(value)
            
            if class_id is None:
                print(f"Предупреждение: неизвестный класс '{key}' в настройках цветов. Пропущено.")
                continue
            if color_tuple is None:
                print(f"Предупреждение: некорректный цвет для '{key}'. Ожидался формат [B,G,R]. Пропущено.")
                continue
            
            self.colors[class_id] = color_tuple
    
    @classmethod
    def _resolve_class_id(cls, key):
        if isinstance(key, int):
            return key if key in cls.CLASSES else None
        if isinstance(key, str):
            key = key.strip().lower()
            if key.isdigit():
                cid = int(key)
                return cid if cid in cls.CLASSES else None
            for cid, name in cls.CLASSES.items():
                if name.lower() == key:
                    return cid
        return None
    
    @staticmethod
    def _parse_color(value):
        if isinstance(value, (list, tuple)) and len(value) == 3:
            try:
                b, g, r = [int(max(0, min(255, v))) for v in value]
                return (b, g, r)
            except (ValueError, TypeError):
                return None
        return None
    
    def detect(self, frame, target_classes=None):
        results = self.model(
            frame,
            conf=self.conf_threshold,
            device=self.device,
            verbose=False,
            half=self.half_precision
        )
        
        detections = []
        if len(results) > 0:
            boxes = results[0].boxes
            for box in boxes:
                class_id = int(box.cls[0])
                confidence = float(box.conf[0])
                
                if target_classes is None or class_id in target_classes:
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    detections.append((class_id, confidence, int(x1), int(y1), int(x2), int(y2)))
        
        return detections
    
    def _run_attribute_model(self, model, frame):
        """
        Выполняет детекцию ppe/clothes и возвращает:
        [("helmet", 0.92, x1, y1, x2, y2), ...]
        """
        results = model(frame, conf=0.4, verbose=False)
        output = []

        if len(results) == 0:
            return output

        for box in results[0].boxes:
            cid = int(box.cls[0])
            conf = float(box.conf[0])
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
            cls_name = results[0].names[cid]

            output.append((cls_name, conf, int(x1), int(y1), int(x2), int(y2)))

        return output
    

    def draw_detections(self, frame, detections, show_track_ids=False, train_numbers=None, per_detection_attrs=None):
        """
        Отрисовка детекций на кадре.

        Принимает:
            frame: изображение
            detections:
                - без трекинга: [(class_id, conf, x1, y1, x2, y2), ...]
                - с трекингом: [(track_id, class_id, conf, x1, y1, x2, y2), ...]
            show_track_ids: если True, ожидаем формат с track_id
            train_numbers: {track_id: train_number}
            per_detection_attrs: словарь для не-трекового варианта {det_index: ['helmet','jacket', ...]}
        """
        result_frame = frame.copy()
        if detections is None:
            return result_frame

        for idx, det in enumerate(detections):
            # Определяем формат
            if show_track_ids and len(det) == 7:
                track_id, class_id, conf, x1, y1, x2, y2 = det
            elif len(det) == 6:
                class_id, conf, x1, y1, x2, y2 = det
                track_id = None
            else:
                # Неподдерживаемый формат — пропускаем
                continue

            color = self.colors.get(class_id, (0, 0, 255))
            cv2.rectangle(result_frame, (x1, y1), (x2, y2), color, 2)

            class_name = self.CLASSES.get(class_id, 'unknown')

            # Формируем подпись
            label = None
            # Если это трек — пробуем взять атрибуты из tracker
            if show_track_ids and track_id is not None:
                label_parts = [f"ID:{track_id}", f"type:{class_name}"]
                # При помощи detector.tracker можем читать атрибуты, если они есть
                tmanager = getattr(self, "tracker", None)
                if tmanager:
                    track_obj = tmanager.tracks.get(track_id)
                    if track_obj and hasattr(track_obj, "attributes"):
                        ppe_list = track_obj.attributes.get("ppe", []) if isinstance(track_obj.attributes, dict) else []
                        clothes_list = track_obj.attributes.get("clothes", []) if isinstance(track_obj.attributes, dict) else []
                        items = []
                        if ppe_list:
                            items += ppe_list
                        if clothes_list:
                            items += clothes_list
                        if items:
                            label_parts.append("items:" + ",".join(items))
                # train numbers
                if train_numbers and track_id in train_numbers:
                    label_parts.append(f"train:{train_numbers[track_id]}")
                else:
                    label_parts.append(f"{conf:.2f}")
                label = " | ".join(label_parts)
            else:
                # non-track drawing: можно подставить per_detection_attrs
                label = f"{class_name}: {conf:.2f}"
                if per_detection_attrs and idx in per_detection_attrs:
                    items = per_detection_attrs[idx]
                    if items:
                        label = f"ID:- type:{class_name} items:{','.join(items)}"

            # Размещение подписи с фоном
            label_size, baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.45, 2)
            label_height = label_size[1] + baseline
            label_y = y1 - 6
            if label_y < label_height:
                label_y = y1 + label_height + 6

            bg_y1 = label_y - label_height
            bg_y2 = label_y
            cv2.rectangle(result_frame, (x1, bg_y1), (x1 + label_size[0], bg_y2), color, -1)
            cv2.putText(result_frame, label, (x1, label_y - baseline),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255, 255, 255), 2)

        return result_frame


    def _draw_attribute_dets(self, frame, dets, color):
        """
        dets: [("helmet", 0.91, x1, y1, x2, y2)]
        """
        for cname, conf, x1, y1, x2, y2 in dets:
            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
            cv2.putText(frame, f"{cname} {conf:.2f}", (x1, y1 - 4),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
        return frame
    
    def draw_objects(self, frame, objects_info):

        result_frame = frame.copy()
        
        for obj_info in objects_info:
            track_id = obj_info.get('track_id')
            object_id = obj_info.get('object_id')
            object_type = obj_info.get('object_type', 'unknown')
            class_id = obj_info.get('class_id', 0)
            bbox = obj_info.get('bbox')
            status = obj_info.get('status', 'unknown')
            train_number = obj_info.get('train_number')
            frame_count = obj_info.get('frame_count', 0)
            
            if not bbox:
                continue
            
            x1, y1, x2, y2 = bbox
            color = self.colors.get(class_id, (0, 0, 255))
            
            # Рисуем прямоугольник
            cv2.rectangle(result_frame, (x1, y1), (x2, y2), color, 2)
            
            # Формируем подпись: только тип, ID и статус
            label_parts = [f"{object_type.upper()}#{object_id}"]
            
            # Добавляем статус
            if status != 'unknown':
                label_parts.append(f"[{status}]")
            
            label = " ".join(label_parts)
            
            label_size, baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)
            label_height = label_size[1] + baseline
            
            # Размещаем текст сверху рамки
            label_y = y1 - 5
            if label_y < label_height:
                label_y = y1 + label_height + 5
            
            # Фон для текста
            bg_y1 = label_y - label_height
            bg_y2 = label_y
            cv2.rectangle(result_frame, (x1, bg_y1),
                         (x1 + label_size[0], bg_y2), color, -1)
            
            # Текст
            cv2.putText(result_frame, label, (x1, label_y - baseline),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
        
        return result_frame
-----

[Файл: c:\Users\kolyb\Identification\XakatonAI\src\filters.py]
[Размер: 22246 байт]
[Дата изменения: 2025-11-29 13:43:05.412517]

"""
Модуль для цветовых фильтров детекций
"""

import cv2
import numpy as np
from src.image_utils import extract_roi


def parse_color_range(range_values):
    """Подготовка нижних и верхних границ цвета"""
    if not isinstance(range_values, (list, tuple)) or len(range_values) != 3:
        return None
    try:
        return np.array([int(max(0, min(255, v))) for v in range_values], dtype=np.uint8)
    except (ValueError, TypeError):
        return None


def resolve_filter_class_id(class_key, class_map):
    """Получение ID класса по имени или числу"""
    if isinstance(class_key, int):
        return class_key if class_key in class_map else None
    if isinstance(class_key, str):
        key = class_key.strip().lower()
        if key.isdigit():
            cid = int(key)
            return cid if cid in class_map else None
        for cid, name in class_map.items():
            if name.lower() == key:
                return cid
    return None


def passes_color_filter(roi, cfg):
    """Проверяет, удовлетворяет ли ROI заданным цветовым фильтрам"""
    total_pixels = roi.shape[0] * roi.shape[1]
    if total_pixels == 0:
        return False, {"reason": "empty_roi"}
    
    info = {}
    positive_mask = None
    
    # RGB диапазон
    if "min_rgb" in cfg and "max_rgb" in cfg:
        lower_rgb = parse_color_range(cfg["min_rgb"])
        upper_rgb = parse_color_range(cfg["max_rgb"])
        if lower_rgb is not None and upper_rgb is not None:
            rgb_mask = cv2.inRange(roi, lower_rgb, upper_rgb)
            positive_mask = rgb_mask if positive_mask is None else cv2.bitwise_and(positive_mask, rgb_mask)
    
    # HSV диапазон
    if "min_hsv" in cfg and "max_hsv" in cfg:
        lower_hsv = parse_color_range(cfg["min_hsv"])
        upper_hsv = parse_color_range(cfg["max_hsv"])
        if lower_hsv is not None and upper_hsv is not None:
            hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
            hsv_mask = cv2.inRange(hsv_roi, lower_hsv, upper_hsv)
            positive_mask = hsv_mask if positive_mask is None else cv2.bitwise_and(positive_mask, hsv_mask)
    
    positive_threshold = float(cfg.get("match_threshold", 0.1))
    positive_threshold = max(0.0, min(1.0, positive_threshold))
    
    if positive_mask is not None:
        match_ratio = cv2.countNonZero(positive_mask) / total_pixels
        info["match_ratio"] = match_ratio
        info["match_threshold"] = positive_threshold
        if match_ratio < positive_threshold:
            info["reason"] = "positive_miss"
            return False, info
    
    # Anti-color фильтр в HSV
    anti_threshold = float(cfg.get("anti_match_threshold", cfg.get("match_threshold", 0.1)))
    anti_threshold = max(0.0, min(1.0, anti_threshold))
    info["anti_threshold"] = anti_threshold
    
    anti_ratio = None
    if "anti_color_hsv" in cfg:
        center = parse_color_range(cfg["anti_color_hsv"])
        if center is not None:
            range_val = int(cfg.get("anti_color_range", 20))
            hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
            lower = np.array([
                max(0, center[0] - range_val),
                max(0, center[1] - range_val),
                max(0, center[2] - range_val)
            ], dtype=np.uint8)
            upper = np.array([
                min(180, center[0] + range_val),
                min(255, center[1] + range_val),
                min(255, center[2] + range_val)
            ], dtype=np.uint8)
            anti_mask = cv2.inRange(hsv_roi, lower, upper)
            anti_ratio = cv2.countNonZero(anti_mask) / total_pixels
    
    if "anti_color_rgb" in cfg and anti_ratio is None:
        center = parse_color_range(cfg["anti_color_rgb"])
        if center is not None:
            range_val = int(cfg.get("anti_color_range", 20))
            lower = np.array([
                max(0, center[0] - range_val),
                max(0, center[1] - range_val),
                max(0, center[2] - range_val)
            ], dtype=np.uint8)
            upper = np.array([
                min(255, center[0] + range_val),
min(255, center[1] + range_val),
                min(255, center[2] + range_val)
            ], dtype=np.uint8)
            anti_mask = cv2.inRange(roi, lower, upper)
            anti_ratio = cv2.countNonZero(anti_mask) / total_pixels
    
    if anti_ratio is not None:
        info["anti_ratio"] = anti_ratio
        if anti_ratio >= anti_threshold:
            info["reason"] = "anti_match"
            return False, info
    
    info["reason"] = "pass"
    return True, info


def apply_color_filters(frame, detections, filters_cfg, class_map, debug_cfg=None):
    """
    Применяет цветовые фильтры и возвращает отфильтрованные и отклонённые детекции
    """
    if not filters_cfg or not filters_cfg.get("enabled", False):
        return detections, []
    
    filtered = []
    rejected = []
    log_details = bool(debug_cfg.get("log_detection_details")) if debug_cfg else False
    
    for det in detections:
        class_id, conf, x1, y1, x2, y2 = det
        class_name = class_map.get(class_id, str(class_id))
        filter_cfg = None
        
        # Поиск конфигурации фильтра по имени или ID
        for key, cfg in filters_cfg.items():
            if key == "enabled":
                continue
            cid = resolve_filter_class_id(key, class_map)
            if cid is not None and cid == class_id:
                filter_cfg = cfg
                break
        
        if not filter_cfg:
            filtered.append(det)
            continue
        
        roi = extract_roi(frame, x1, y1, x2, y2)
        if roi is None:
            rejected.append({
                "det": det,
                "info": {"reason": "empty_roi"}
            })
            continue
        
        passed, info = passes_color_filter(roi, filter_cfg)
        
        # Дополнительная проверка: для поездов требуется наличие красного цвета
        if passed and class_name == "train" and filter_cfg.get("require_red_color", False):
            # Используем настройки lighting_compensation из config, если они есть
            lighting_compensation = None
            if debug_cfg:
                # Получаем настройки из config через debug_cfg, если они переданы
                # Или используем стандартные настройки для лучшего определения красного
                lighting_compensation = {"enabled": True, "normalize_brightness": False, "wider_color_ranges": True}
            
            color_info = detect_dominant_color(roi, top_n=3, lighting_compensation=lighting_compensation)
            all_percentages = color_info.get("all_percentages", {})
            top_colors = color_info.get("top_colors", [])
            
            # Проверяем наличие красного цвета (red и red2 объединяются в red)
            red_percentage = all_percentages.get("red", 0.0)
            # Также проверяем в top_colors
            for color_item in top_colors:
                if color_item.get("name") == "red":
                    red_percentage = max(red_percentage, color_item.get("percentage", 0.0))
            
            min_red_threshold = filter_cfg.get("min_red_threshold", 0.1)
            if red_percentage < min_red_threshold:
                passed = False
                info["reason"] = "no_red_color"
                info["red_percentage"] = red_percentage
                info["min_red_threshold"] = min_red_threshold
        
        if log_details:
            reason = info.get("reason", "pass")
            match_ratio = info.get("match_ratio")
            anti_ratio = info.get("anti_ratio")
            details = []
            if match_ratio is not None:
                details.append(f"match={match_ratio:.2f}/{info.get('match_threshold', '-')}")
            if anti_ratio is not None:
                details.append(f"anti={anti_ratio:.2f}/{info.get('anti_threshold', '-')}")
            detail_text = " | ".join(details)
            status = "PASS" if passed else f"FILTERED({reason})"
            print(f"[DEBUG] {class_name} ({conf:.2f}) -> {status} {detail_text}")
        
        if passed:
            filtered.append(det)
        else:
            rejected.append({
                "det": det,
                "info": info
            })
    
    return filtered, rejected


def detect_dominant_color(roi, top_n=2, lighting_compensation=None):
    """
    Определяет несколько преобладающих цветов объекта
    
    Args:
        roi: Область интереса (numpy array BGR)
        top_n: Количество цветов для возврата (по умолчанию 2)
    
    Returns:
        dict: {
            "top_colors": [  # Список топ цветов, отсортированных по проценту
                {"name": "red", "percentage": 0.45},
                {"name": "blue", "percentage": 0.30}
            ],
            "color_name": "red",  # Основной цвет (для обратной совместимости)
            "color_percentage": 0.45,  # Процент основного цвета
            "bgr_avg": [100, 50, 200]  # Средний BGR цвет
        }
    """
    if roi is None or roi.size == 0:
        return {"color_name": "unknown", "color_percentage": 0.0}
    
    total_pixels = roi.shape[0] * roi.shape[1]
    if total_pixels == 0:
        return {"color_name": "unknown", "color_percentage": 0.0}
    
    # Преобразуем в HSV для более точного определения цвета
    hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
    
    # Компенсация освещения: нормализация яркости
    if lighting_compensation and lighting_compensation.get("normalize_brightness", False):
        # Вычисляем среднюю яркость
        v_channel = hsv_roi[:, :, 2].astype(np.float32)
        avg_v = np.mean(v_channel)
        
        # Нормализуем яркость к среднему значению (128)
        if avg_v > 0:
            scale_factor = 128.0 / avg_v
            v_channel_normalized = np.clip(v_channel * scale_factor, 0, 255).astype(np.uint8)
            hsv_roi = hsv_roi.copy()
            hsv_roi[:, :, 2] = v_channel_normalized
    
    # Определяем основные цвета в HSV
    # Используем более широкие диапазоны для устойчивости к освещению
    wider_ranges = lighting_compensation and lighting_compensation.get("wider_color_ranges", False)
    
    if wider_ranges:
        # Более широкие диапазоны, но с минимальной насыщенностью выше порога серых
        # чтобы гарантировать, что серые не попадут в цветные диапазоны
        sat_min = 45  # Выше порога серых (40), но ниже стандартного (50)
        val_min = 30  # Вместо 50
        color_ranges = [
            ("red", ([0, sat_min, val_min], [10, 255, 255])),  # Красный (0-10)
            ("red2", ([170, sat_min, val_min], [180, 255, 255])),  # Красный (170-180)
            ("orange", ([11, sat_min, val_min], [25, 255, 255])),  # Оранжевый
            ("yellow", ([26, sat_min, val_min], [35, 255, 255])),  # Желтый
            ("green", ([36, sat_min, val_min], [85, 255, 255])),  # Зеленый
            ("cyan", ([86, sat_min, val_min], [100, 255, 255])),  # Голубой
            ("blue", ([101, sat_min, val_min], [130, 255, 255])),  # Синий
            ("purple", ([131, sat_min, val_min], [169, 255, 255])),  # Фиолетовый
        ]
    else:
        # Стандартные диапазоны (насыщенность 50, что выше порога серых 40)
        color_ranges = [
            ("red", ([0, 50, 50], [10, 255, 255])),  # Красный (0-10)
            ("red2", ([170, 50, 50], [180, 255, 255])),  # Красный (170-180)
            ("orange", ([11, 50, 50], [25, 255, 255])),  # Оранжевый
            ("yellow", ([26, 50, 50], [35, 255, 255])),  # Желтый
            ("green", ([36, 50, 50], [85, 255, 255])),  # Зеленый
            ("cyan", ([86, 50, 50], [100, 255, 255])),  # Голубой
            ("blue", ([101, 50, 50], [130, 255, 255])),  # Синий
            ("purple", ([131, 50, 50], [169, 255, 255])),  # Фиолетовый
        ]
    
    # Также проверяем яркость для белого/черного/серого
    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
    avg_brightness = np.mean(gray)
    
    color_percentages = {}
    
    # СНАЧАЛА определяем оттенки серого, чтобы исключить их из цветных диапазонов
    # Используем более строгий порог насыщенности для серых (40 вместо 30)
    # чтобы гарантировать, что серые пиксели не попадут в цветные диапазоны
    gray_saturation_threshold = 40
# Маска для всех серых оттенков (низкая насыщенность)
    gray_mask_all = hsv_roi[:, :, 1] < gray_saturation_threshold
    
    # Определяем оттенки серого как отдельные категории
    # Светло-серый (высокая яркость, низкая насыщенность)
    light_gray_mask = gray_mask_all & (hsv_roi[:, :, 2] > 180) & (hsv_roi[:, :, 2] <= 240)
    light_gray_pixels = np.sum(light_gray_mask)
    if light_gray_pixels / total_pixels > 0.05:
        color_percentages["light_gray"] = light_gray_pixels / total_pixels
    
    # Темно-серый (низкая яркость, низкая насыщенность)
    dark_gray_mask = gray_mask_all & (hsv_roi[:, :, 2] >= 50) & (hsv_roi[:, :, 2] < 120)
    dark_gray_pixels = np.sum(dark_gray_mask)
    if dark_gray_pixels / total_pixels > 0.05:
        color_percentages["dark_gray"] = dark_gray_pixels / total_pixels
    
    # Средний серый
    gray_mask = gray_mask_all & (hsv_roi[:, :, 2] >= 120) & (hsv_roi[:, :, 2] <= 180)
    gray_pixels = np.sum(gray_mask)
    if gray_pixels / total_pixels > 0.05:
        color_percentages["gray"] = gray_pixels / total_pixels
    
    # Белый (очень высокая яркость, очень низкая насыщенность)
    white_mask = (hsv_roi[:, :, 1] < 20) & (hsv_roi[:, :, 2] > 240)
    white_pixels = np.sum(white_mask)
    if white_pixels / total_pixels > 0.05:
        color_percentages["white"] = white_pixels / total_pixels
    
    # Черный (очень низкая яркость, очень низкая насыщенность)
    black_mask = (hsv_roi[:, :, 1] < 20) & (hsv_roi[:, :, 2] < 50)
    black_pixels = np.sum(black_mask)
    if black_pixels / total_pixels > 0.05:
        color_percentages["black"] = black_pixels / total_pixels
    
    # Объединяем все серые маски для исключения из цветных диапазонов
    # Включаем ВСЕ пиксели с низкой насыщенностью, даже если они не попали в конкретные категории
    all_gray_mask = gray_mask_all | white_mask | black_mask
    
    # Проверяем каждый цвет, ИСКЛЮЧАЯ серые пиксели
    for color_name, (lower, upper) in color_ranges:
        lower_hsv = np.array(lower, dtype=np.uint8)
        upper_hsv = np.array(upper, dtype=np.uint8)
        color_mask = cv2.inRange(hsv_roi, lower_hsv, upper_hsv)
        # Исключаем серые пиксели из цветной маски
        color_mask = color_mask & (~all_gray_mask.astype(np.uint8) * 255)
        percentage = cv2.countNonZero(color_mask) / total_pixels
        color_percentages[color_name] = percentage
    
    # Объединяем красные диапазоны
    if "red" in color_percentages and "red2" in color_percentages:
        color_percentages["red"] = color_percentages["red"] + color_percentages["red2"]
        del color_percentages["red2"]
    
    # Сортируем цвета по проценту (от большего к меньшему)
    if not color_percentages:
        top_colors = []
        dominant_color = "unknown"
        max_percentage = 0.0
    else:
        # Фильтруем цвета с минимальным процентом (больше 5%)
        filtered_colors = {k: v for k, v in color_percentages.items() if v > 0.05}
        
        # Сортируем по проценту
        sorted_colors = sorted(filtered_colors.items(), key=lambda x: x[1], reverse=True)
        
        # Берем топ-N цветов
        top_colors = [
            {"name": name, "percentage": pct}
            for name, pct in sorted_colors[:top_n]
        ]
        
        # Для обратной совместимости оставляем основной цвет
        if top_colors:
            dominant_color = top_colors[0]["name"]
            max_percentage = top_colors[0]["percentage"]
        else:
            dominant_color = "unknown"
            max_percentage = 0.0
    
    # Вычисляем средний BGR цвет
    avg_bgr = np.mean(roi, axis=(0, 1)).astype(int)
    
    return {
        "top_colors": top_colors,
        "color_name": dominant_color,  # Для обратной совместимости
        "color_percentage": max_percentage,  # Для обратной совместимости
        "bgr_avg": avg_bgr.tolist(),
        "all_percentages": color_percentages  # Все проценты для отладки
    }
def get_color_for_detection(frame, detection, top_n=2, crop_border_ratio=0.15, excluded_colors=None, lighting_compensation=None):
    """
    Определяет преобладающие цвета для конкретной детекции
    
    Args:
        frame: Кадр видео
        detection: (class_id, confidence, x1, y1, x2, y2)
        top_n: Количество цветов для возврата (по умолчанию 2)
        crop_border_ratio: Коэффициент обрезки краев ROI (0.0-0.5)
        excluded_colors: Список запрещенных цветов для этого класса (например, ["green"])
        lighting_compensation: Настройки компенсации освещения
    
    Returns:
        dict: Информация о преобладающих цветах (без запрещенных цветов)
    """
    class_id, conf, x1, y1, x2, y2 = detection
    roi = extract_roi(frame, x1, y1, x2, y2, crop_border_ratio=crop_border_ratio)
    color_info = detect_dominant_color(roi, top_n=top_n, lighting_compensation=lighting_compensation)
    
    # Фильтруем запрещенные цвета
    if excluded_colors and isinstance(excluded_colors, list):
        excluded_colors_lower = [c.lower() for c in excluded_colors]
        
        # Фильтруем из top_colors
        if "top_colors" in color_info:
            filtered_top_colors = [
                color_item for color_item in color_info["top_colors"]
                if color_item.get("name", "").lower() not in excluded_colors_lower
            ]
            color_info["top_colors"] = filtered_top_colors
            
            # Обновляем основной цвет (для обратной совместимости)
            if filtered_top_colors:
                color_info["color_name"] = filtered_top_colors[0]["name"]
                color_info["color_percentage"] = filtered_top_colors[0]["percentage"]
            else:
                # Если все цвета были исключены, возвращаем unknown
                color_info["color_name"] = "unknown"
                color_info["color_percentage"] = 0.0
        
        # Также удаляем из all_percentages для отладки
        if "all_percentages" in color_info:
            for excluded_color in excluded_colors_lower:
                if excluded_color in color_info["all_percentages"]:
                    del color_info["all_percentages"][excluded_color]
    
    return color_info


def annotate_rejected(frame, rejected, detector, color=(0, 0, 255)):
    """Возвращает кадр с выделенными отклонёнными объектами"""
    overlay = frame.copy()
    for item in rejected:
        det = item.get("det")
        info = item.get("info", {})
        if not det:
            continue
        class_id, _, x1, y1, x2, y2 = det
        class_name = detector.CLASSES.get(class_id, f"class_{class_id}")
        reason = info.get("reason", "filtered")
        label = f"{class_name} [{reason}]"
        cv2.rectangle(overlay, (x1, y1), (x2, y2), color, 2)
        cv2.putText(overlay, label, (x1, max(15, y1 - 10)),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
    return overlay
-----

[Файл: c:\Users\kolyb\Identification\XakatonAI\src\image_utils.py]
[Размер: 2971 байт]
[Дата изменения: 2025-11-29 13:43:14.257225]

import cv2
import numpy as np
import os

def imread_unicode(image_path):
    """Чтение изображения с поддержкой кириллицы в пути"""
    image_path = os.path.normpath(image_path)
    try:
        with open(image_path, "rb") as f:
            bytes_data = bytearray(f.read())
        numpy_array = np.asarray(bytes_data, dtype=np.uint8)
        image = cv2.imdecode(numpy_array, cv2.IMREAD_COLOR)
        return image
    except Exception as e:
        pass
    try:
        return cv2.imread(image_path)
    except:
        return None

def resize_frame(frame, max_width, max_height, maintain_aspect=True, keep_width_native=False):
    """Изменение размера кадра с сохранением пропорций"""
    height, width = frame.shape[:2]
    
    if keep_width_native:
        if max_height and height > max_height:
            new_height = max_height
            resized = cv2.resize(frame, (width, new_height), interpolation=cv2.INTER_AREA)
            scale = new_height / height
            return resized, scale
        return frame, 1.0
    
    if (max_width == 0 or max_width is None) and (max_height == 0 or max_height is None):
        return frame, 1.0
    
    if max_width == 0: max_width = width
    if max_height == 0: max_height = height
    
    if width <= max_width and height <= max_height:
        return frame, 1.0
    
    if maintain_aspect:
        scale = min(max_width / width, max_height / height)
        new_width = int(width * scale)
        new_height = int(height * scale)
    else:
        new_width = max_width
        new_height = max_height
        scale = min(max_width / width, max_height / height)
    
    resized = cv2.resize(frame, (new_width, new_height), interpolation=cv2.INTER_AREA)
    return resized, scale

def extract_roi(frame, x1, y1, x2, y2, crop_border_ratio=0.0):
    """
    Вырезает область интереса
    
    Args:
        frame: кадр изображения
        x1, y1, x2, y2: координаты области
        crop_border_ratio: коэффициент обрезки краев (0.0-0.5), чтобы исключить края детекции
    """
    h, w = frame.shape[:2]
    
    # Применяем обрезку краев, если указано
    if crop_border_ratio > 0.0 and crop_border_ratio < 0.5:
        width = x2 - x1
        height = y2 - y1
        crop_x = int(width * crop_border_ratio)
        crop_y = int(height * crop_border_ratio)
        x1 = x1 + crop_x
        y1 = y1 + crop_y
        x2 = x2 - crop_x
        y2 = y2 - crop_y
    
    # Проверяем границы
    x1 = max(0, min(w - 1, x1))
    x2 = max(0, min(w, x2))
    y1 = max(0, min(h - 1, y1))
    y2 = max(0, min(h, y2))
    
    if x2 <= x1 or y2 <= y1:
        return None
    return frame[y1:y2, x1:x2]
-----

[Файл: c:\Users\kolyb\Identification\XakatonAI\src\object_manager.py]
[Размер: 9746 байт]
[Дата изменения: 2025-11-29 13:43:34.350129]

"""
Модуль для управления объектами на экране
"""

from typing import Dict, List, Optional, Tuple
from collections import defaultdict
from src.screen_object import ScreenObject, ObjectStatus
from src.tracker import ReIDTracker, Track
import numpy as np


class ObjectManager:
    """Менеджер для управления объектами на экране"""
    
    def __init__(self, class_names: Dict[int, str]):
        """
        Инициализация менеджера объектов
        
        Args:
            class_names: словарь соответствия class_id -> имя класса
        """
        self.class_names = class_names
        # Хранилище объектов по типу: {object_type: {object_id: ScreenObject}}
        self.objects_by_type: Dict[str, Dict[int, ScreenObject]] = defaultdict(dict)
        # Счетчики ID для каждого типа объекта
        self.next_id_by_type: Dict[str, int] = defaultdict(int)
        # Связь track_id -> object_id для каждого типа
        self.track_to_object: Dict[int, Tuple[str, int]] = {}  # track_id -> (object_type, object_id)
    
    def get_object_type(self, class_id: int) -> str:
        
        return self.class_names.get(class_id, f"class_{class_id}").lower()
    
    def create_object_from_track(self, track: Track, frame_num: int) -> ScreenObject:
        """
        Создает ScreenObject из Track
        
        Args:
            track: трек объекта
            frame_num: номер кадра
            
        Returns:
            созданный ScreenObject
        """
        object_type = self.get_object_type(track.class_id)
        
        # Получаем следующий ID для этого типа
        object_id = self.next_id_by_type[object_type]
        self.next_id_by_type[object_type] += 1
        
        # Создаем объект
        screen_object = ScreenObject(
            object_id=object_id,
            object_type=object_type,
            class_id=track.class_id,
            bbox=track.bbox,
            confidence=track.confidence,
            frame_num=frame_num,
            features=track.features
        )
        
        # Копируем дополнительную информацию
        if hasattr(track, 'train_number'):
            screen_object.train_number = track.train_number
        
        # Сохраняем объект
        self.objects_by_type[object_type][object_id] = screen_object
        
        # Сохраняем связь track_id -> object_id
        self.track_to_object[track.track_id] = (object_type, object_id)
        
        return screen_object
    
    def update_object_from_track(self, track: Track, frame_num: int, frame: np.ndarray) -> Optional[ScreenObject]:
        """
        Обновляет существующий объект из трека или создает новый
        
        Args:
            track: трек объекта
            frame_num: номер кадра
            frame: кадр изображения для обновления цветов
            
        Returns:
            обновленный или созданный ScreenObject
        """
        # Проверяем, есть ли уже объект для этого трека
        if track.track_id in self.track_to_object:
            object_type, object_id = self.track_to_object[track.track_id]
            screen_object = self.objects_by_type[object_type].get(object_id)
            
            if screen_object:
                # Обновляем существующий объект
                screen_object.update(
                    bbox=track.bbox,
                    confidence=track.confidence,
                    frame_num=frame_num,
                    features=track.features
                )
                
                # Обновляем цвета (каждый N-й кадр для оптимизации)
                if frame_num % 5 == 0:  # Обновляем цвета каждые 5 кадров
                    # Используем компенсацию освещения для лучшего определения цветов
                    lighting_compensation = {"enabled": True, "normalize_brightness": False, "wider_color_ranges": True}
                    screen_object.update_colors(frame, top_n=4, lighting_compensation=lighting_compensation)
                
                # Обновляем статус
                screen_object.update_status()
                
                # Обновляем номер поезда, если есть
                if hasattr(track, 'train_number') and track.train_number:
                    screen_object.train_number = track.train_number
                
                return screen_object
        
        # Если объекта нет, создаем новый
        return self.create_object_from_track(track, frame_num)
    
    def get_object_by_track_id(self, track_id: int) -> Optional[ScreenObject]:
        """
        Получает объект по track_id
        
        Args:
            track_id: ID трека
            
        Returns:
            ScreenObject или None
        """
        if track_id not in self.track_to_object:
            return None
        
        object_type, object_id = self.track_to_object[track_id]
        return self.objects_by_type[object_type].get(object_id)
    
    def get_objects_by_type(self, object_type: str) -> List[ScreenObject]:
        """
        Получает все объекты определенного типа
        
        Args:
            object_type: тип объекта
            
        Returns:
            список объектов
        """
        return list(self.objects_by_type[object_type].values())
    
    def get_all_objects(self) -> List[ScreenObject]:
        """
        Получает все объекты
        
        Returns:
            список всех объектов
        """
        all_objects = []
        for objects_dict in self.objects_by_type.values():
            all_objects.extend(objects_dict.values())
        return all_objects
    
    def remove_inactive_objects(self, active_track_ids: set, max_age: int = 150):
        """
        Удаляет неактивные объекты (треки которых больше не существуют)
        
        Args:
            active_track_ids: множество активных track_id
            max_age: максимальный возраст объекта без обновления
        """
        # Находим неактивные треки
        inactive_tracks = set(self.track_to_object.keys()) - active_track_ids
        
        for track_id in list(inactive_tracks):
            if track_id in self.track_to_object:
                object_type, object_id = self.track_to_object[track_id]
                screen_object = self.objects_by_type[object_type].get(object_id)
                
                if screen_object:
                    # Проверяем возраст объекта
                    frames_since_update = screen_object.last_update_frame - screen_object.frame_num
                    if frames_since_update > max_age:
                        # Удаляем объект
                        del self.objects_by_type[object_type][object_id]
                        del self.track_to_object[track_id]
    
    def get_statistics(self) -> Dict:
        """
        Получает статистику по объектам
        
        Returns:
            словарь со статистикой
        """
        stats = {}
        for object_type, objects_dict in self.objects_by_type.items():
            stats[object_type] = {
                'total': len(objects_dict),
                'by_status': {},
                'by_colors': {}  # Статистика по цветам
            }
            
            # Подсчет по статусам и цветам
            for obj in objects_dict.values():
                # Статусы
                status = obj.status.value
                if status not in stats[object_type]['by_status']:
                    stats[object_type]['by_status'][status] = 0
                stats[object_type]['by_status'][status] += 1
                
                # Цвета
                if obj.color_info and obj.color_info.get('top_colors'):
                    # Берем основной цвет (первый в списке)
                    top_colors = obj.color_info.get('top_colors', [])
                    if top_colors:
                        main_color = top_colors[0].get('name', 'unknown')
                        if main_color not in stats[object_type]['by_colors']:
                            stats[object_type]['by_colors'][main_color] = 0
                        stats[object_type]['by_colors'][main_color] += 1
                else:
                    # Если цвет не определен
                    if 'unknown' not in stats[object_type]['by_colors']:
                        stats[object_type]['by_colors']['unknown'] = 0
                    stats[object_type]['by_colors']['unknown'] += 1
        
        return stats

-----

[Файл: c:\Users\kolyb\Identification\XakatonAI\src\ocr_reader.py]
[Размер: 14708 байт]
[Дата изменения: 2025-11-29 09:41:57.488981]

"""
Модуль для распознавания номеров поездов с помощью OCR
"""

import cv2
import numpy as np
from typing import Optional, Tuple, Dict
import re


class TrainNumberOCR:
    """Класс для распознавания номеров поездов"""
    
    def __init__(self, ocr_engine="easyocr", languages=['en', 'ru']):
        """
        Инициализация OCR
        
        Args:
            ocr_engine: движок OCR ('easyocr' или 'tesseract')
            languages: языки для распознавания
        """
        self.ocr_engine = ocr_engine
        self.languages = languages
        self.reader = None
        
        if ocr_engine == "easyocr":
            try:
                import easyocr
                import warnings
                # Подавляем предупреждение о pin_memory при использовании CPU
                with warnings.catch_warnings():
                    warnings.filterwarnings("ignore", message=".*pin_memory.*")
                    self.reader = easyocr.Reader(languages, gpu=False)
                print(f"EasyOCR инициализирован для языков: {languages}")
            except ImportError:
                print("EasyOCR не установлен. Установите: pip install easyocr")
                self.reader = None
            except Exception as e:
                print(f"Ошибка при инициализации EasyOCR: {e}")
                self.reader = None
        elif ocr_engine == "tesseract":
            try:
                import pytesseract
                self.reader = pytesseract
                print("Tesseract OCR готов к использованию")
            except ImportError:
                print("pytesseract не установлен. Установите: pip install pytesseract")
                self.reader = None
            except Exception as e:
                print(f"Ошибка при инициализации Tesseract: {e}")
                self.reader = None
    
    def preprocess_image(self, roi: np.ndarray) -> np.ndarray:
        """
        Предобработка изображения для улучшения распознавания
        
        Args:
            roi: область интереса
            
        Returns:
            обработанное изображение
        """
        # Конвертируем в grayscale если нужно
        if len(roi.shape) == 3:
            gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        else:
            gray = roi.copy()
        
        # Увеличиваем контраст
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        enhanced = clahe.apply(gray)
        
        # Бинаризация (адаптивная)
        binary = cv2.adaptiveThreshold(
            enhanced, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
            cv2.THRESH_BINARY, 11, 2
        )
        
        # Морфологические операции для очистки
        kernel = np.ones((2, 2), np.uint8)
        cleaned = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
        
        # Увеличение размера для лучшего распознавания
        height, width = cleaned.shape
        scale = max(2.0, 300.0 / max(height, width))
        new_width = int(width * scale)
        new_height = int(height * scale)
        resized = cv2.resize(cleaned, (new_width, new_height), 
                            interpolation=cv2.INTER_CUBIC)
        
        return resized
    
    def recognize_text_easyocr(self, roi: np.ndarray) -> Optional[str]:
        """Распознавание текста с помощью EasyOCR"""
        if self.reader is None:
            return None
        
        try:
            # Пробуем распознавание без предобработки (для лучшего качества)
            results_raw = self.reader.readtext(roi)
            
            # Если не получилось, пробуем с предобработкой
            if not results_raw or len(results_raw) == 0:
                processed = self.preprocess_image(roi)
                results = self.reader.readtext(processed)
            else:
                results = results_raw
            
            if not results:
                return None
            
            # Объединяем все найденные тексты, сортируем по уверенности
            texts_with_conf = []
            for (bbox, text, confidence) in results:
                if confidence > 0.2:  # Снижаем порог для лучшего распознавания
                    texts_with_conf.append((text.strip(), confidence))
            
            if texts_with_conf:
                # Сортируем по уверенности (от большей к меньшей)
                texts_with_conf.sort(key=lambda x: x[1], reverse=True)
                
                # Берем текст с наибольшей уверенностью или объединяем несколько
                if len(texts_with_conf) == 1:
                    combined_text = texts_with_conf[0][0]
                else:
                    # Объединяем несколько результатов
                    combined_text = " ".join([t[0] for t in texts_with_conf[:3]])  # Берем до 3 лучших
                
                # Очищаем текст, но сохраняем пробелы и кириллицу
                # Убираем только специальные символы, оставляем буквы, цифры и пробелы
                cleaned = re.sub(r'[^\w\sА-Яа-яЁё]', '', combined_text)
                cleaned = re.sub(r'\s+', ' ', cleaned)  # Убираем множественные пробелы
                result = cleaned.strip()
                
                # Проверяем, что результат не пустой и содержит хотя бы одну букву или цифру
                if result and (re.search(r'[А-Яа-яЁёA-Za-z]', result) or re.search(r'\d', result)):
                    return result
            
            return None
        except Exception as e:
            print(f"Ошибка EasyOCR: {e}")
            return None
    
    def recognize_text_tesseract(self, roi: np.ndarray) -> Optional[str]:
        """Распознавание текста с помощью Tesseract"""
        if self.reader is None:
            return None
        
        try:
            # Предобработка
            processed = self.preprocess_image(roi)
            
            # Конфигурация для распознавания цифр и букв
            config = r'--oem 3 --psm 7 -c tessedit_char_whitelist=0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZАБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ'
            
            # Распознавание
            text = self.reader.image_to_string(processed, config=config, lang='eng+rus')
            
            if text:
                # Очищаем текст
                cleaned = re.sub(r'[^\w\s]', '', text)
                return cleaned.strip()
            
            return None
        except Exception as e:
            print(f"Ошибка Tesseract: {e}")
            return None
    
    def recognize_train_number(self, frame: np.ndarray, 
                              roi_config: Dict) -> Optional[str]:
        """
        Распознавание номера поезда из заданной области кадра
        
        Args:
            frame: кадр изображения
            roi_config: конфигурация области интереса
                {
                    "x": процент от ширины (0.0-1.0),
                    "y": процент от высоты (0.0-1.0),
                    "width": процент ширины (0.0-1.0),
                    "height": процент высоты (0.0-1.0)
                }
            
        Returns:
            распознанный номер поезда или None
        """
        if self.reader is None:
            return None
        
        h, w = frame.shape[:2]
        
        # Вычисляем координаты ROI
        x = int(w * roi_config.get("x", 0.0))
        y = int(h * roi_config.get("y", 0.0))
        width = int(w * roi_config.get("width", 0.2))
        height = int(h * roi_config.get("height", 0.1))
        
        # Проверяем границы
        x = max(0, min(w - 1, x))
        y = max(0, min(h - 1, y))
        width = max(1, min(w - x, width))
        height = max(1, min(h - y, height))
        
        # Извлекаем ROI
        roi = frame[y:y+height, x:x+width]
        
        if roi.size == 0:
            return None
        
        # Распознавание в зависимости от движка
        if self.ocr_engine == "easyocr":
            return self.recognize_text_easyocr(roi)
        elif self.ocr_engine == "tesseract":
            return self.recognize_text_tesseract(roi)
        
        return None
    
    def recognize_from_right_half(self, frame: np.ndarray) -> Optional[str]:
        """
        Распознавание номера поезда из правой половины экрана
        (разделение вертикальной чертой по середине, ищем в правой половине)
        
        Args:
            frame: кадр изображения
            
        Returns:
            распознанный номер поезда или None
        """
        if self.reader is None:
            return None
        
        h, w = frame.shape[:2]
        
        # Центр экрана - вертикальная черта по середине
        center_x = w // 2
        
        # Правая половина экрана: от центра до края по всей высоте
        x = center_x  # Начинаем с центра по X
        y = 0  # Начинаем с верха экрана
        width = w - center_x  # Вся правая половина ширины
        height = h  # Вся высота экрана
        
        # Проверяем границы
        x = max(0, min(w - 1, x))
        y = max(0, min(h - 1, y))
        width = max(1, min(w - x, width))
        height = max(1, min(h - y, height))
        
        # Извлекаем ROI
        roi = frame[y:y+height, x:x+width]
        
        if roi.size == 0:
            return None
        
        # Распознавание в зависимости от движка
        if self.ocr_engine == "easyocr":
            return self.recognize_text_easyocr(roi)
        elif self.ocr_engine == "tesseract":
            return self.recognize_text_tesseract(roi)
        
        return None
    
    def recognize_from_bottom_right_quadrant(self, frame: np.ndarray) -> Optional[str]:
        """
        Распознавание номера поезда из правого нижнего квадранта кадра
        (устаревший метод, используйте recognize_from_right_half)
        """
        return self.recognize_from_right_half(frame)
    
    def recognize_from_train_bbox(self, frame: np.ndarray, 
                                 bbox: Tuple[int, int, int, int],
                                 roi_offset: Dict = None) -> Optional[str]:
        """
        Распознавание номера поезда из области детектированного поезда
        
        Args:
            frame: кадр изображения
            bbox: координаты поезда (x1, y1, x2, y2)
            roi_offset: смещение и размер ROI относительно bbox
                {
                    "x_offset": смещение по X (процент от ширины bbox),
                    "y_offset": смещение по Y (процент от высоты bbox),
                    "width": ширина ROI (процент от ширины bbox),
                    "height": высота ROI (процент от высоты bbox)
                }
        
        Returns:
            распознанный номер поезда или None
        """
        if self.reader is None:
            return None
        
        x1, y1, x2, y2 = bbox
        train_width = x2 - x1
        train_height = y2 - y1
        
        # Параметры по умолчанию для номера в верхней части поезда
        if roi_offset is None:
            roi_offset = {
                "x_offset": 0.1,  # 10% от левого края
                "y_offset": 0.05,  # 5% от верхнего края
                "width": 0.3,      # 30% ширины поезда
                "height": 0.15     # 15% высоты поезда
            }
        
        # Вычисляем координаты ROI
        roi_x = int(x1 + train_width * roi_offset["x_offset"])
        roi_y = int(y1 + train_height * roi_offset["y_offset"])
        roi_w = int(train_width * roi_offset["width"])
        roi_h = int(train_height * roi_offset["height"])
        
        # Проверяем границы
        h, w = frame.shape[:2]
        roi_x = max(0, min(w - 1, roi_x))
        roi_y = max(0, min(h - 1, roi_y))
        roi_w = max(1, min(w - roi_x, roi_w))
        roi_h = max(1, min(h - roi_y, roi_h))
        
        # Извлекаем ROI
        roi = frame[roi_y:roi_y+roi_h, roi_x:roi_x+roi_w]
        
        if roi.size == 0:
            return None
        
        # Распознавание
        if self.ocr_engine == "easyocr":
            return self.recognize_text_easyocr(roi)
        elif self.ocr_engine == "tesseract":
            return self.recognize_text_tesseract(roi)
        
        return None


-----

[Файл: c:\Users\kolyb\Identification\XakatonAI\src\processors.py]
[Размер: 16248 байт]
[Дата изменения: 2025-11-29 16:43:14.987692]

import cv2
import os
from pathlib import Path

from src.image_utils import imread_unicode, resize_frame, extract_roi
from src.filters import apply_color_filters, annotate_rejected
from src.reid import FeatureExtractor
from src.tracker import ReIDTracker
from src.ocr_reader import TrainNumberOCR

# Наборы имен классов для attribute моделей (если нужны — можно вынести в конфиг)
PPE_NAMES = {
    0: 'boots', 1: 'gloves', 2: 'helmet', 3: 'noBoots',
    4: 'noGloves', 5: 'noHelmet', 6: 'noVest', 7: 'vest'
}

CLOTHES_NAMES = {
    3: 'hat', 4: 'jacket',
    5: 'pants', 6: 'shirt', 7: 'shoe', 8: 'shorts',
    10: 'sunglass'
}

def _map_ppe_names(pairs):
    # pairs: list of (cls_id, conf)
    return [PPE_NAMES.get(int(cid), f"ppe_{cid}") for cid, _ in pairs]


def _map_clothes_names(pairs):
    return [CLOTHES_NAMES.get(int(cid), f"cloth_{cid}") for cid, _ in pairs]


def process_image(image_path, detector, attribute_models, config):
    """Обработка одиночного изображения — добавляем атрибуты к людям и рисуем их рядом с ID"""
    print(f"Загрузка изображения: {image_path}")

    if not os.path.exists(image_path):
        print(f"Ошибка: файл не существует!")
        return

    image = imread_unicode(image_path)
    if image is None:
        print("Ошибка загрузки изображения")
        return

    print(f"Изображение загружено: {image.shape[1]}x{image.shape[0]}")

    processing_cfg = config.get("processing", {})
    show_preview = processing_cfg.get("show_preview", True)
    save_results = processing_cfg.get("save_results", False)
    output_dir = processing_cfg.get("output_dir", "results")
    debug_cfg = config.get("debug", {})

    # Resize если нужно
    opt_cfg = config.get("video_optimization", {})
    max_width = opt_cfg.get("max_width", 1920)
    max_height = opt_cfg.get("max_height", 1080)
    maintain_aspect = opt_cfg.get("maintain_aspect_ratio", True)
    keep_width_native = opt_cfg.get("keep_width_native", False)

    needs_resize = (
        (keep_width_native and image.shape[0] > max_height)
        or (image.shape[1] > max_width or image.shape[0] > max_height)
    )

    if needs_resize:
        image, scale = resize_frame(image, max_width, max_height, maintain_aspect, keep_width_native)
        print(f"Изменён размер: {image.shape[1]}x{image.shape[0]} (scale={scale:.2f})")

    target_classes = config.get("detection", {}).get("target_classes", [0, 6])
    detections = detector.detect(image, target_classes=target_classes)
    detections, rejected = apply_color_filters(image, detections, config.get("color_filters", {}), detector.CLASSES, debug_cfg)

    # Если ReID не используется — мы всё равно хотим приписать атрибуты к человеку.
    per_detection_attrs = {}  # key: index in detections, value: list of attrs strings

    if attribute_models:
        for i, det in enumerate(detections):
            class_id, conf, x1, y1, x2, y2 = det
            if class_id != 0:
                continue
            roi = extract_roi(image, x1, y1, x2, y2, crop_border_ratio=0.0)
            if roi is None:
                continue
            ppe = attribute_models.run_ppe(roi) if attribute_models.ppe else []
            clothes = attribute_models.run_clothes(roi) if attribute_models.clothes else []
            names = _map_ppe_names(ppe) + _map_clothes_names(clothes)
            if names:
                per_detection_attrs[i] = names

    # Рисуем только основные детекции; атрибуты будут добавлены к подписям
    result = detector.draw_detections(image, detections, show_track_ids=False, per_detection_attrs=per_detection_attrs)

    if debug_cfg.get("show_filtered_objects") and rejected:
        result = annotate_rejected(result, rejected, detector)

    if save_results:
        os.makedirs(output_dir, exist_ok=True)
        out = Path(output_dir) / f"{Path(image_path).stem}_detected.png"
        cv2.imwrite(str(out), result)
        print(f"Результат сохранён: {out}")

    if show_preview:
        try:
            cv2.imshow("Результат", result)
            cv2.waitKey(0)
            cv2.destroyAllWindows()
        except Exception:
            # headless environment — просто сохранён результат
            pass


def process_video(video_path, detector, attribute_models, config):
    """Обработка видео: ReID + OCR + приписывание атрибутов к трекам"""
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        alt = str(Path(video_path).absolute())
        cap = cv2.VideoCapture(alt)
    if not cap.isOpened():
        print("Ошибка: не удалось открыть видео")
        return

    original_fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    print(f"Видео: {width}x{height}, {original_fps} FPS, {total_frames} кадров")

    # CONFIG
    opt_cfg = config.get("video_optimization", {})
    target_fps = opt_cfg.get("target_fps", 10)
    max_width = opt_cfg.get("max_width", 1280)
    max_height = opt_cfg.get("max_height", 720)
    frame_skip_cfg = opt_cfg.get("frame_skip", 1)
    maintain_aspect = opt_cfg.get("maintain_aspect_ratio", True)
    keep_width_native = opt_cfg.get("keep_width_native", False)

    processing_cfg = config.get("processing", {})
    show_preview = processing_cfg.get("show_preview", True)
    save_results = processing_cfg.get("save_results", False)
    output_dir = processing_cfg.get("output_dir", "results")
    debug_cfg = config.get("debug", {})

    if frame_skip_cfg > 1:
        skip_frames = frame_skip_cfg
    else:
        if target_fps <= 0 or original_fps <= 0:
            skip_frames = 1
        else:
            skip_frames = max(1, int(original_fps / target_fps))

    print(f"Пропуск кадров: {skip_frames}")

    # sample resize
    ret, temp = cap.read()
    if not ret:
        print("Ошибка чтения первого кадра")
        return
    resized_sample, _ = resize_frame(temp, max_width, max_height, maintain_aspect, keep_width_native)
    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)

    # ReID init
    reid_cfg = config.get("re_identification", {})
    use_reid = reid_cfg.get("enabled", False)
    tracker = None
    feature_extractor = None

    if use_reid:
        try:
            dev = config.get("yolo", {}).get("device", "cpu")
            feature_extractor = FeatureExtractor(device=dev)
            tracker = ReIDTracker(
                feature_extractor=feature_extractor,
                max_distance=reid_cfg.get("max_distance", 0.5),
                max_age=reid_cfg.get("max_age", 30),
                min_hits=reid_cfg.get("min_hits", 1),
                iou_threshold=reid_cfg.get("iou_threshold", 0.3)
            )
            print("ReID инициализирован")
        except Exception as e:
            print("ReID отключён:", e)
            use_reid = False

    # OCR init (для поездов)
    ocr_cfg = config.get("train_number_ocr", {})
    use_ocr = ocr_cfg.get("enabled", False)
    ocr_reader = None
    ocr_frame_skip = ocr_cfg.get("frame_skip", 10)
    ocr_counter = 0
    if use_ocr:
        try:
            ocr_reader = TrainNumberOCR(ocr_engine=ocr_cfg.get("engine", "easyocr"))
            if ocr_reader.reader is None:
                use_ocr = False
                print("OCR не инициализирован")
        except Exception as e:
            print("OCR отключён:", e)
            use_ocr = False

    frame_count = 0
    writer = None
    output_file = None

    last_recognized_train_number = None

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            frame_count += 1
            if frame_count % skip_frames != 0:
                continue

            # resize
            need_resize = (
                (keep_width_native and frame.shape[0] > max_height)
                or (frame.shape[1] > max_width or frame.shape[0] > max_height)
            )
            resized, _ = resize_frame(frame, max_width, max_height, maintain_aspect, keep_width_native) if need_resize else (frame, 1)

            # main detection
            target_classes = config.get("detection", {}).get("target_classes", [0, 6])
            detections = detector.detect(resized, target_classes=target_classes)
            detections, rejected = apply_color_filters(resized, detections, config.get("color_filters", {}), detector.CLASSES, debug_cfg)

            tracks = None
            train_numbers = {}

            if use_reid and tracker is not None:
                try:
                    # tracker.update expects detections list in format (class_id, conf, x1, y1, x2, y2)
                    tracks = tracker.update(resized, detections)

                    # Присваиваем атрибуты к трекам: для каждого person track делаем crop и запускаем attribute_models
                    if attribute_models:
                        for track_id, class_id, conf, x1, y1, x2, y2 in tracks:
                            if class_id != 0:
                                continue
                            track_obj = tracker.tracks.get(track_id)
                            if track_obj is None:
                                continue

                            # Периодически (или если нет attributes) обновляем атрибуты
                            update_attrs = not hasattr(track_obj, "attributes") or (frame_count % 10 == 0)
                            if update_attrs:
                                person_crop = extract_roi(resized, x1, y1, x2, y2, crop_border_ratio=0.0)
                                if person_crop is not None:
                                    ppe = attribute_models.run_ppe(person_crop) if attribute_models.ppe else []
                                    clothes = attribute_models.run_clothes(person_crop) if attribute_models.clothes else []
                                    ppe_names = _map_ppe_names(ppe)
                                    clothes_names = _map_clothes_names(clothes)
                                    track_obj.attributes = {
                                        "ppe": sorted(set(track_obj.attributes.get("ppe", []) + ppe_names)) if hasattr(track_obj, "attributes") else ppe_names,
                                        "clothes": sorted(set(track_obj.attributes.get("clothes", []) + clothes_names)) if hasattr(track_obj, "attributes") else clothes_names
                                    }

                    # OCR для поездов (пример)
                    if use_ocr and ocr_reader is not None:
                        ocr_counter += 1
                        if ocr_counter >= ocr_frame_skip:
                            ocr_counter = 0
                            tn = ocr_reader.recognize_from_right_half(resized)
                            if tn:
                                last_recognized_train_number = tn
                                # присваиваем всем трекам поездов если есть
                                for tid, cid, *_ in tracks:
                                    if cid == 6:
                                        t = tracker.tracks.get(tid)
                                        if t and not getattr(t, "train_number", None):
                                            t.train_number = tn

                    # собираем train_numbers из треков
                    if tracks:
                        for tid, cid, *_ in tracks:
                            t = tracker.tracks.get(tid)
                            if t and getattr(t, "train_number", None):
                                train_numbers[tid] = t.train_number
                            elif cid == 6 and last_recognized_train_number:
                                train_numbers[tid] = last_recognized_train_number

                    # Перед отрисовкой даём детектору доступ к tracker (чтобы он мог брать attributes)
                    detector.tracker = tracker

                    # draw with IDs (детектор сам добавит items из track.attributes)
                    if tracks and len(tracks) > 0:
                        result_frame = detector.draw_detections(resized, tracks, show_track_ids=True, train_numbers=train_numbers)
                    else:
                        result_frame = detector.draw_detections(resized, detections, show_track_ids=False)

                except Exception as e:
                    print(f"Ошибка в трекинге: {e}")
                    result_frame = detector.draw_detections(resized, detections, show_track_ids=False)
            else:
                # no reid: для каждой обнаруженной персоны считаем атрибуты и передаём в per_detection_attrs
                per_detection_attrs = {}
                if attribute_models:
                    for idx, det in enumerate(detections):
                        class_id, conf, x1, y1, x2, y2 = det
                        if class_id != 0:
                            continue
                        roi = extract_roi(resized, x1, y1, x2, y2, crop_border_ratio=0.0)
                        if roi is None:
                            continue
                        ppe = attribute_models.run_ppe(roi) if attribute_models.ppe else []
                        clothes = attribute_models.run_clothes(roi) if attribute_models.clothes else []
                        items = _map_ppe_names(ppe) + _map_clothes_names(clothes)
                        if items:
                            per_detection_attrs[idx] = items

                result_frame = detector.draw_detections(resized, detections, show_track_ids=False, per_detection_attrs=per_detection_attrs)

            if debug_cfg.get("show_filtered_objects") and rejected:
                result_frame = annotate_rejected(result_frame, rejected, detector)

            # info text
            cv2.putText(result_frame, f"Frame {frame_count}", (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)

            # save writer
            if save_results:
                if writer is None:
                    os.makedirs(output_dir, exist_ok=True)
                    output_file = Path(output_dir) / f"{Path(video_path).stem}_detected.mp4"
                    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                    writer = cv2.VideoWriter(str(output_file), fourcc, target_fps, (result_frame.shape[1], result_frame.shape[0]))
                writer.write(result_frame)

            # show
            if show_preview:
                try:
                    cv2.imshow("Детекция", result_frame)
                    if cv2.waitKey(int(1000 / target_fps)) & 0xFF == ord('q'):
                        break
                except Exception:
                    # headless
                    pass

    except KeyboardInterrupt:
        print("\nОбработка прервана")
    finally:
        cap.release()
        if writer:
            writer.release()
        cv2.destroyAllWindows()
        print("Готово!")
        if output_file:
            print("Видео сохранено:", output_file)

-----

[Файл: c:\Users\kolyb\Identification\XakatonAI\src\reid.py]
[Размер: 6967 байт]
[Дата изменения: 2025-11-29 09:41:57.489767]

"""
Модуль для извлечения признаков (features) для re-identification
"""

import cv2
import numpy as np
from typing import List, Tuple, Optional
import torch
import torch.nn as nn
from torchvision import transforms
from src.image_utils import extract_roi


class FeatureExtractor:
    """Класс для извлечения признаков из изображений объектов"""
    
    def __init__(self, device="cpu", feature_dim=128):
        """
        Инициализация экстрактора признаков
        
        Args:
            device: устройство для обработки (cpu/cuda)
            feature_dim: размерность вектора признаков
        """
        self.device = device
        self.feature_dim = feature_dim
        self.model = self._build_model()
        self.model.eval()
        self.model.to(device)
        
        # Трансформации для предобработки изображений
        self.transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((256, 128)),  # Стандартный размер для person re-id
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
    
    def _build_model(self):
        """Построение модели для извлечения признаков"""
        # Используем упрощенную архитектуру на основе ResNet
        # Можно заменить на предобученную модель для person re-id
        model = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
            
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(256, self.feature_dim),
            nn.BatchNorm1d(self.feature_dim)
        )
        return model
    
    def extract_features(self, frame: np.ndarray, detections: List[Tuple]) -> np.ndarray:
        """
        Извлечение признаков для всех детекций
        
        Args:
            frame: кадр изображения (BGR)
            detections: список детекций [(class_id, confidence, x1, y1, x2, y2), ...]
            
        Returns:
            массив признаков shape=(n_detections, feature_dim)
        """
        if len(detections) == 0:
            return np.array([])
        
        features_list = []
        
        for det in detections:
            class_id, confidence, x1, y1, x2, y2 = det
            
            # Извлекаем ROI
            roi = extract_roi(frame, x1, y1, x2, y2)
            if roi is None or roi.size == 0:
                # Если ROI пустой, используем нулевой вектор
                features_list.append(np.zeros(self.feature_dim))
                continue
            
            # Конвертируем BGR в RGB
            roi_rgb = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)
            
            # Предобработка и извлечение признаков
            try:
                # Преобразуем в тензор
                input_tensor = self.transform(roi_rgb).unsqueeze(0).to(self.device)
                
                # Извлекаем признаки
                with torch.no_grad():
                    features = self.model(input_tensor)
                    # Нормализуем признаки
                    features = torch.nn.functional.normalize(features, p=2, dim=1)
                    features = features.cpu().numpy().flatten()
                
                features_list.append(features)
            except Exception as e:
                # В случае ошибки используем нулевой вектор
                print(f"Ошибка при извлечении признаков: {e}")
                features_list.append(np.zeros(self.feature_dim))
        
        return np.array(features_list)
    
    def compute_distance(self, features1: np.ndarray, features2: np.ndarray) -> float:
        """
        Вычисление расстояния между двумя векторами признаков (cosine distance)
        
        Args:
            features1: первый вектор признаков
            features2: второй вектор признаков
            
        Returns:
            расстояние (0-1, где 0 - идентичны, 1 - максимально разные)
        """
        if features1.shape != features2.shape:
            return 1.0
        
        # Cosine distance
        dot_product = np.dot(features1, features2)
        norm1 = np.linalg.norm(features1)
        norm2 = np.linalg.norm(features2)
        
        if norm1 == 0 or norm2 == 0:
            return 1.0
        
        cosine_similarity = dot_product / (norm1 * norm2)
        # Преобразуем similarity в distance
        distance = 1.0 - cosine_similarity
        
        return max(0.0, min(1.0, distance))
    
    def compute_distance_matrix(self, features1: np.ndarray, features2: np.ndarray) -> np.ndarray:
        """
        Вычисление матрицы расстояний между двумя наборами признаков
        
        Args:
            features1: массив признаков shape=(n1, feature_dim)
            features2: массив признаков shape=(n2, feature_dim)
            
        Returns:
            матрица расстояний shape=(n1, n2)
        """
        if len(features1) == 0 or len(features2) == 0:
            return np.array([])
        
        # Нормализуем признаки
        features1_norm = features1 / (np.linalg.norm(features1, axis=1, keepdims=True) + 1e-8)
        features2_norm = features2 / (np.linalg.norm(features2, axis=1, keepdims=True) + 1e-8)
        
        # Вычисляем cosine distance через dot product
        similarity_matrix = np.dot(features1_norm, features2_norm.T)
        distance_matrix = 1.0 - similarity_matrix
        
        return np.clip(distance_matrix, 0.0, 1.0)


-----

[Файл: c:\Users\kolyb\Identification\XakatonAI\src\screen_object.py]
[Размер: 10050 байт]
[Дата изменения: 2025-11-29 13:43:57.490039]

"""
Модуль для представления объектов на экране
"""

import numpy as np
from typing import List, Tuple, Dict, Optional
from enum import Enum
import cv2
from src.image_utils import extract_roi
from src.filters import detect_dominant_color


class ObjectStatus(Enum):
    """Статусы объекта"""
    STAY = "stay"      # Остается на месте
    GO = "go"         # Движется
    WORK = "work"     # Работает (для поездов - в движении)
    UNKNOWN = "unknown"  # Неизвестно


class ScreenObject:
    """Класс для представления объекта на экране"""
    
    def __init__(self, object_id: int, object_type: str, class_id: int,
                 bbox: Tuple[int, int, int, int], confidence: float,
                 frame_num: int, features: Optional[np.ndarray] = None):
       
        self.object_id = object_id  # Уникальный ID в группе по типу
        self.object_type = object_type  # 'person', 'train', и т.д.
        self.class_id = class_id
        self.bbox = bbox
        self.confidence = confidence
        self.frame_num = frame_num
        self.features = features
        
        # История позиций для определения статуса
        self.position_history: List[Tuple[int, int, int, int]] = [bbox]
        self.frame_count = 1  # Количество кадров, которое объект фиксировался камерой
        
        # Основные цвета объекта
        self.dominant_colors: List[Tuple[int, int, int]] = []  # BGR формат (для обратной совместимости)
        self.color_info: Optional[Dict] = None  # Полная информация о цветах (названия, проценты, BGR)
        
        # Статус объекта
        self.status = ObjectStatus.UNKNOWN
        
        # Дополнительная информация
        self.train_number: Optional[str] = None  # Номер поезда (для train)
        self.last_update_frame = frame_num
        
        # Пороги для определения статуса
        self.movement_threshold = 5.0  # Минимальное перемещение для статуса GO (в пикселях)
        self.stay_threshold = 2.0  # Максимальное перемещение для статуса STAY
    
    def update(self, bbox: Tuple[int, int, int, int], confidence: float,
               frame_num: int, features: Optional[np.ndarray] = None):
        """
        Обновление объекта новыми данными
        
        Args:
            bbox: новые координаты
            confidence: новая уверенность
            frame_num: номер кадра
            features: новые признаки
        """
        self.bbox = bbox
        self.confidence = confidence
        self.frame_count += 1
        self.last_update_frame = frame_num
        
        # Обновляем историю позиций
        self.position_history.append(bbox)
        # Ограничиваем историю (храним последние 30 позиций)
        if len(self.position_history) > 30:
            self.position_history.pop(0)
        
        # Обновляем признаки
        if features is not None:
            if self.features is not None:
                alpha = 0.1  # Коэффициент обновления
                self.features = (1 - alpha) * self.features + alpha * features
            else:
                self.features = features
    
    def update_colors(self, frame: np.ndarray, top_n: int = 4, lighting_compensation: Optional[Dict] = None):
        """
        Обновляет основные цвета объекта на основе текущего кадра
        Использует улучшенный метод detect_dominant_color из filters.py
        
        Args:
            frame: кадр изображения (BGR)
            top_n: количество основных цветов для определения (по умолчанию 4)
            lighting_compensation: настройки компенсации освещения (опционально)
        """
        roi = extract_roi(frame, *self.bbox, crop_border_ratio=0.1)  # Обрезаем края на 10%
        if roi is None or roi.size == 0:
            return
        
        # Используем улучшенный метод определения цветов
        try:
            color_info = detect_dominant_color(
                roi, 
                top_n=top_n, 
                lighting_compensation=lighting_compensation
            )
            
            # Сохраняем полную информацию о цветах
            self.color_info = color_info
            
            # Преобразуем результаты в формат BGR для обратной совместимости
            top_colors = color_info.get("top_colors", [])
            avg_bgr = color_info.get("bgr_avg", [0, 0, 0])
            
            if top_colors:
                # Сохраняем средний BGR для каждого доминирующего цвета
                # Используем средний BGR из всего ROI (можно улучшить, вычисляя для каждого цвета отдельно)
                self.dominant_colors = [tuple(avg_bgr)] * min(len(top_colors), top_n)
            else:
                # Если цвета не определены, используем средний цвет ROI
                if not avg_bgr or sum(avg_bgr) == 0:
                    avg_color = np.mean(roi.reshape(-1, 3), axis=0)
                    self.dominant_colors = [tuple(map(int, avg_color))]
                else:
                    self.dominant_colors = [tuple(avg_bgr)]
        except Exception as e:
            # Fallback на простой метод при ошибке
            avg_color = np.mean(roi.reshape(-1, 3), axis=0)
            self.dominant_colors = [tuple(map(int, avg_color))]
            self.color_info = None
    
    def update_status(self):
        """
        Обновляет статус объекта на основе истории перемещений
        """
        if len(self.position_history) < 2:
            self.status = ObjectStatus.UNKNOWN
            return
        
        # Вычисляем среднее перемещение за последние кадры
        movements = []
        for i in range(1, min(len(self.position_history), 10)):  # Последние 10 позиций
            prev_x1, prev_y1, prev_x2, prev_y2 = self.position_history[-i-1]
            curr_x1, curr_y1, curr_x2, curr_y2 = self.position_history[-i]
            
            # Центр предыдущей позиции
            prev_center_x = (prev_x1 + prev_x2) / 2
            prev_center_y = (prev_y1 + prev_y2) / 2
            
            # Центр текущей позиции
            curr_center_x = (curr_x1 + curr_x2) / 2
            curr_center_y = (curr_y1 + curr_y2) / 2
            
            # Расстояние перемещения
            distance = np.sqrt((curr_center_x - prev_center_x)**2 + 
                             (curr_center_y - prev_center_y)**2)
            movements.append(distance)
        
        if not movements:
            self.status = ObjectStatus.UNKNOWN
            return
        
        avg_movement = np.mean(movements)
        
        # Определяем статус
        if avg_movement < self.stay_threshold:
            self.status = ObjectStatus.STAY
        elif avg_movement >= self.movement_threshold:
            if self.object_type == 'train':
                self.status = ObjectStatus.WORK  # Поезд в движении = работает
            else:
                self.status = ObjectStatus.GO
        else:
            # Среднее перемещение - может быть медленное движение
            if self.object_type == 'train':
                self.status = ObjectStatus.WORK
            else:
                self.status = ObjectStatus.GO
    
    def get_info_dict(self) -> Dict:
        """
        Возвращает словарь с информацией об объекте
        
        Returns:
            словарь с информацией
        """
        info = {
            'object_id': self.object_id,
            'object_type': self.object_type,
            'class_id': self.class_id,
            'bbox': self.bbox,
            'confidence': self.confidence,
            'frame_count': self.frame_count,
            'status': self.status.value,
            'dominant_colors': self.dominant_colors,
            'train_number': self.train_number,
            'first_seen_frame': self.frame_num,
            'last_update_frame': self.last_update_frame
        }
        
        # Добавляем полную информацию о цветах, если доступна
        if self.color_info:
            info['color_info'] = {
                'top_colors': self.color_info.get('top_colors', []),
                'all_percentages': self.color_info.get('all_percentages', {}),
                'bgr_avg': self.color_info.get('bgr_avg', [])
            }
        
        return info
    
    def __repr__(self):
        return (f"ScreenObject(id={self.object_id}, type={self.object_type}, "
                f"status={self.status.value}, frames={self.frame_count})")

-----

[Файл: c:\Users\kolyb\Identification\XakatonAI\src\tracker.py]
[Размер: 13189 байт]
[Дата изменения: 2025-11-29 11:31:23.590312]

"""
Модуль для отслеживания объектов между кадрами (re-identification)
"""

import numpy as np
from typing import List, Tuple, Dict, Optional
from collections import defaultdict
from src.reid import FeatureExtractor


class Track:
    """Класс для представления трека объекта"""
    
    def __init__(self, track_id: int, class_id: int, bbox: Tuple[int, int, int, int], 
                 confidence: float, features: np.ndarray, frame_num: int):
        """
        Инициализация трека
        
        Args:
            track_id: уникальный ID трека
            class_id: ID класса объекта
            bbox: координаты (x1, y1, x2, y2)
            confidence: уверенность детекции
            features: вектор признаков
            frame_num: номер кадра
        """
        self.track_id = track_id
        self.class_id = class_id
        self.bbox = bbox
        self.confidence = confidence
        self.features = features
        self.frame_num = frame_num
        self.age = 1  # Возраст трека (количество кадров)
        self.time_since_update = 0  # Кадров с последнего обновления
        self.history = [bbox]  # История позиций
        self.train_number = None  # Номер поезда (для класса train)
        self.attributes = {"ppe": [], "clothes": []}

        
    def update(self, bbox: Tuple[int, int, int, int], confidence: float, 
               features: np.ndarray, frame_num: int):
        """Обновление трека"""
        self.bbox = bbox
        self.confidence = confidence
        # Обновляем признаки как взвешенное среднее
        alpha = 0.1  # Коэффициент обновления
        self.features = (1 - alpha) * self.features + alpha * features
        self.frame_num = frame_num
        self.age += 1
        self.time_since_update = 0
        self.history.append(bbox)
        # Ограничиваем историю
        if len(self.history) > 30:
            self.history.pop(0)
    
    def predict(self):
        """Предсказание следующей позиции (простая линейная экстраполяция)"""
        if len(self.history) < 2:
            return self.bbox
        
        # Простое предсказание на основе последних двух позиций
        x1, y1, x2, y2 = self.bbox
        if len(self.history) >= 2:
            prev_x1, prev_y1, prev_x2, prev_y2 = self.history[-2]
            dx1 = x1 - prev_x1
            dy1 = y1 - prev_y1
            dx2 = x2 - prev_x2
            dy2 = y2 - prev_y2
            
            pred_x1 = int(x1 + dx1)
            pred_y1 = int(y1 + dy1)
            pred_x2 = int(x2 + dx2)
            pred_y2 = int(y2 + dy2)
            
            return (pred_x1, pred_y1, pred_x2, pred_y2)
        
        return self.bbox


class ReIDTracker:
    """Трекер для re-identification объектов"""
    
    def __init__(self, feature_extractor: FeatureExtractor, 
                 max_distance: float = 0.5,
                 max_age: int = 30,
                 min_hits: int = 1,
                 iou_threshold: float = 0.3):
        """
        Инициализация трекера
        
        Args:
            feature_extractor: экстрактор признаков
            max_distance: максимальное расстояние признаков для сопоставления
            max_age: максимальный возраст трека без обновления
            min_hits: минимальное количество попаданий для подтверждения трека
            iou_threshold: порог IoU для сопоставления по позиции
        """
        self.feature_extractor = feature_extractor
        self.max_distance = max_distance
        self.max_age = max_age
        self.min_hits = min_hits
        self.iou_threshold = iou_threshold
        
        self.tracks: Dict[int, Track] = {}
        self.next_id = 1
        self.frame_count = 0
        # Счетчик всех когда-либо созданных треков по классам
        self.all_tracks_by_class: Dict[int, set] = defaultdict(set)
    
    def _compute_iou(self, bbox1: Tuple[int, int, int, int], 
                     bbox2: Tuple[int, int, int, int]) -> float:
        """Вычисление IoU между двумя bbox"""
        x1_1, y1_1, x2_1, y2_1 = bbox1
        x1_2, y1_2, x2_2, y2_2 = bbox2
        
        # Вычисляем пересечение
        x1_i = max(x1_1, x1_2)
        y1_i = max(y1_1, y1_2)
        x2_i = min(x2_1, x2_2)
        y2_i = min(y2_1, y2_2)
        
        if x2_i <= x1_i or y2_i <= y1_i:
            return 0.0
        
        inter_area = (x2_i - x1_i) * (y2_i - y1_i)
        
        # Вычисляем объединение
        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
        union_area = area1 + area2 - inter_area
        
        if union_area == 0:
            return 0.0
        
        return inter_area / union_area
    
    def _associate_detections_to_tracks(self, detections: List[Tuple], 
                                       detection_features: np.ndarray,
                                       tracks: List[Track]) -> Tuple[List[int], List[int], List[int]]:
        """
        Сопоставление детекций с существующими треками
        
        Returns:
            matched: список индексов (det_idx, track_idx)
            unmatched_dets: список индексов несоответствующих детекций
            unmatched_tracks: список индексов несоответствующих треков
        """
        if len(tracks) == 0:
            return [], list(range(len(detections))), []
        
        if len(detections) == 0:
            return [], [], list(range(len(tracks)))
        
        # Вычисляем матрицу расстояний признаков
        track_features = np.array([t.features for t in tracks])
        distance_matrix = self.feature_extractor.compute_distance_matrix(
            detection_features, track_features
        )
        
        # Вычисляем матрицу IoU
        iou_matrix = np.zeros((len(detections), len(tracks)))
        for i, det in enumerate(detections):
            _, _, x1, y1, x2, y2 = det
            det_bbox = (x1, y1, x2, y2)
            for j, track in enumerate(tracks):
                iou_matrix[i, j] = self._compute_iou(det_bbox, track.bbox)
        
        # Комбинированная метрика: расстояние признаков + IoU
        # Нормализуем IoU (чем больше, тем лучше, поэтому инвертируем)
        iou_cost = 1.0 - iou_matrix
        combined_cost = 0.7 * distance_matrix + 0.3 * iou_cost
        
        # Жадное сопоставление (простой алгоритм)
        matched = []
        unmatched_dets = list(range(len(detections)))
        unmatched_tracks = list(range(len(tracks)))
        
        # Сортируем по стоимости и сопоставляем
        cost_pairs = []
        for i in range(len(detections)):
            for j in range(len(tracks)):
                cost = combined_cost[i, j]
                # Проверяем пороги
                if distance_matrix[i, j] <= self.max_distance and iou_matrix[i, j] >= self.iou_threshold:
                    cost_pairs.append((cost, i, j))
        
        cost_pairs.sort(key=lambda x: x[0])
        
        used_dets = set()
        used_tracks = set()
        
        for cost, det_idx, track_idx in cost_pairs:
            if det_idx not in used_dets and track_idx not in used_tracks:
                matched.append((det_idx, track_idx))
                used_dets.add(det_idx)
                used_tracks.add(track_idx)
        
        unmatched_dets = [i for i in range(len(detections)) if i not in used_dets]
        unmatched_tracks = [i for i in range(len(tracks)) if i not in used_tracks]
        
        return matched, unmatched_dets, unmatched_tracks
    
    def update(self, frame: np.ndarray, detections: List[Tuple]) -> List[Tuple]:
        """
        Обновление трекера новыми детекциями
        
        Args:
            frame: текущий кадр
            detections: список детекций [(class_id, confidence, x1, y1, x2, y2), ...]
            
        Returns:
            список треков с ID: [(track_id, class_id, confidence, x1, y1, x2, y2), ...]
        """
        self.frame_count += 1
        
        # Извлекаем признаки для всех детекций
        detection_features = self.feature_extractor.extract_features(frame, detections)
        
        # Получаем активные треки (не слишком старые)
        active_tracks = [t for t in self.tracks.values() 
                        if t.time_since_update < self.max_age]
        
        # Предсказываем позиции для активных треков
        for track in active_tracks:
            track.predict()
            track.time_since_update += 1
        
        # Сопоставляем детекции с треками
        matched, unmatched_dets, unmatched_tracks = self._associate_detections_to_tracks(
            detections, detection_features, active_tracks
        )
        
        # Обновляем сопоставленные треки
        for det_idx, track_idx in matched:
            det = detections[det_idx]
            track = active_tracks[track_idx]
            class_id, confidence, x1, y1, x2, y2 = det
            track.update((x1, y1, x2, y2), confidence, 
                        detection_features[det_idx], self.frame_count)
        
        # Создаем новые треки для несоответствующих детекций
        for det_idx in unmatched_dets:
            det = detections[det_idx]
            class_id, confidence, x1, y1, x2, y2 = det
            track_id = self.next_id
            self.next_id += 1
            
            new_track = Track(
                track_id=track_id,
                class_id=class_id,
                bbox=(x1, y1, x2, y2),
                confidence=confidence,
                features=detection_features[det_idx],
                frame_num=self.frame_count
            )
            self.tracks[track_id] = new_track
            # Сохраняем информацию о всех созданных треках
            self.all_tracks_by_class[class_id].add(track_id)
        
        # Удаляем старые треки
        tracks_to_remove = []
        for track_id, track in self.tracks.items():
            if track.time_since_update >= self.max_age:
                tracks_to_remove.append(track_id)
        
        for track_id in tracks_to_remove:
            del self.tracks[track_id]
        
        # Возвращаем все треки, которые были обновлены в текущем кадре
        # (time_since_update == 0 означает, что трек был обновлен в этом кадре)
        # Также показываем новые треки сразу (age >= min_hits)
        active_tracks_list = []
        for track in self.tracks.values():
            # Показываем треки, которые были обновлены в текущем кадре (включая новые)
            # или которые достаточно старые и еще активны
            if track.time_since_update == 0:
                x1, y1, x2, y2 = track.bbox
                active_tracks_list.append((
                    track.track_id,
                    track.class_id,
                    track.confidence,
                    x1, y1, x2, y2
                ))
            elif track.age >= self.min_hits and track.time_since_update < self.max_age:
                # Показываем старые треки, которые еще активны
                x1, y1, x2, y2 = track.bbox
                active_tracks_list.append((
                    track.track_id,
                    track.class_id,
                    track.confidence,
                    x1, y1, x2, y2
                ))
        
        return active_tracks_list


-----

[Файл: c:\Users\kolyb\Identification\XakatonAI\src\__init__.py]
[Размер: 0 байт]
[Дата изменения: 2025-11-28 18:24:27.065698]


-----

