[Файл: c:\Hahatonn\XakatonAI\Copy_Files_POSSIBLE_DANGER_RUN_ONLY_HERE.py]
[Размер: 5309 байт]
[Дата изменения: 2025-11-30 00:05:57.306416]

import os
import sys
import glob
from pathlib import Path
from datetime import datetime

def main():
    # Установка корневой директории
    if len(sys.argv) > 1:
        root_dir = sys.argv[1]
    else:
        root_dir = os.path.dirname(os.path.abspath(__file__))
    
    # Проверка существования директории
    if not os.path.exists(root_dir):
        print(f'Ошибка: Директория "{root_dir}" не существует!')
        input('Нажмите Enter для выхода...')
        return 1
    
    # Получение абсолютного пути
    root_dir = os.path.abspath(root_dir)
    
    print(f'Рабочая директория: {root_dir}')
    print()
    
    # Удалить существующий файл результатов
    if os.path.exists('Project_Files.txt'):
        os.remove('Project_Files.txt')
    
    # Список разрешенных расширений для исходного кода
    allowed_extensions = [
        '*.py', '*.json', '*.config','*.sql', '*.yml', '*.yaml', '.dockerignore', '.env.example'
    ]
    
    # Исключаемые директории
    exclude_dirs = ['.git', 'node_modules', 'bin', 'obj', 'packages', '.vs', '.idea']
    
    # Максимальный размер файла (1 МБ)
    max_size = 1048576
    
    print('Начало обработки файлов...')
    print()
    
    processed_files = 0
    skipped_files = 0
    
    # Рекурсивный поиск файлов
    for extension in allowed_extensions:
        pattern = os.path.join(root_dir, '**', extension)
        
        for file_path in glob.glob(pattern, recursive=True):
            # Проверка на исключаемые директории
            skip_file = False
            for exclude_dir in exclude_dirs:
                if exclude_dir in file_path.split(os.sep):
                    skip_file = True
                    break
            
            if skip_file:
                continue
            
            # Получение информации о файле
            try:
                file_size = os.path.getsize(file_path)
                mod_time = datetime.fromtimestamp(os.path.getmtime(file_path))
                
                # Проверка размера файла
                if file_size < max_size:
                    print(f'Обработка: {file_path}')
                    
                    with open('Project_Files.txt', 'a', encoding='utf-8') as result_file:
                        result_file.write(f'[Файл: {file_path}]\n')
                        result_file.write(f'[Размер: {file_size} байт]\n')
                        result_file.write(f'[Дата изменения: {mod_time}]\n')
                        result_file.write('\n')
                        
                        # Попытка чтения файла
                        try:
                            with open(file_path, 'r', encoding='utf-8') as src_file:
                                content = src_file.read()
                                result_file.write(content)
                        except UnicodeDecodeError:
                            # Попробуем другие кодировки
                            try:
                                with open(file_path, 'r', encoding='cp1251') as src_file:
                                    content = src_file.read()
                                    result_file.write(content)
                            except:
                                result_file.write('[Ошибка чтения файла - возможно бинарный файл]\n')
                        except Exception as e:
                            result_file.write(f'[Ошибка чтения файла: {str(e)}]\n')
                        
                        result_file.write('\n')
                        result_file.write('-----\n')
                        result_file.write('\n')
                    
                    processed_files += 1
                else:
                    print(f'Пропуск большого файла: {file_path} ({file_size} байт)')
                    
                    with open('Project_Files.txt', 'a', encoding='utf-8') as result_file:
                        result_file.write(f'[Файл: {file_path} - ПРОПУЩЕН (слишком большой: {file_size} байт)]\n')
                    
                    skipped_files += 1
                    
            except Exception as e:
                print(f'Ошибка при обработке файла {file_path}: {e}')
    
    print()
    print(f'Готово! Результат сохранен в Project_Files.txt')
    print(f'Обработано файлов: {processed_files}, пропущено: {skipped_files}')
    print(f'Обработана директория: {root_dir}')
    
    # Пауза в конце
    input('Нажмите Enter для выхода...')
    return 0

if __name__ == '__main__':
    sys.exit(main())
-----

[Файл: c:\Hahatonn\XakatonAI\main.py]
[Размер: 1986 байт]
[Дата изменения: 2025-11-29 20:58:32.224595]

import sys
import os
import warnings
from pathlib import Path

# Подавляем предупреждение о pin_memory при использовании CPU
warnings.filterwarnings("ignore", message=".*pin_memory.*", category=UserWarning)

# Импортируем наши модули
from src.config_loader import load_config
from src.detector import YOLODetector
from src.processors import process_image, process_video

def main():
    print("=" * 60)
    print("Детекция людей и поездов (YOLO11) - Modular Version")
    print("=" * 60)
    
    # 1. Загрузка конфига
    config = load_config("config.json")
    
    # 2. Проверка входного файла
    input_file = config.get("input_file", "")
    if not input_file:
        print("Ошибка: не указан input_file в конфиге")
        sys.exit(1)
        
    input_file = os.path.normpath(input_file.strip('"\''))
    if not os.path.exists(input_file):
        print(f"Файл не найден: {input_file}")
        sys.exit(1)

    # 3. Инициализация детектора
    yolo_cfg = config.get("yolo", {})
    detector = YOLODetector(
        model_path=yolo_cfg.get("model", "yolo11m.pt"),
        conf_threshold=config.get("detection", {}).get("confidence_threshold", 0.5),
        device=yolo_cfg.get("device", "cpu"),
        custom_colors=config.get("colors", {}),
        half_precision=config.get("processing", {}).get("half_precision", False)
    )

    # 4. Запуск обработки
    ext = Path(input_file).suffix.lower()
    image_exts = {'.jpg', '.jpeg', '.png', '.bmp', '.webp'}
    video_exts = {'.mp4', '.avi', '.mov', '.mkv'}

    if ext in image_exts:
        process_image(input_file, detector, config)
    elif ext in video_exts:
        process_video(input_file, detector, config)
    else:
        print(f"Неизвестный формат файла: {ext}")

if __name__ == "__main__":
    main()
-----

[Файл: c:\Hahatonn\XakatonAI\app\main.py]
[Размер: 21763 байт]
[Дата изменения: 2025-11-29 23:19:51.568406]

"""
FastAPI микросервис для детекции людей и поездов
"""

import os
import sys
import json
import time
import cv2
import numpy as np
from pathlib import Path
from typing import Optional
from fastapi import FastAPI, File, UploadFile, HTTPException, Form
from fastapi.responses import JSONResponse, StreamingResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware
import io
import base64
from PIL import Image

from app.models import (
    DetectionRequest, DetectionResponse, HealthResponse, 
    ErrorResponse, ObjectInfo, Statistics
)
from app.services.detector_service import DetectorService
from app.services.processing_service import ProcessingService

# Версия API
API_VERSION = "1.0.0"

# Инициализация FastAPI
app = FastAPI(
    title="Detection API",
    description="API для детекции людей и поездов с использованием YOLO",
    version=API_VERSION
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # В продакшене указать конкретные домены
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Глобальные переменные для сервисов
detector_service: Optional[DetectorService] = None
processing_service: Optional[ProcessingService] = None
config: dict = {}


def load_config(config_path: Optional[str] = None) -> dict:
    """Загрузка конфигурации"""
    if config_path is None:
        # Ищем config.json в корне проекта или в api_service
        possible_paths = [
            Path(__file__).parent.parent / "config.json",  # api_service/config.json
            Path(__file__).parent.parent.parent / "config.json",  # корень проекта
            Path("config.json")  # текущая директория
        ]
        
        for path in possible_paths:
            if path.exists():
                config_path = str(path)
                break
        
        if config_path is None:
            # Используем конфиг по умолчанию
            print("Конфиг не найден, используем конфиг по умолчанию")
            return get_default_config()
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"Ошибка при загрузке конфига: {e}, используем конфиг по умолчанию")
        return get_default_config()


def get_default_config() -> dict:
    """Конфигурация по умолчанию"""
    return {
        "yolo": {
            "model": "yolo11m.pt",
            "device": "cpu"
        },
        "detection": {
            "confidence_threshold": 0.5,
            "target_classes": [0, 6],
            "class_names": {
                "0": "person",
                "6": "train"
            }
        },
        "video_optimization": {
            "target_fps": 20,
            "max_width": 1280,
            "max_height": 540,
            "frame_skip": 1,
            "maintain_aspect_ratio": True,
            "keep_width_native": True
        },
        "processing": {
            "show_preview": False,
            "half_precision": False,
            "save_results": True,
            "output_dir": "results"
        },
        "debug": {
            "show_filtered_objects": False,
            "log_detection_details": False
        },
        "re_identification": {
            "enabled": True,
            "max_distance": 0.5,
            "max_age": 150,
            "min_hits": 10,
            "iou_threshold": 0.2,
            "use_objects": True
        },
        "train_number_ocr": {
            "enabled": True,
            "engine": "easyocr",
            "frame_skip": 10,
            "use_bottom_right_quadrant": True,
            "allowed_chars": "0123456789ЭП",
            "expected_length": 7,
            "languages": ["en", "ru"]
        },
        "ppe_detection": {
            "enabled": False,
            "model_path": None,
            "confidence_threshold": 0.5,
            "target_classes": None
        },
        "attributes": {
            "enabled": True,
            "ppe_model": "ppe_best.pt",
            "clothes_model": "clothes_best.pt",
            "device": "cpu",
            "confidence": 0.25,
            "update_interval": 10
        }
    }


@app.on_event("startup")
async def startup_event():
    """Инициализация при запуске"""
    import asyncio
    from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError
    
    global detector_service, processing_service, config
    
    print("Инициализация сервисов...")
    
    try:
        # Загрузка конфига - быстрая операция
        config = load_config()
        
        # Инициализация сервисов в отдельном потоке, чтобы не блокировать event loop
        def init_services():
            try:
                global detector_service, processing_service
                detector_service = DetectorService(config)
                detector_service.initialize()
                processing_service = ProcessingService(detector_service, config)
                return True
            except Exception as e:
                print(f"Ошибка в init_services: {e}")
                import traceback
                traceback.print_exc()
                return False
        
        # Выполняем в executor с таймаутом
        loop = asyncio.get_event_loop()
        executor = ThreadPoolExecutor(max_workers=1, thread_name_prefix="init")
        
        try:
            # Таймаут 5 минут на инициализацию (модели могут загружаться долго)
            result = await asyncio.wait_for(
                loop.run_in_executor(executor, init_services),
                timeout=300.0
            )
            if not result:
                raise Exception("Инициализация сервисов завершилась с ошибкой")
        except asyncio.TimeoutError:
            print("Таймаут при инициализации сервисов (превышено 5 минут)")
            executor.shutdown(wait=False, cancel_futures=True)
            raise
        except Exception as e:
            print(f"Ошибка при инициализации: {e}")
            executor.shutdown(wait=False, cancel_futures=True)
            raise
        finally:
            executor.shutdown(wait=False)
        
        print("Сервисы инициализированы успешно!")
    except Exception as e:
        print(f"Критическая ошибка при инициализации сервисов: {e}")
        import traceback
        traceback.print_exc()
        # Инициализируем пустые сервисы, чтобы приложение не упало
        detector_service = None
        processing_service = None
        config = get_default_config()


@app.get("/", response_model=dict)
async def root():
    """Корневой эндпоинт"""
    return {
        "message": "Detection API",
        "version": API_VERSION,
        "docs": "/docs"
    }


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Проверка здоровья сервиса"""
    if detector_service is None:
        return HealthResponse(
            status="error",
            version=API_VERSION,
            models_loaded={}
        )
    
    try:
        models_status = detector_service.get_models_status()
        # Проверяем, что хотя бы основной детектор загружен
        is_healthy = models_status.get("detector", False)
        
        return HealthResponse(
            status="healthy" if is_healthy else "degraded",
            version=API_VERSION,
            models_loaded=models_status
        )
    except Exception as e:
        return HealthResponse(
            status="error",
            version=API_VERSION,
            models_loaded={}
        )


def decode_image(file_content: bytes) -> np.ndarray:
    """Декодирование изображения из bytes"""
    if not file_content or len(file_content) == 0:
        raise ValueError("Пустой файл")
    nparr = np.frombuffer(file_content, np.uint8)
    image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    if image is None:
        raise ValueError("Не удалось декодировать изображение. Убедитесь, что файл является корректным изображением (JPEG, PNG и т.д.)")
    if image.size == 0:
        raise ValueError("Изображение пустое после декодирования")
    return image


def encode_image(image: np.ndarray) -> str:
    """Кодирование изображения в base64"""
    _, buffer = cv2.imencode('.jpg', image)
    img_base64 = base64.b64encode(buffer).decode('utf-8')
    return f"data:image/jpeg;base64,{img_base64}"


@app.post("/detect/image", response_model=DetectionResponse)
async def detect_image(
    file: UploadFile = File(...),
    confidence_threshold: float = Form(0.5),
    target_classes: str = Form("[0, 6]"),  # JSON строка
    return_image: bool = Form(True),
    return_statistics: bool = Form(True)
):
    """
    Детекция объектов на изображении
    
    Args:
        file: загруженное изображение
        confidence_threshold: порог уверенности (0.0-1.0)
        target_classes: JSON массив классов для детекции (по умолчанию [0, 6] - person, train)
        return_image: возвращать ли обработанное изображение
        return_statistics: возвращать ли статистику
    """
    if detector_service is None or processing_service is None:
        raise HTTPException(status_code=503, detail="Сервис не инициализирован")
    
    try:
        # Проверка типа файла
        if not file.content_type:
            # Если content_type не указан, проверяем расширение
            if file.filename:
                ext = Path(file.filename).suffix.lower()
                if ext not in ['.jpg', '.jpeg', '.png', '.bmp', '.gif', '.webp']:
                    raise HTTPException(status_code=400, detail="Неподдерживаемый формат файла. Используйте JPEG, PNG, BMP, GIF или WebP")
        elif not file.content_type.startswith('image/'):
            raise HTTPException(status_code=400, detail="Файл должен быть изображением")
        
        # Чтение файла
        file_content = await file.read()
        if not file_content or len(file_content) == 0:
            raise HTTPException(status_code=400, detail="Файл пустой")
        
        image = decode_image(file_content)
        
        # Парсинг target_classes
        try:
            import json
            target_classes_list = json.loads(target_classes)
        except:
            target_classes_list = [0, 6]
        
        # Параметры запроса
        request_params = {
            "confidence_threshold": confidence_threshold,
            "target_classes": target_classes_list,
            "return_image": return_image,
            "return_statistics": return_statistics
        }
        
        # Обработка
        objects, result_image, metadata = processing_service.process_image(
            image, request_params
        )
        
        # Формирование ответа
        response_data = {
            "success": True,
            "message": f"Найдено объектов: {len(objects)}",
            "objects": objects,
            "processing_time": metadata.get("processing_time"),
        }
        
        # Добавление изображения
        if return_image and result_image is not None:
            response_data["image"] = encode_image(result_image)
        
        # Добавление статистики
        if return_statistics:
            stats = metadata.get("statistics", {})
            if stats:
                response_data["statistics"] = {
                    obj_type: Statistics(**stat_data) 
                    for obj_type, stat_data in stats.items()
                }
            else:
                response_data["statistics"] = None
        
        return DetectionResponse(**response_data)
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Ошибка обработки: {str(e)}")


@app.post("/detect/video")
async def detect_video(
    file: UploadFile = File(...),
    confidence_threshold: float = Form(0.5),
    target_classes: str = Form("[0, 6]"),
    return_video: bool = Form(True),
    return_statistics: bool = Form(True)
):
    """
    Детекция объектов на видео
    
    Args:
        file: загруженное видео
        confidence_threshold: порог уверенности (0.0-1.0)
        target_classes: JSON массив классов для детекции
        return_video: возвращать ли обработанное видео
        return_statistics: возвращать ли статистику
    """
    if detector_service is None or processing_service is None:
        raise HTTPException(status_code=503, detail="Сервис не инициализирован. Проверьте /health")
    
    try:
        # Логируем информацию о файле для отладки
        print(f"Received video file: filename={file.filename}, content_type={file.content_type}")
        
        # Проверка типа файла - более мягкая проверка
        is_video = False
        
        # Проверяем расширение файла
        if file.filename:
            ext = Path(file.filename).suffix.lower()
            video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.webm', '.flv', '.m4v', '.wmv', '.3gp']
            if ext in video_extensions:
                is_video = True
                print(f"Video extension detected: {ext}")
        
        # Проверяем content_type
        if file.content_type:
            if file.content_type.startswith('video/'):
                is_video = True
                print(f"Video content_type detected: {file.content_type}")
            elif file.content_type.startswith('application/'):
                # Некоторые клиенты отправляют video как application/octet-stream
                if is_video:  # Если расширение правильное, принимаем
                    print(f"Accepting application/* with video extension")
                    is_video = True
        
        # Если не определили как видео, выдаем ошибку
        if not is_video:
            error_msg = f"File must be a video. filename={file.filename}, content_type={file.content_type}"
            print(f"ERROR: {error_msg}")
            raise HTTPException(status_code=400, detail=error_msg)
        
        # Сохранение временного файла
        temp_dir = Path("temp")
        temp_dir.mkdir(exist_ok=True)
        filename = file.filename or "upload_video"
        temp_file = temp_dir / f"upload_{int(time.time())}_{filename}"
        
        with open(temp_file, "wb") as f:
            content = await file.read()
            if not content or len(content) == 0:
                raise HTTPException(status_code=400, detail="Файл пустой")
            f.write(content)
        
        # Парсинг target_classes
        try:
            import json
            target_classes_list = json.loads(target_classes)
        except:
            target_classes_list = [0, 6]
        
        # Параметры запроса
        request_params = {
            "confidence_threshold": confidence_threshold,
            "target_classes": target_classes_list,
            "return_video": return_video,
            "return_statistics": return_statistics
        }
        
        # Обработка
        objects, output_path, metadata = processing_service.process_video(
            str(temp_file), request_params
        )
        
        # Если нужно вернуть только статистику
        if not return_video and return_statistics:
            stats = metadata.get("statistics", {})
            stats_dict = {
                obj_type: Statistics(**stat_data) 
                for obj_type, stat_data in stats.items()
            }
            
            # Удаление временного файла
            if temp_file.exists():
                temp_file.unlink()
            
            return {
                "success": True,
                "message": f"Обработано объектов: {len(objects)}",
                "objects": objects,
                "statistics": stats_dict,
                "processing_time": metadata.get("processing_time"),
                "total_frames": metadata.get("total_frames"),
                "processed_frames": metadata.get("processed_frames"),
            }
        
        # Если нужно вернуть видео
        if return_video and output_path and os.path.exists(output_path):
            # Если также нужна статистика, возвращаем JSON с информацией и ссылкой на видео
            if return_statistics:
                stats = metadata.get("statistics", {})
                stats_dict = {
                    obj_type: Statistics(**stat_data) 
                    for obj_type, stat_data in stats.items()
                }
                
                # Удаление временного файла
                if temp_file.exists():
                    temp_file.unlink()
                
                return {
                    "success": True,
                    "message": f"Обработано объектов: {len(objects)}",
                    "objects": objects,
                    "statistics": stats_dict,
                    "processing_time": metadata.get("processing_time"),
                    "total_frames": metadata.get("total_frames"),
                    "processed_frames": metadata.get("processed_frames"),
                    "video_url": f"/download/{Path(output_path).name}",
                    "video_filename": Path(output_path).name
                }
            else:
                # Только видео без статистики
                # Удаление временного файла
                if temp_file.exists():
                    temp_file.unlink()
                
                return FileResponse(
                    path=output_path,
                    filename=Path(output_path).name,
                    media_type='video/mp4'
                )
        
        # Если видео не создано, возвращаем только статистику
        stats = metadata.get("statistics", {})
        stats_dict = {
            obj_type: Statistics(**stat_data) 
            for obj_type, stat_data in stats.items()
        } if return_statistics else None
        
        # Удаление временного файла
        if temp_file.exists():
            temp_file.unlink()
        
        return {
            "success": True,
            "message": f"Обработано объектов: {len(objects)}",
            "objects": objects,
            "statistics": stats_dict,
            "processing_time": metadata.get("processing_time"),
            "total_frames": metadata.get("total_frames"),
            "processed_frames": metadata.get("processed_frames"),
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Ошибка обработки: {str(e)}")


@app.get("/download/{filename}")
async def download_file(filename: str):
    """Скачивание обработанного файла"""
    output_dir = Path(config.get("processing", {}).get("output_dir", "results"))
    file_path = output_dir / filename
    
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="Файл не найден")
    
    return FileResponse(
        path=str(file_path),
        filename=filename,
        media_type='application/octet-stream'
    )


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)


-----

[Файл: c:\Hahatonn\XakatonAI\app\models.py]
[Размер: 2229 байт]
[Дата изменения: 2025-11-29 22:55:20.655525]

"""
Pydantic модели для API запросов и ответов
"""

from pydantic import BaseModel
from typing import List, Optional, Dict, Any
from enum import Enum


class ObjectStatus(str, Enum):
    """Статусы объектов"""
    STAY = "stay"
    GO = "go"
    WORK = "work"
    UNKNOWN = "unknown"


class DetectionRequest(BaseModel):
    """Запрос на детекцию"""
    confidence_threshold: Optional[float] = 0.5
    target_classes: Optional[List[int]] = [0, 6]  # person, train
    return_image: Optional[bool] = True
    return_statistics: Optional[bool] = True


class ObjectInfo(BaseModel):
    """Информация об объекте"""
    object_id: int
    object_type: str
    class_id: int
    bbox: List[int]  # [x1, y1, x2, y2]
    confidence: float
    frame_count: int
    status: str
    stay_frames: int
    go_frames: int
    work_frames: int
    profession: Optional[str] = None
    attributes: Optional[Dict[str, List[str]]] = None
    dominant_colors: Optional[List[List[int]]] = None
    train_number: Optional[str] = None
    first_seen_frame: int
    last_update_frame: int


class Statistics(BaseModel):
    """Статистика по объектам"""
    total: int
    by_status: Dict[str, int]
    by_colors: Dict[str, int]
    by_profession: Dict[str, int]
    by_ppe: Optional[Dict[str, int]] = None
    by_clothes: Optional[Dict[str, int]] = None


class DetectionResponse(BaseModel):
    """Ответ на запрос детекции"""
    success: bool
    message: str
    objects: List[ObjectInfo]
    statistics: Optional[Dict[str, Statistics]] = None
    processing_time: Optional[float] = None
    total_frames: Optional[int] = None
    processed_frames: Optional[int] = None
    image: Optional[str] = None  # Base64 encoded image


class HealthResponse(BaseModel):
    """Ответ на проверку здоровья сервиса"""
    status: str
    version: str
    models_loaded: Dict[str, bool]


class ErrorResponse(BaseModel):
    """Ответ с ошибкой"""
    success: bool = False
    error: str
    detail: Optional[str] = None


-----

[Файл: c:\Hahatonn\XakatonAI\app\__init__.py]
[Размер: 97 байт]
[Дата изменения: 2025-11-29 22:30:02.801651]

"""
FastAPI микросервис для детекции людей и поездов
"""


-----

[Файл: c:\Hahatonn\XakatonAI\app\services\detector_service.py]
[Размер: 7871 байт]
[Дата изменения: 2025-11-29 22:45:56.952471]

"""
Сервис для инициализации и управления детекторами
"""

import os
import sys
from pathlib import Path
from typing import Optional, Dict, Any
import warnings

# Добавляем путь к src модулям
current_dir = Path(__file__).parent.parent.parent
src_path = current_dir / "src"
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))

from src.detector import YOLODetector
from src.ppe_detector import PPEDetector
from src.attribute_models import AttributeModels
from src.reid import FeatureExtractor
from src.tracker import ReIDTracker
from src.ocr_reader import TrainNumberOCR

warnings.filterwarnings("ignore", message=".*pin_memory.*", category=UserWarning)


class DetectorService:
    """Сервис для управления детекторами"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.detector: Optional[YOLODetector] = None
        self.ppe_detector: Optional[PPEDetector] = None
        self.attribute_models: Optional[AttributeModels] = None
        self.tracker: Optional[ReIDTracker] = None
        self.ocr_reader: Optional[TrainNumberOCR] = None
        self._initialized = False
    
    def initialize(self):
        """Инициализация всех детекторов"""
        if self._initialized:
            return
        
        # Инициализация основного детектора
        yolo_cfg = self.config.get("yolo", {})
        model_path = yolo_cfg.get("model", "yolo11m.pt")
        # Если путь относительный, ищем в api_service
        if not os.path.isabs(model_path):
            # Проверяем в api_service (относительно корня api_service)
            api_service_root = Path(__file__).parent.parent.parent
            api_service_model_path = api_service_root / model_path
            if api_service_model_path.exists():
                model_path = str(api_service_model_path)
            # Иначе используем как есть (YOLO сам найдет)
        
        self.detector = YOLODetector(
            model_path=model_path,
            conf_threshold=self.config.get("detection", {}).get("confidence_threshold", 0.5),
            device=yolo_cfg.get("device", "cpu"),
            custom_colors=self.config.get("colors", {}),
            half_precision=self.config.get("processing", {}).get("half_precision", False)
        )
        
        # Инициализация PPE детектора
        ppe_cfg = self.config.get("ppe_detection", {})
        if ppe_cfg.get("enabled", False):
            ppe_model_path = ppe_cfg.get("model_path")
            if ppe_model_path and os.path.exists(ppe_model_path):
                try:
                    self.ppe_detector = PPEDetector(
                        model_path=ppe_model_path,
                        conf_threshold=ppe_cfg.get("confidence_threshold", 0.5),
                        device=yolo_cfg.get("device", "cpu"),
                        custom_colors=ppe_cfg.get("colors", {}),
                        half_precision=self.config.get("processing", {}).get("half_precision", False)
                    )
                except Exception as e:
                    print(f"Ошибка при инициализации PPE детектора: {e}")
        
        # Инициализация моделей атрибутов
        attr_cfg = self.config.get("attributes", {})
        if attr_cfg.get("enabled", False):
            try:
                ppe_model_path = attr_cfg.get("ppe_model")
                clothes_model_path = attr_cfg.get("clothes_model")
                
                # Проверяем пути к моделям (относительно корня api_service)
                api_service_root = Path(__file__).parent.parent.parent
                if ppe_model_path and not os.path.isabs(ppe_model_path):
                    api_service_model_path = api_service_root / ppe_model_path
                    if api_service_model_path.exists():
                        ppe_model_path = str(api_service_model_path)
                
                if clothes_model_path and not os.path.isabs(clothes_model_path):
                    api_service_model_path = api_service_root / clothes_model_path
                    if api_service_model_path.exists():
                        clothes_model_path = str(api_service_model_path)
                
                attr_device = attr_cfg.get("device", yolo_cfg.get("device", "cpu"))
                attr_conf = attr_cfg.get("confidence", 0.25)
                
                self.attribute_models = AttributeModels(
                    ppe_model_path=ppe_model_path,
                    clothes_model_path=clothes_model_path,
                    device=attr_device,
                    conf=attr_conf
                )
            except Exception as e:
                print(f"Ошибка при инициализации моделей атрибутов: {e}")
        
        # Инициализация трекера
        reid_cfg = self.config.get("re_identification", {})
        if reid_cfg.get("enabled", False):
            try:
                device = yolo_cfg.get("device", "cpu")
                feature_extractor = FeatureExtractor(device=device)
                self.tracker = ReIDTracker(
                    feature_extractor=feature_extractor,
                    max_distance=reid_cfg.get("max_distance", 0.5),
                    max_age=reid_cfg.get("max_age", 150),
                    min_hits=reid_cfg.get("min_hits", 1),
                    iou_threshold=reid_cfg.get("iou_threshold", 0.3)
                )
            except Exception as e:
                print(f"Ошибка при инициализации трекера: {e}")
        
        # Инициализация OCR (может быть долгой, поэтому делаем в конце)
        ocr_cfg = self.config.get("train_number_ocr", {})
        if ocr_cfg.get("enabled", False):
            try:
                from src.ocr_reader import TrainNumberOCR
                print("Инициализация OCR (это может занять некоторое время)...")
                self.ocr_reader = TrainNumberOCR(
                    ocr_engine=ocr_cfg.get("engine", "easyocr"),
                    languages=ocr_cfg.get("languages", ["en", "ru"]),
                    allowed_chars=ocr_cfg.get("allowed_chars", "0123456789ЭП"),
                    expected_length=ocr_cfg.get("expected_length", 7)
                )
                if self.ocr_reader.reader is not None:
                    print("OCR инициализирован успешно")
                else:
                    print("OCR не инициализирован (ошибка загрузки)")
            except Exception as e:
                print(f"Ошибка при инициализации OCR: {e}")
                import traceback
                traceback.print_exc()
                self.ocr_reader = None
        
        self._initialized = True
    
    def get_models_status(self) -> Dict[str, bool]:
        """Получить статус загрузки моделей"""
        return {
            "detector": self.detector is not None,
            "ppe_detector": self.ppe_detector is not None and self.ppe_detector.is_enabled(),
            "attribute_models": self.attribute_models is not None and self.attribute_models.is_enabled(),
            "tracker": self.tracker is not None,
            "ocr_reader": self.ocr_reader is not None
        }


-----

[Файл: c:\Hahatonn\XakatonAI\app\services\processing_service.py]
[Размер: 24026 байт]
[Дата изменения: 2025-11-30 01:37:57.346470]

"""
Сервис для обработки изображений и видео
"""

import cv2
import numpy as np
import time
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import sys
import os

# Добавляем путь к src модулям
current_dir = Path(__file__).parent.parent.parent
src_path = current_dir / "src"
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))

from src.object_manager import ObjectManager
from src.filters import apply_color_filters
from src.image_utils import resize_frame
from src.db import insert_worker, insert_train
from app.models import ObjectInfo, Statistics, DetectionResponse
from app.services.detector_service import DetectorService


class ProcessingService:
    """Сервис для обработки изображений и видео"""
    
    def __init__(self, detector_service: DetectorService, config: Dict[str, Any]):
        self.detector_service = detector_service
        self.config = config
        self.object_manager = ObjectManager(detector_service.detector.CLASSES)
    
    def process_image(
        self, 
        image: np.ndarray,
        request_params: Dict[str, Any]
    ) -> Tuple[List[ObjectInfo], Optional[np.ndarray], Dict[str, Any]]:
        """
        Обработка изображения
        
        Args:
            image: изображение (numpy array)
            request_params: параметры запроса
            
        Returns:
            tuple: (список объектов, обработанное изображение, статистика)
        """
        start_time = time.time()
        
        # Получаем параметры
        confidence_threshold = request_params.get("confidence_threshold", 0.5)
        target_classes = request_params.get("target_classes", [0, 6])
        return_image = request_params.get("return_image", True)
        
        # Обновляем порог уверенности детектора
        if hasattr(self.detector_service.detector, 'conf_threshold'):
            self.detector_service.detector.conf_threshold = confidence_threshold
        
        # Детекция
        detections = self.detector_service.detector.detect(image, target_classes=target_classes)
        
        # Применяем цветовые фильтры
        color_filters = self.config.get("color_filters", {})
        debug_cfg = self.config.get("debug", {})
        detections, rejected = apply_color_filters(
            image, detections, color_filters, 
            self.detector_service.detector.CLASSES, debug_cfg
        )
        
        # Обработка через трекер (для единообразия с видео)
        result_image = None
        if return_image:
            result_image = image.copy()
            result_image = self.detector_service.detector.draw_detections(result_image, detections)
        
        # Создаем объекты из детекций
        objects = []
        for i, (class_id, conf, x1, y1, x2, y2) in enumerate(detections):
            object_type = self.object_manager.get_object_type(class_id)
            object_id = self.object_manager.next_id_by_type[object_type]
            self.object_manager.next_id_by_type[object_type] += 1
            
            # Создаем ScreenObject
            from src.screen_object import ScreenObject
            screen_object = ScreenObject(
                object_id=object_id,
                object_type=object_type,
                class_id=class_id,
                bbox=(x1, y1, x2, y2),
                confidence=conf,
                frame_num=0
            )
            
            # Обновляем цвета
            screen_object.update_colors(image, top_n=4)
            
            # Обновляем статус
            frame_height, frame_width = image.shape[:2]
            screen_object.update_status(frame_width=frame_width)
            
            # Обновляем атрибуты для людей
            if (self.detector_service.attribute_models and 
                self.detector_service.attribute_models.is_enabled() and 
                object_type == "person"):
                from src.image_utils import extract_roi
                from src.attribute_models import map_ppe_names, map_clothes_names
                
                person_crop = extract_roi(image, x1, y1, x2, y2, crop_border_ratio=0.0)
                if person_crop is not None and person_crop.size > 0:
                    try:
                        ppe = self.detector_service.attribute_models.run_ppe(person_crop)
                        clothes = self.detector_service.attribute_models.run_clothes(person_crop)
                        ppe_names = map_ppe_names(ppe)
                        clothes_names = map_clothes_names(clothes)
                        screen_object.attributes = {
                            "ppe": sorted(set(ppe_names)),
                            "clothes": sorted(set(clothes_names))
                        }
                    except Exception:
                        pass
            
            # Сохраняем объект
            self.object_manager.objects_by_type[object_type][object_id] = screen_object
            
            # Преобразуем в ObjectInfo
            obj_info = screen_object.get_info_dict()
            # Преобразуем bbox из кортежа в список
            if isinstance(obj_info.get('bbox'), tuple):
                obj_info['bbox'] = list(obj_info['bbox'])
            # Убеждаемся, что dominant_colors - список списков
            if obj_info.get('dominant_colors'):
                obj_info['dominant_colors'] = [
                    list(color) if isinstance(color, tuple) else color
                    for color in obj_info['dominant_colors']
                ]
            objects.append(ObjectInfo(**obj_info))
        
        # Статистика
        stats = self.object_manager.get_statistics(min_frames=1)
        
        # Убеждаемся, что статистика имеет правильную структуру
        for obj_type, stat_data in stats.items():
            # Убеждаемся, что все обязательные поля присутствуют
            if 'by_ppe' not in stat_data:
                stat_data['by_ppe'] = {}
            if 'by_clothes' not in stat_data:
                stat_data['by_clothes'] = {}
        
        processing_time = time.time() - start_time
        
        return objects, result_image, {
            "statistics": stats,
            "processing_time": processing_time,
            "total_detections": len(detections),
            "rejected": len(rejected)
        }
    
    def process_video(
        self,
        video_path: str,
        request_params: Dict[str, Any]
    ) -> Tuple[List[ObjectInfo], Optional[str], Dict[str, Any]]:
        """
        Обработка видео
        
        Args:
            video_path: путь к видео файлу
            request_params: параметры запроса
            
        Returns:
            tuple: (список объектов, путь к обработанному видео, метаданные)
        """
        start_time = time.time()
        
        # Обновляем конфиг с параметрами запроса
        config = self.config.copy()
        if "confidence_threshold" in request_params:
            if "detection" not in config:
                config["detection"] = {}
            config["detection"]["confidence_threshold"] = request_params["confidence_threshold"]
        if "target_classes" in request_params:
            if "detection" not in config:
                config["detection"] = {}
            config["detection"]["target_classes"] = request_params["target_classes"]
        
        # Обновляем порог уверенности детектора
        if hasattr(self.detector_service.detector, 'conf_threshold'):
            self.detector_service.detector.conf_threshold = request_params.get("confidence_threshold", 0.5)
        
        # Создаем новый ObjectManager для этого видео
        video_object_manager = ObjectManager(self.detector_service.detector.CLASSES)
        
        # Открываем видео
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"Не удалось открыть видео: {video_path}")
        
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        # Параметры оптимизации
        opt_config = config.get("video_optimization", {})
        frame_skip = opt_config.get("frame_skip", 1)
        max_width = opt_config.get("max_width", 1280)
        max_height = opt_config.get("max_height", 540)
        
        # Обработка кадров
        frame_count = 0
        processed_count = 0
        target_classes = request_params.get("target_classes", [0, 6])
        return_video = request_params.get("return_video", True)
        
        output_dir = Path(config.get("processing", {}).get("output_dir", "results"))
        output_dir.mkdir(exist_ok=True)
        
        # Создаем writer для выходного видео (если нужно)
        output_path = None
        writer = None
        if return_video:
            output_filename = f"processed_{Path(video_path).stem}_{int(time.time())}.mp4"
            output_path = str(output_dir / output_filename)
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            # Получаем размер первого кадра для writer
            ret, first_frame = cap.read()
            if ret:
                if first_frame.shape[1] > max_width or first_frame.shape[0] > max_height:
                    first_frame, _ = resize_frame(first_frame, max_width, max_height, True, False)
                frame_width = first_frame.shape[1]
                frame_height = first_frame.shape[0]
                writer = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))
                cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # Возвращаемся к началу
        
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_count += 1
                
                # Пропускаем кадры
                if frame_count % frame_skip != 0:
                    continue
                
                processed_count += 1
                
                # Изменяем размер если нужно
                resized_frame = frame
                if frame.shape[1] > max_width or frame.shape[0] > max_height:
                    resized_frame, _ = resize_frame(frame, max_width, max_height, True, False)
                
                # Детекция
                detections = self.detector_service.detector.detect(resized_frame, target_classes=target_classes)
                
                # Применяем фильтры
                color_filters = config.get("color_filters", {})
                debug_cfg = config.get("debug", {})
                detections, rejected = apply_color_filters(
                    resized_frame, detections, color_filters,
                    self.detector_service.detector.CLASSES, debug_cfg
                )
                
                # Обработка через трекер
                tracks = None
                if self.detector_service.tracker:
                    tracks = self.detector_service.tracker.update(resized_frame, detections)
                    
                    # Обновляем объекты
                    active_track_ids = set()
                    for track_data in tracks:
                        if len(track_data) >= 7:
                            track_id, class_id, conf, x1, y1, x2, y2 = track_data[:7]
                            active_track_ids.add(track_id)
                            
                            # Получаем Track объект из трекера
                            track_obj = self.detector_service.tracker.tracks.get(track_id)
                            if track_obj:
                                video_object_manager.update_object_from_track(
                                    track_obj, processed_count, resized_frame,
                                    self.detector_service.attribute_models,
                                    config.get("attributes", {}).get("update_interval", 10)
                                )
                    
                    # Удаляем неактивные объекты
                    video_object_manager.remove_inactive_objects(
                        active_track_ids,
                        max_age=config.get("re_identification", {}).get("max_age", 150)
                    )
                
                # Отрисовка для видео
                if writer:
                    if tracks and len(tracks) > 0:
                        # Отрисовка с треками
                        result_frame = self.detector_service.detector.draw_detections(
                            resized_frame, tracks, show_track_ids=True
                        )
                    else:
                        # Отрисовка без треков
                        result_frame = self.detector_service.detector.draw_detections(
                            resized_frame, detections, show_track_ids=False
                        )
                    writer.write(result_frame)
        
        finally:
            cap.release()
            if writer:
                writer.release()
        
        # Получаем все объекты
        all_objects = video_object_manager.get_all_objects()
        objects = []
        for obj in all_objects:
            obj_info = obj.get_info_dict()
            # Преобразуем bbox из кортежа в список
            if isinstance(obj_info.get('bbox'), tuple):
                obj_info['bbox'] = list(obj_info['bbox'])
            # Убеждаемся, что dominant_colors - список списков
            if obj_info.get('dominant_colors'):
                obj_info['dominant_colors'] = [
                    list(color) if isinstance(color, tuple) else color
                    for color in obj_info['dominant_colors']
                ]
            objects.append(ObjectInfo(**obj_info))
        
        # Статистика
        stats = video_object_manager.get_statistics(min_frames=1)
        
        # Убеждаемся, что статистика имеет правильную структуру
        for obj_type, stat_data in stats.items():
            # Убеждаемся, что все обязательные поля присутствуют
            if 'by_ppe' not in stat_data:
                stat_data['by_ppe'] = {}
            if 'by_clothes' not in stat_data:
                stat_data['by_clothes'] = {}
        
        processing_time = time.time() - start_time
        
        # Сохранение данных в БД (всегда, независимо от return_video)
        print(f"\n=== Сохранение данных в БД ===")
        try:
            # Проверяем подключение к БД
            from src.db import get_conn
            test_conn = get_conn()
            test_conn.close()
            print("Подключение к БД успешно")
            
            # Фильтруем объекты с минимальным количеством кадров (>= 20)
            min_frames_threshold = 20
            valid_objects = [obj for obj in all_objects if obj.frame_count >= min_frames_threshold]
            
            print(f"Найдено объектов для сохранения: {len(valid_objects)}")
            
            if len(valid_objects) == 0:
                print("Нет объектов для сохранения (нужно >= 20 кадров)")
            else:
                # Создаем обратный словарь для быстрого поиска track_id
                object_to_track = {}
                print(f"Всего связей track_to_object: {len(video_object_manager.track_to_object)}")
                for track_id, (obj_type, obj_id) in video_object_manager.track_to_object.items():
                    object_to_track[(obj_type, obj_id)] = track_id
                    if obj_type == "train":
                        print(f"  Связь поезда: track_id={track_id} -> (type={obj_type}, object_id={obj_id})")
                
                # Сохраняем данные о людях
                people_count = 0
                people_errors = 0
                for obj in valid_objects:
                    if obj.object_type == "person":
                        # Получаем цвета объекта
                        color = None
                        if obj.color_info and obj.color_info.get('top_colors'):
                            top_colors = obj.color_info.get('top_colors', [])
                            color_names = [color_item.get('name', 'unknown') for color_item in top_colors[:3]]
                            color = ",".join(color_names) if color_names else None
                        
                        # Получаем track_id из обратного словаря
                        track_id = object_to_track.get((obj.object_type, obj.object_id))
                        
                        if track_id is not None:
                            try:
                                insert_worker(
                                    track_id=track_id,
                                    color=color,
                                    stand_frames=obj.stay_frames,
                                    walk_frames=obj.go_frames,
                                    work_frames=obj.work_frames,
                                    attributes=obj.attributes if obj.attributes else None
                                )
                                people_count += 1
                                print(f"✓ Работник track_id={track_id} сохранен в БД")
                            except Exception as e:
                                people_errors += 1
                                error_msg = str(e)
                                print(f"✗ Ошибка при сохранении работника track_id={track_id}: {error_msg}")
                                if people_errors <= 3 and ("Connection refused" in error_msg or "connection to server" in error_msg.lower()):
                                    print("БД недоступна. Прерываем сохранение.")
                                    break
                        else:
                            print(f"Не найден track_id для работника object_id={obj.object_id}")
                
                # Сохраняем данные о поездах в таблицу trains
                trains_count = 0
                trains_errors = 0
                train_objects = [obj for obj in valid_objects if obj.object_type == "train"]
                print(f"Найдено поездов для сохранения: {len(train_objects)}")
                
                # Отладочная информация
                if len(train_objects) > 0:
                    print(f"Доступные связи в object_to_track для поездов:")
                    for (ot, oid), tid in object_to_track.items():
                        if ot == "train":
                            print(f"  (type={ot}, object_id={oid}) -> track_id={tid}")
                    print(f"Поезда в valid_objects:")
                    for obj in train_objects:
                        print(f"  object_id={obj.object_id}, object_type={obj.object_type}, class_id={obj.class_id}, frames={obj.frame_count}")
                
                for obj in train_objects:
                    # Получаем track_id из обратного словаря
                    track_id = object_to_track.get((obj.object_type, obj.object_id))
                    print(f"Поиск track_id для поезда: (type={obj.object_type}, object_id={obj.object_id}) -> track_id={track_id}")
                    
                    if track_id is not None:
                        try:
                            print(f"Сохранение поезда в таблицу trains: track_id={track_id}, "
                                  f"номер={obj.train_number}, время={obj.frame_count} кадров")
                            insert_train(
                                track_id=track_id,
                                number=obj.train_number,
                                total=obj.frame_count
                            )
                            trains_count += 1
                            print(f"Поезд track_id={track_id} успешно сохранен в таблицу trains")
                        except Exception as e:
                            trains_errors += 1
                            error_msg = str(e)
                            print(f"Ошибка при сохранении поезда track_id={track_id} в таблицу trains: {error_msg}")
                            if trains_errors <= 3 and ("Connection refused" in error_msg or "connection to server" in error_msg.lower()):
                                print("БД недоступна. Прерываем сохранение.")
                                break
                    else:
                        print(f"Не найден track_id для поезда object_id={obj.object_id}")
                
                print(f"Сохранено в БД: работников={people_count}, поездов={trains_count}")
                if people_errors > 0 or trains_errors > 0:
                    print(f"Ошибок при сохранении: работников={people_errors}, поездов={trains_errors}")
        except Exception as e:
            error_msg = str(e)
            print(f"Ошибка подключения к БД: {error_msg}")
            if "Connection refused" in error_msg or "connection to server" in error_msg.lower():
                print("PostgreSQL сервер недоступен. Проверьте:")
                print("   1. PostgreSQL сервер запущен (docker-compose ps)")
                print("   2. Переменные окружения POSTGRES_* установлены")
                print("   3. Сеть между контейнерами работает")
            print("Продолжаем без сохранения в БД...")
        
        return objects, output_path, {
            "statistics": stats,
            "processing_time": processing_time,
            "total_frames": total_frames,
            "processed_frames": processed_count
        }


-----

[Файл: c:\Hahatonn\XakatonAI\app\services\__init__.py]
[Размер: 84 байт]
[Дата изменения: 2025-11-29 22:30:02.836715]

"""
Сервисный слой для обработки детекций
"""


-----

[Файл: c:\Hahatonn\XakatonAI\src\attribute_models.py]
[Размер: 5403 байт]
[Дата изменения: 2025-11-29 22:15:22.256908]

"""
Модуль для детекции атрибутов (PPE и одежда) на рабочих
"""

from ultralytics import YOLO
import numpy as np
from typing import List, Tuple, Optional


# Маппинг ID классов PPE на имена
PPE_NAMES = {
    0: 'boots',
    1: 'gloves',
    2: 'helmet',
    3: 'noBoots',
    4: 'noGloves',
    5: 'noHelmet',
    6: 'noVest',
    7: 'vest'
    
}

# Маппинг ID классов одежды на имена
CLOTHES_NAMES = {
    3: 'hat',
    4: 'jacket',
    5: 'pants',
    6: 'shirt',
    7: 'shoe',
    8: 'shorts',
    10: 'sunglass'
}


def map_ppe_names(pairs: List[Tuple[int, float]]) -> List[str]:
    """
    Преобразует пары (class_id, confidence) в имена PPE
    
    Args:
        pairs: список кортежей (class_id, confidence)
    
    Returns:
        список имен PPE
    """
    return [PPE_NAMES.get(int(cid), f"ppe_{cid}") for cid, _ in pairs]


def map_clothes_names(pairs: List[Tuple[int, float]]) -> List[str]:
    """
    Преобразует пары (class_id, confidence) в имена одежды
    
    Args:
        pairs: список кортежей (class_id, confidence)
    
    Returns:
        список имен одежды
    """
    return [CLOTHES_NAMES.get(int(cid), f"cloth_{cid}") for cid, _ in pairs]


class AttributeModels:
    """
    Класс для работы с моделями детекции атрибутов (PPE и одежда)
    """
    
    def __init__(self, ppe_model_path: Optional[str] = None, 
                 clothes_model_path: Optional[str] = None,
                 device: str = "cpu", 
                 conf: float = 0.25):
        """
        Инициализация моделей атрибутов
        
        Args:
            ppe_model_path: путь к модели PPE (ppe_best.pt)
            clothes_model_path: путь к модели одежды (clothes_best.pt)
            device: устройство для обработки (cpu, cuda)
            conf: порог уверенности
        """
        self.device = device
        self.conf = conf
        
        # Загрузка модели PPE
        self.ppe = None
        if ppe_model_path:
            try:
                print(f"Загрузка модели PPE: {ppe_model_path}")
                self.ppe = YOLO(ppe_model_path)
                self.ppe.to(device)
                print(f"Модель PPE загружена успешно!")
            except Exception as e:
                print(f"Ошибка при загрузке модели PPE: {e}")
                self.ppe = None
        
        # Загрузка модели одежды
        self.clothes = None
        if clothes_model_path:
            try:
                print(f"Загрузка модели одежды: {clothes_model_path}")
                self.clothes = YOLO(clothes_model_path)
                self.clothes.to(device)
                print(f"Модель одежды загружена успешно!")
            except Exception as e:
                print(f"Ошибка при загрузке модели одежды: {e}")
                self.clothes = None
    
    def _infer(self, model, crop: np.ndarray) -> List[Tuple[int, float]]:
        """
        Выполняет инференс модели на обрезанном изображении
        
        Args:
            model: модель YOLO
            crop: обрезанное изображение (ROI)
        
        Returns:
            список кортежей (class_id, confidence)
        """
        if model is None:
            return []
        
        try:
            results = model(crop, device=self.device, conf=self.conf, verbose=False)
            if len(results) == 0:
                return []
            
            res = results[0]
            out = []
            for box in res.boxes:
                cls = int(box.cls[0].cpu().numpy()) if hasattr(box.cls[0], 'cpu') else int(box.cls[0])
                conf = float(box.conf[0].cpu().numpy()) if hasattr(box.conf[0], 'cpu') else float(box.conf[0])
                out.append((cls, conf))
            return out
        except Exception as e:
            return []
    
    def run_ppe(self, crop: np.ndarray) -> List[Tuple[int, float]]:
        """
        Детекция PPE на обрезанном изображении человека
        
        Args:
            crop: обрезанное изображение человека (ROI)
        
        Returns:
            список кортежей (class_id, confidence)
        """
        return self._infer(self.ppe, crop)
    
    def run_clothes(self, crop: np.ndarray) -> List[Tuple[int, float]]:
        """
        Детекция одежды на обрезанном изображении человека
        
        Args:
            crop: обрезанное изображение человека (ROI)
        
        Returns:
            список кортежей (class_id, confidence)
        """
        return self._infer(self.clothes, crop)
    
    def is_enabled(self) -> bool:
        """Проверяет, включены ли модели"""
        return self.ppe is not None or self.clothes is not None


-----

[Файл: c:\Hahatonn\XakatonAI\src\config_loader.py]
[Размер: 1194 байт]
[Дата изменения: 2025-11-29 22:15:33.179533]

"""
Модуль для загрузки конфигурации
"""

import os
import sys
import json



def load_config(config_path="config.json"):
    """Загрузка конфигурации"""
    # Получаем абсолютный путь к конфигу
    if not os.path.isabs(config_path):
        script_dir = os.path.dirname(os.path.abspath(__file__))
        # Поднимаемся на уровень выше (из src в корень)
        root_dir = os.path.dirname(script_dir)
        config_path = os.path.join(root_dir, config_path)
    
    if not os.path.exists(config_path):
        print(f"Ошибка: файл config.json не найден!")
        print(f"Ожидался путь: {config_path}")
        sys.exit(1)
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except json.JSONDecodeError as e:
        print(f"Ошибка при парсинге config.json: {e}")
        print(f"Проверьте синтаксис JSON файла")
        sys.exit(1)
    except Exception as e:
        print(f"Ошибка при загрузке config.json: {e}")
        sys.exit(1)

-----

[Файл: c:\Hahatonn\XakatonAI\src\db.py]
[Размер: 5124 байт]
[Дата изменения: 2025-11-30 01:02:52.004028]

"""
Модуль для работы с PostgreSQL базой данных
"""
import os
import psycopg2
from psycopg2 import OperationalError, IntegrityError
from psycopg2.extras import RealDictCursor, Json
from typing import Optional, Dict, Any, List


def get_conn():
    """Создает подключение к PostgreSQL базе данных"""
    try:
        return psycopg2.connect(
            dbname=os.getenv("POSTGRES_DB", "xakaton"),
            user=os.getenv("POSTGRES_USER", "postgres"),
            password=os.getenv("POSTGRES_PASSWORD", "12345"),
            host=os.getenv("POSTGRES_HOST", "localhost"),
            port=int(os.getenv("POSTGRES_PORT", "5432"))
        )
    except OperationalError as e:
        raise Exception(f"Ошибка подключения к базе данных: {e}")


def insert_worker(
    track_id: int,
    color: Optional[str] = None,
    stand_frames: int = 0,
    walk_frames: int = 0,
    work_frames: int = 0,
    attributes: Optional[Dict[str, List[str]]] = None
):
    """
    Вставляет или обновляет данные о работнике в таблицу workers
    
    Args:
        track_id: ID трека
        color: цвет одежды (строка, разделенная запятыми)
        stand_frames: количество кадров в состоянии STAY
        walk_frames: количество кадров в состоянии GO
        work_frames: количество кадров в состоянии WORK
        attributes: словарь с атрибутами (PPE и одежда)
    """
    conn = None
    cur = None
    try:
        conn = get_conn()
        cur = conn.cursor()
        
        # Преобразуем attributes в JSONB
        attributes_json = Json(attributes) if attributes else None
        
        # Проверяем, существует ли запись
        cur.execute("SELECT id FROM workers WHERE track_id = %s", (track_id,))
        exists = cur.fetchone()
        
        if exists:
            # Обновляем существующую запись
            cur.execute("""
                UPDATE workers 
                SET color = %s, 
                    stand_frames = %s, 
                    walk_frames = %s, 
                    work_frames = %s,
                    attributes = %s
                WHERE track_id = %s
            """, (color, stand_frames, walk_frames, work_frames, attributes_json, track_id))
        else:
            # Вставляем новую запись
            cur.execute("""
                INSERT INTO workers (track_id, color, stand_frames, walk_frames, work_frames, attributes)
                VALUES (%s, %s, %s, %s, %s, %s)
            """, (track_id, color, stand_frames, walk_frames, work_frames, attributes_json))
        
        conn.commit()
    except IntegrityError as e:
        if conn:
            conn.rollback()
        raise Exception(f"Ошибка при сохранении работника: {e}")
    except Exception as e:
        if conn:
            conn.rollback()
        raise Exception(f"Ошибка при вставке работника: {e}")
    finally:
        if cur:
            cur.close()
        if conn:
            conn.close()


def insert_train(
    track_id: int,
    number: Optional[str] = None,
    total_time: int = 0
):
    """
    Вставляет или обновляет данные о поезде в таблицу trains
    
    Args:
        track_id: ID трека
        number: номер поезда (строка)
        total_time: общее время в кадрах
    """
    conn = None
    cur = None
    try:
        conn = get_conn()
        cur = conn.cursor()
        
        # Проверяем, существует ли запись
        cur.execute("SELECT id FROM trains WHERE track_id = %s", (track_id,))
        exists = cur.fetchone()
        
        if exists:
            # Обновляем существующую запись
            cur.execute("""
                UPDATE trains 
                SET number = %s, 
                    total_time = %s
                WHERE track_id = %s
            """, (number, total_time, track_id))
        else:
            # Вставляем новую запись
            cur.execute("""
                INSERT INTO trains (track_id, number, total_time)
                VALUES (%s, %s, %s)
            """, (track_id, number, total_time))
        
        conn.commit()
    except IntegrityError as e:
        if conn:
            conn.rollback()
        raise Exception(f"Ошибка при сохранении поезда: {e}")
    except Exception as e:
        if conn:
            conn.rollback()
        raise Exception(f"Ошибка при вставке поезда: {e}")
    finally:
        if cur:
            cur.close()
        if conn:
            conn.close()


-----

[Файл: c:\Hahatonn\XakatonAI\src\detector.py]
[Размер: 13492 байт]
[Дата изменения: 2025-11-29 22:15:35.542380]

"""
Модуль для детекции объектов с использованием YOLO11
"""

import sys
import cv2
from ultralytics import YOLO



class YOLODetector:
    """Класс для детекции объектов с использованием YOLO11"""
    
    CLASSES = {
        0: "person",
        6: "train"
    }
    
    DEFAULT_COLORS = {
        0: (0, 255, 0),  # Зеленый для людей (BGR)
        6: (255, 0, 0),  # Синий для поездов (BGR)
    }
    
    def __init__(self, model_path="yolo11m.pt", conf_threshold=0.5, device="cpu",
                 custom_colors=None, half_precision=False):
        """
        Инициализация детектора YOLO11
        
        Args:
            model_path: путь к модели YOLO11 (yolo11n.pt - nano, самый быстрый)
            conf_threshold: порог уверенности
            device: устройство для обработки (cpu, cuda и т.д.)
            custom_colors: словарь пользовательских цветов
            half_precision: использовать половинную точность
        """
        self.conf_threshold = conf_threshold
        self.device = device
        self.half_precision = half_precision
        
        print(f"Загрузка модели YOLO11: {model_path}")
        
        try:
            self.model = YOLO(model_path)
            print(f"Модель загружена успешно!")
            print(f"Используется устройство: {device.upper()}")
        except Exception as e:
            print(f"Ошибка при загрузке модели: {e}")
            print("Модель будет автоматически скачана при первом запуске")
            sys.exit(1)
        
        # Подготовка цветов
        self.colors = dict(self.DEFAULT_COLORS)
        if custom_colors:
            self._apply_custom_colors(custom_colors)
    
    def _apply_custom_colors(self, colors_config):
        """Применение пользовательских цветов"""
        for key, value in colors_config.items():
            class_id = self._resolve_class_id(key)
            color_tuple = self._parse_color(value)
            
            if class_id is None:
                print(f"Предупреждение: неизвестный класс '{key}' в настройках цветов. Пропущено.")
                continue
            if color_tuple is None:
                print(f"Предупреждение: некорректный цвет для '{key}'. Ожидался формат [B,G,R]. Пропущено.")
                continue
            
            self.colors[class_id] = color_tuple
    
    @classmethod
    def _resolve_class_id(cls, key):
        """Определение ID класса по имени или числу"""
        if isinstance(key, int):
            return key if key in cls.CLASSES else None
        if isinstance(key, str):
            key = key.strip().lower()
            if key.isdigit():
                cid = int(key)
                return cid if cid in cls.CLASSES else None
            for cid, name in cls.CLASSES.items():
                if name.lower() == key:
                    return cid
        return None
    
    @staticmethod
    def _parse_color(value):
        """Преобразование цвета в кортеж BGR"""
        if isinstance(value, (list, tuple)) and len(value) == 3:
            try:
                b, g, r = [int(max(0, min(255, v))) for v in value]
                return (b, g, r)
            except (ValueError, TypeError):
                return None
        return None
    
    def detect(self, frame, target_classes=None):
        """
        Детекция объектов на кадре
        
        Args:
            frame: изображение (numpy array)
            target_classes: список классов для детекции (None = все)
            
        Returns:
            список детекций: [(class_id, confidence, x1, y1, x2, y2), ...]
        """
        # Запускаем детекцию
        results = self.model(
            frame,
            conf=self.conf_threshold,
            device=self.device,
            verbose=False,
            half=self.half_precision
        )
        
        detections = []
        if len(results) > 0:
            boxes = results[0].boxes
            for box in boxes:
                class_id = int(box.cls[0])
                confidence = float(box.conf[0])
                
                # Фильтруем только нужные классы
                if target_classes is None or class_id in target_classes:
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    detections.append((
                        class_id,
                        confidence,
                        int(x1),
                        int(y1),
                        int(x2),
                        int(y2)
                    ))
        
        return detections
    
    def draw_detections(self, frame, detections, show_track_ids=False, train_numbers=None):
        """
        Отрисовка детекций на кадре
        
        Args:
            frame: кадр для отрисовки
            detections: список детекций
                - Без трекинга: [(class_id, confidence, x1, y1, x2, y2), ...]
                - С трекингом: [(track_id, class_id, confidence, x1, y1, x2, y2), ...]
            show_track_ids: показывать ли ID треков (если True, ожидается формат с track_id)
            train_numbers: словарь {track_id: train_number} для отображения номеров поездов
        """
        result_frame = frame.copy()
        
        for det in detections:
            # Определяем формат детекции
            if show_track_ids and len(det) == 7:
                # Формат с трекингом: (track_id, class_id, confidence, x1, y1, x2, y2)
                track_id, class_id, confidence, x1, y1, x2, y2 = det
            elif len(det) == 6:
                # Формат без трекинга: (class_id, confidence, x1, y1, x2, y2)
                class_id, confidence, x1, y1, x2, y2 = det
                track_id = None
            else:
                continue
            
            color = self.colors.get(class_id, (0, 0, 255))
            
            # Рисуем прямоугольник
            cv2.rectangle(result_frame, (x1, y1), (x2, y2), color, 2)
            
            # Формируем подпись
            class_name = self.CLASSES.get(class_id, 'unknown')
            if show_track_ids and track_id is not None:
                label = f"ID:{track_id} {class_name}"
                # Добавляем номер поезда, если есть (отображаем так же, как в отладке)
                if train_numbers and track_id in train_numbers and train_numbers[track_id]:
                    label += f" {train_numbers[track_id]}"
                else:
                    label += f" {confidence:.2f}"
            else:
                label = f"{class_name}: {confidence:.2f}"
            
            label_size, baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)
            label_height = label_size[1] + baseline
            
            # Размещаем текст сверху рамки (выше y1)
            # Если рамка слишком близко к верху экрана, размещаем внутри, но стараемся сверху
            label_y = y1 - 5  # 5 пикселей выше верхней границы рамки
            
            # Если текст выходит за верхний край экрана, размещаем его внутри рамки
            if label_y < label_height:
                label_y = y1 + label_height + 5
            
            # Фон для текста (прямоугольник за текстом)
            bg_y1 = label_y - label_height
            bg_y2 = label_y
            cv2.rectangle(result_frame, (x1, bg_y1),
                         (x1 + label_size[0], bg_y2), color, -1)
            
            # Текст
            cv2.putText(result_frame, label, (x1, label_y - baseline),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
        
        return result_frame
    
    def draw_objects(self, frame, objects_info):
        """
        Отрисовка объектов на кадре с полной информацией
        
        Args:
            frame: кадр для отрисовки
            objects_info: список словарей с информацией об объектах
                [{'track_id': int, 'object_id': int, 'object_type': str, 
                  'bbox': (x1, y1, x2, y2), 'status': str, 
                  'train_number': str, 'frame_count': int, 'attributes': dict}, ...]
        """
        result_frame = frame.copy()
        
        # Рисуем вертикальную линию посередине экрана (для определения WORK)
        h, w = frame.shape[:2]
        middle_x = w // 2
        cv2.line(result_frame, (middle_x, 0), (middle_x, h), (255, 255, 0), 2)  # Желтая линия
        
        for obj_info in objects_info:
            track_id = obj_info.get('track_id')
            object_id = obj_info.get('object_id')
            object_type = obj_info.get('object_type', 'unknown')
            class_id = obj_info.get('class_id', 0)
            bbox = obj_info.get('bbox')
            status = obj_info.get('status', 'unknown')
            train_number = obj_info.get('train_number')
            frame_count = obj_info.get('frame_count', 0)
            attributes = obj_info.get('attributes', {})
            
            if not bbox:
                continue
            
            x1, y1, x2, y2 = bbox
            color = self.colors.get(class_id, (0, 0, 255))
            
            # Рисуем прямоугольник
            cv2.rectangle(result_frame, (x1, y1), (x2, y2), color, 2)
            
            # Формируем подпись: только тип, ID и статус (без профессии)
            label_parts = [f"{object_type.upper()}#{object_id}"]
            
            # Добавляем статус
            if status != 'unknown':
                label_parts.append(f"[{status}]")
            
            label = " ".join(label_parts)
            
            label_size, baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)
            label_height = label_size[1] + baseline
            
            # Размещаем текст сверху рамки
            label_y = y1 - 5
            if label_y < label_height:
                label_y = y1 + label_height + 5
            
            # Фон для текста
            bg_y1 = label_y - label_height
            bg_y2 = label_y
            cv2.rectangle(result_frame, (x1, bg_y1),
                         (x1 + label_size[0], bg_y2), color, -1)
            
            # Текст
            cv2.putText(result_frame, label, (x1, label_y - baseline),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
            
            # Отрисовка атрибутов (PPE и одежда) для людей
            if object_type == "person" and attributes:
                ppe_list = attributes.get("ppe", [])
                clothes_list = attributes.get("clothes", [])
                all_items = ppe_list + clothes_list
                
                if all_items:
                    # Формируем строку с атрибутами
                    attrs_text = ", ".join(all_items[:5])  # Ограничиваем до 5 элементов для читаемости
                    if len(all_items) > 5:
                        attrs_text += "..."
                    
                    # Размер текста атрибутов (меньше основного)
                    attrs_size, attrs_baseline = cv2.getTextSize(attrs_text, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)
                    attrs_height = attrs_size[1] + attrs_baseline
                    
                    # Размещаем атрибуты под основной подписью
                    attrs_y = label_y + attrs_height + 5
                    
                    # Фон для текста атрибутов (светло-серый)
                    attrs_bg_y1 = attrs_y - attrs_height
                    attrs_bg_y2 = attrs_y
                    cv2.rectangle(result_frame, (x1, attrs_bg_y1),
                                 (x1 + attrs_size[0], attrs_bg_y2), (100, 100, 100), -1)
                    
                    # Текст атрибутов
                    cv2.putText(result_frame, attrs_text, (x1, attrs_y - attrs_baseline),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
        
        return result_frame

-----

[Файл: c:\Hahatonn\XakatonAI\src\filters.py]
[Размер: 24546 байт]
[Дата изменения: 2025-11-29 22:15:37.665672]

"""
Модуль для цветовых фильтров детекций
"""

import cv2
import numpy as np
from src.image_utils import extract_roi



def parse_color_range(range_values):
    """Подготовка нижних и верхних границ цвета"""
    if not isinstance(range_values, (list, tuple)) or len(range_values) != 3:
        return None
    try:
        return np.array([int(max(0, min(255, v))) for v in range_values], dtype=np.uint8)
    except (ValueError, TypeError):
        return None


def resolve_filter_class_id(class_key, class_map):
    """Получение ID класса по имени или числу"""
    if isinstance(class_key, int):
        return class_key if class_key in class_map else None
    if isinstance(class_key, str):
        key = class_key.strip().lower()
        if key.isdigit():
            cid = int(key)
            return cid if cid in class_map else None
        for cid, name in class_map.items():
            if name.lower() == key:
                return cid
    return None


def passes_color_filter(roi, cfg):
    """Проверяет, удовлетворяет ли ROI заданным цветовым фильтрам"""
    total_pixels = roi.shape[0] * roi.shape[1]
    if total_pixels == 0:
        return False, {"reason": "empty_roi"}
    
    info = {}
    positive_mask = None
    
    # RGB диапазон
    if "min_rgb" in cfg and "max_rgb" in cfg:
        lower_rgb = parse_color_range(cfg["min_rgb"])
        upper_rgb = parse_color_range(cfg["max_rgb"])
        if lower_rgb is not None and upper_rgb is not None:
            rgb_mask = cv2.inRange(roi, lower_rgb, upper_rgb)
            positive_mask = rgb_mask if positive_mask is None else cv2.bitwise_and(positive_mask, rgb_mask)
    
    # HSV диапазон
    if "min_hsv" in cfg and "max_hsv" in cfg:
        lower_hsv = parse_color_range(cfg["min_hsv"])
        upper_hsv = parse_color_range(cfg["max_hsv"])
        if lower_hsv is not None and upper_hsv is not None:
            hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
            hsv_mask = cv2.inRange(hsv_roi, lower_hsv, upper_hsv)
            positive_mask = hsv_mask if positive_mask is None else cv2.bitwise_and(positive_mask, hsv_mask)
    
    positive_threshold = float(cfg.get("match_threshold", 0.1))
    positive_threshold = max(0.0, min(1.0, positive_threshold))
    
    if positive_mask is not None:
        match_ratio = cv2.countNonZero(positive_mask) / total_pixels
        info["match_ratio"] = match_ratio
        info["match_threshold"] = positive_threshold
        if match_ratio < positive_threshold:
            info["reason"] = "positive_miss"
            return False, info
    
    # Anti-color фильтр в HSV
    anti_threshold = float(cfg.get("anti_match_threshold", cfg.get("match_threshold", 0.1)))
    anti_threshold = max(0.0, min(1.0, anti_threshold))
    info["anti_threshold"] = anti_threshold
    
    anti_ratio = None
    if "anti_color_hsv" in cfg:
        center = parse_color_range(cfg["anti_color_hsv"])
        if center is not None:
            range_val = int(cfg.get("anti_color_range", 20))
            hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
            lower = np.array([
                max(0, center[0] - range_val),
                max(0, center[1] - range_val),
                max(0, center[2] - range_val)
            ], dtype=np.uint8)
            upper = np.array([
                min(180, center[0] + range_val),
                min(255, center[1] + range_val),
                min(255, center[2] + range_val)
            ], dtype=np.uint8)
            anti_mask = cv2.inRange(hsv_roi, lower, upper)
            anti_ratio = cv2.countNonZero(anti_mask) / total_pixels
    
    if "anti_color_rgb" in cfg and anti_ratio is None:
        center = parse_color_range(cfg["anti_color_rgb"])
        if center is not None:
            range_val = int(cfg.get("anti_color_range", 20))
            lower = np.array([
                max(0, center[0] - range_val),
                max(0, center[1] - range_val),
                max(0, center[2] - range_val)
            ], dtype=np.uint8)
            upper = np.array([
                min(255, center[0] + range_val),
min(255, center[1] + range_val),
                min(255, center[2] + range_val)
            ], dtype=np.uint8)
            anti_mask = cv2.inRange(roi, lower, upper)
            anti_ratio = cv2.countNonZero(anti_mask) / total_pixels
    
    if anti_ratio is not None:
        info["anti_ratio"] = anti_ratio
        if anti_ratio >= anti_threshold:
            info["reason"] = "anti_match"
            return False, info
    
    info["reason"] = "pass"
    return True, info


def apply_color_filters(frame, detections, filters_cfg, class_map, debug_cfg=None):
    """
    Применяет цветовые фильтры и возвращает отфильтрованные и отклонённые детекции
    """
    if not filters_cfg or not filters_cfg.get("enabled", False):
        return detections, []
    
    filtered = []
    rejected = []
    log_details = bool(debug_cfg.get("log_detection_details")) if debug_cfg else False
    
    for det in detections:
        class_id, conf, x1, y1, x2, y2 = det
        class_name = class_map.get(class_id, str(class_id))
        filter_cfg = None
        
        # Поиск конфигурации фильтра по имени или ID
        for key, cfg in filters_cfg.items():
            if key == "enabled":
                continue
            cid = resolve_filter_class_id(key, class_map)
            if cid is not None and cid == class_id:
                filter_cfg = cfg
                break
        
        if not filter_cfg:
            filtered.append(det)
            continue
        
        roi = extract_roi(frame, x1, y1, x2, y2)
        if roi is None:
            rejected.append({
                "det": det,
                "info": {"reason": "empty_roi"}
            })
            continue
        
        passed, info = passes_color_filter(roi, filter_cfg)
        
        # Дополнительная проверка: для поездов требуется наличие красного цвета
        if passed and class_name == "train" and filter_cfg.get("require_red_color", False):
            # Используем настройки lighting_compensation из config, если они есть
            lighting_compensation = None
            if debug_cfg:
                # Получаем настройки из config через debug_cfg, если они переданы
                # Или используем стандартные настройки для лучшего определения красного
                lighting_compensation = {"enabled": True, "normalize_brightness": False, "wider_color_ranges": True}
            
            color_info = detect_dominant_color(roi, top_n=3, lighting_compensation=lighting_compensation)
            all_percentages = color_info.get("all_percentages", {})
            top_colors = color_info.get("top_colors", [])
            
            # Проверяем наличие красного цвета (red и red2 объединяются в red)
            red_percentage = all_percentages.get("red", 0.0)
            # Также проверяем в top_colors
            for color_item in top_colors:
                if color_item.get("name") == "red":
                    red_percentage = max(red_percentage, color_item.get("percentage", 0.0))
            
            min_red_threshold = filter_cfg.get("min_red_threshold", 0.1)
            if red_percentage < min_red_threshold:
                passed = False
                info["reason"] = "no_red_color"
                info["red_percentage"] = red_percentage
                info["min_red_threshold"] = min_red_threshold
        
        # Фильтр для поездов: требуется красный цвет >= 20% и синий цвет <= 5%
        if passed and class_name == "train":
            # Используем настройки lighting_compensation для лучшего определения цветов
            lighting_compensation = None
            if debug_cfg:
                lighting_compensation = {"enabled": True, "normalize_brightness": False, "wider_color_ranges": True}
            
            color_info = detect_dominant_color(roi, top_n=5, lighting_compensation=lighting_compensation)
            all_percentages = color_info.get("all_percentages", {})
            top_colors = color_info.get("top_colors", [])
            
            # Проверяем процент красного цвета (должен быть >= 20%)
            red_percentage = all_percentages.get("red", 0.0)
            # Также проверяем в top_colors
            for color_item in top_colors:
                if color_item.get("name") == "red":
                    red_percentage = max(red_percentage, color_item.get("percentage", 0.0))
            
            min_red_threshold = 0.2  # 20%
            if red_percentage < min_red_threshold:
                passed = False
                info["reason"] = "insufficient_red_color"
                info["red_percentage"] = red_percentage
                info["min_red_threshold"] = min_red_threshold
            
            # Проверяем процент синего и голубого цвета (должен быть <= 5%)
            if passed:
                blue_percentage = all_percentages.get("blue", 0.0)
                cyan_percentage = all_percentages.get("cyan", 0.0)
                
                # Также проверяем в top_colors
                for color_item in top_colors:
                    color_name = color_item.get("name", "")
                    if color_name == "blue":
                        blue_percentage = max(blue_percentage, color_item.get("percentage", 0.0))
                    elif color_name == "cyan":
                        cyan_percentage = max(cyan_percentage, color_item.get("percentage", 0.0))
                
                # Суммируем синий и голубой цвета
                total_blue_cyan = blue_percentage + cyan_percentage
                max_blue_threshold = 0.05  # 5%
                
                if total_blue_cyan > max_blue_threshold:
                    passed = False
                    info["reason"] = "too_much_blue"
                    info["blue_percentage"] = total_blue_cyan
                    info["max_blue_threshold"] = max_blue_threshold
        
        if log_details:
            reason = info.get("reason", "pass")
            match_ratio = info.get("match_ratio")
            anti_ratio = info.get("anti_ratio")
            details = []
            if match_ratio is not None:
                details.append(f"match={match_ratio:.2f}/{info.get('match_threshold', '-')}")
            if anti_ratio is not None:
                details.append(f"anti={anti_ratio:.2f}/{info.get('anti_threshold', '-')}")
            detail_text = " | ".join(details)
            status = "PASS" if passed else f"FILTERED({reason})"
            print(f"[DEBUG] {class_name} ({conf:.2f}) -> {status} {detail_text}")
        
        if passed:
            filtered.append(det)
        else:
            rejected.append({
                "det": det,
                "info": info
            })
    
    return filtered, rejected


def detect_dominant_color(roi, top_n=2, lighting_compensation=None):
    """
    Определяет несколько преобладающих цветов объекта
    
    Args:
        roi: Область интереса (numpy array BGR)
        top_n: Количество цветов для возврата (по умолчанию 2)
    
    Returns:
        dict: {
            "top_colors": [  # Список топ цветов, отсортированных по проценту
                {"name": "red", "percentage": 0.45},
                {"name": "blue", "percentage": 0.30}
            ],
            "color_name": "red",  # Основной цвет (для обратной совместимости)
            "color_percentage": 0.45,  # Процент основного цвета
            "bgr_avg": [100, 50, 200]  # Средний BGR цвет
        }
    """
    if roi is None or roi.size == 0:
        return {"color_name": "unknown", "color_percentage": 0.0}
    
    total_pixels = roi.shape[0] * roi.shape[1]
    if total_pixels == 0:
        return {"color_name": "unknown", "color_percentage": 0.0}
    
    # Преобразуем в HSV для более точного определения цвета
    hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
    
    # Компенсация освещения: нормализация яркости
    if lighting_compensation and lighting_compensation.get("normalize_brightness", False):
        # Вычисляем среднюю яркость
        v_channel = hsv_roi[:, :, 2].astype(np.float32)
        avg_v = np.mean(v_channel)
        
        # Нормализуем яркость к среднему значению (128)
        if avg_v > 0:
            scale_factor = 128.0 / avg_v
            v_channel_normalized = np.clip(v_channel * scale_factor, 0, 255).astype(np.uint8)
            hsv_roi = hsv_roi.copy()
            hsv_roi[:, :, 2] = v_channel_normalized
    
    # Определяем основные цвета в HSV
    # Используем более широкие диапазоны для устойчивости к освещению
    wider_ranges = lighting_compensation and lighting_compensation.get("wider_color_ranges", False)
    
    if wider_ranges:
        # Более широкие диапазоны, но с минимальной насыщенностью выше порога серых
        # чтобы гарантировать, что серые не попадут в цветные диапазоны
        sat_min = 45  # Выше порога серых (40), но ниже стандартного (50)
        val_min = 30  # Вместо 50
        color_ranges = [
            ("red", ([0, sat_min, val_min], [10, 255, 255])),  # Красный (0-10)
            ("red2", ([170, sat_min, val_min], [180, 255, 255])),  # Красный (170-180)
            ("orange", ([11, sat_min, val_min], [25, 255, 255])),  # Оранжевый
            ("yellow", ([26, sat_min, val_min], [35, 255, 255])),  # Желтый
            ("green", ([36, sat_min, val_min], [85, 255, 255])),  # Зеленый
            ("cyan", ([86, sat_min, val_min], [100, 255, 255])),  # Голубой
            ("blue", ([101, sat_min, val_min], [130, 255, 255])),  # Синий
            ("purple", ([131, sat_min, val_min], [169, 255, 255])),  # Фиолетовый
        ]
    else:
        # Стандартные диапазоны (насыщенность 50, что выше порога серых 40)
        color_ranges = [
            ("red", ([0, 50, 50], [10, 255, 255])),  # Красный (0-10)
            ("red2", ([170, 50, 50], [180, 255, 255])),  # Красный (170-180)
            ("orange", ([11, 50, 50], [25, 255, 255])),  # Оранжевый
            ("yellow", ([26, 50, 50], [35, 255, 255])),  # Желтый
            ("green", ([36, 50, 50], [85, 255, 255])),  # Зеленый
            ("cyan", ([86, 50, 50], [100, 255, 255])),  # Голубой
            ("blue", ([101, 50, 50], [130, 255, 255])),  # Синий
            ("purple", ([131, 50, 50], [169, 255, 255])),  # Фиолетовый
        ]
    
    # Также проверяем яркость для белого/черного/серого
    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
    avg_brightness = np.mean(gray)
    
    color_percentages = {}
    
    # СНАЧАЛА определяем оттенки серого, чтобы исключить их из цветных диапазонов
    # Используем более строгий порог насыщенности для серых (40 вместо 30)
    # чтобы гарантировать, что серые пиксели не попадут в цветные диапазоны
    gray_saturation_threshold = 40
# Маска для всех серых оттенков (низкая насыщенность)
    gray_mask_all = hsv_roi[:, :, 1] < gray_saturation_threshold
    
    # Определяем оттенки серого как отдельные категории
    # Светло-серый (высокая яркость, низкая насыщенность)
    light_gray_mask = gray_mask_all & (hsv_roi[:, :, 2] > 180) & (hsv_roi[:, :, 2] <= 240)
    light_gray_pixels = np.sum(light_gray_mask)
    if light_gray_pixels / total_pixels > 0.05:
        color_percentages["light_gray"] = light_gray_pixels / total_pixels
    
    # Темно-серый (низкая яркость, низкая насыщенность)
    dark_gray_mask = gray_mask_all & (hsv_roi[:, :, 2] >= 50) & (hsv_roi[:, :, 2] < 120)
    dark_gray_pixels = np.sum(dark_gray_mask)
    if dark_gray_pixels / total_pixels > 0.05:
        color_percentages["dark_gray"] = dark_gray_pixels / total_pixels
    
    # Средний серый
    gray_mask = gray_mask_all & (hsv_roi[:, :, 2] >= 120) & (hsv_roi[:, :, 2] <= 180)
    gray_pixels = np.sum(gray_mask)
    if gray_pixels / total_pixels > 0.05:
        color_percentages["gray"] = gray_pixels / total_pixels
    
    # Белый (очень высокая яркость, очень низкая насыщенность)
    white_mask = (hsv_roi[:, :, 1] < 20) & (hsv_roi[:, :, 2] > 240)
    white_pixels = np.sum(white_mask)
    if white_pixels / total_pixels > 0.05:
        color_percentages["white"] = white_pixels / total_pixels
    
    # Черный (очень низкая яркость, очень низкая насыщенность)
    black_mask = (hsv_roi[:, :, 1] < 20) & (hsv_roi[:, :, 2] < 50)
    black_pixels = np.sum(black_mask)
    if black_pixels / total_pixels > 0.05:
        color_percentages["black"] = black_pixels / total_pixels
    
    # Объединяем все серые маски для исключения из цветных диапазонов
    # Включаем ВСЕ пиксели с низкой насыщенностью, даже если они не попали в конкретные категории
    all_gray_mask = gray_mask_all | white_mask | black_mask
    
    # Проверяем каждый цвет, ИСКЛЮЧАЯ серые пиксели
    for color_name, (lower, upper) in color_ranges:
        lower_hsv = np.array(lower, dtype=np.uint8)
        upper_hsv = np.array(upper, dtype=np.uint8)
        color_mask = cv2.inRange(hsv_roi, lower_hsv, upper_hsv)
        # Исключаем серые пиксели из цветной маски
        color_mask = color_mask & (~all_gray_mask.astype(np.uint8) * 255)
        percentage = cv2.countNonZero(color_mask) / total_pixels
        color_percentages[color_name] = percentage
    
    # Объединяем красные диапазоны
    if "red" in color_percentages and "red2" in color_percentages:
        color_percentages["red"] = color_percentages["red"] + color_percentages["red2"]
        del color_percentages["red2"]
    
    # Сортируем цвета по проценту (от большего к меньшему)
    if not color_percentages:
        top_colors = []
        dominant_color = "unknown"
        max_percentage = 0.0
    else:
        # Фильтруем цвета с минимальным процентом (больше 5%)
        filtered_colors = {k: v for k, v in color_percentages.items() if v > 0.05}
        
        # Сортируем по проценту
        sorted_colors = sorted(filtered_colors.items(), key=lambda x: x[1], reverse=True)
        
        # Берем топ-N цветов
        top_colors = [
            {"name": name, "percentage": pct}
            for name, pct in sorted_colors[:top_n]
        ]
        
        # Для обратной совместимости оставляем основной цвет
        if top_colors:
            dominant_color = top_colors[0]["name"]
            max_percentage = top_colors[0]["percentage"]
        else:
            dominant_color = "unknown"
            max_percentage = 0.0
    
    # Вычисляем средний BGR цвет
    avg_bgr = np.mean(roi, axis=(0, 1)).astype(int)
    
    return {
        "top_colors": top_colors,
        "color_name": dominant_color,  # Для обратной совместимости
        "color_percentage": max_percentage,  # Для обратной совместимости
        "bgr_avg": avg_bgr.tolist(),
        "all_percentages": color_percentages  # Все проценты для отладки
    }
def get_color_for_detection(frame, detection, top_n=2, crop_border_ratio=0.15, excluded_colors=None, lighting_compensation=None):
    """
    Определяет преобладающие цвета для конкретной детекции
    
    Args:
        frame: Кадр видео
        detection: (class_id, confidence, x1, y1, x2, y2)
        top_n: Количество цветов для возврата (по умолчанию 2)
        crop_border_ratio: Коэффициент обрезки краев ROI (0.0-0.5)
        excluded_colors: Список запрещенных цветов для этого класса (например, ["green"])
        lighting_compensation: Настройки компенсации освещения
    
    Returns:
        dict: Информация о преобладающих цветах (без запрещенных цветов)
    """
    class_id, conf, x1, y1, x2, y2 = detection
    roi = extract_roi(frame, x1, y1, x2, y2, crop_border_ratio=crop_border_ratio)
    color_info = detect_dominant_color(roi, top_n=top_n, lighting_compensation=lighting_compensation)
    
    # Фильтруем запрещенные цвета
    if excluded_colors and isinstance(excluded_colors, list):
        excluded_colors_lower = [c.lower() for c in excluded_colors]
        
        # Фильтруем из top_colors
        if "top_colors" in color_info:
            filtered_top_colors = [
                color_item for color_item in color_info["top_colors"]
                if color_item.get("name", "").lower() not in excluded_colors_lower
            ]
            color_info["top_colors"] = filtered_top_colors
            
            # Обновляем основной цвет (для обратной совместимости)
            if filtered_top_colors:
                color_info["color_name"] = filtered_top_colors[0]["name"]
                color_info["color_percentage"] = filtered_top_colors[0]["percentage"]
            else:
                # Если все цвета были исключены, возвращаем unknown
                color_info["color_name"] = "unknown"
                color_info["color_percentage"] = 0.0
        
        # Также удаляем из all_percentages для отладки
        if "all_percentages" in color_info:
            for excluded_color in excluded_colors_lower:
                if excluded_color in color_info["all_percentages"]:
                    del color_info["all_percentages"][excluded_color]
    
    return color_info


def annotate_rejected(frame, rejected, detector, color=(0, 0, 255)):
    """Возвращает кадр с выделенными отклонёнными объектами"""
    overlay = frame.copy()
    for item in rejected:
        det = item.get("det")
        info = item.get("info", {})
        if not det:
            continue
        class_id, _, x1, y1, x2, y2 = det
        class_name = detector.CLASSES.get(class_id, f"class_{class_id}")
        reason = info.get("reason", "filtered")
        label = f"{class_name} [{reason}]"
        cv2.rectangle(overlay, (x1, y1), (x2, y2), color, 2)
        cv2.putText(overlay, label, (x1, max(15, y1 - 10)),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
    return overlay
-----

[Файл: c:\Hahatonn\XakatonAI\src\image_utils.py]
[Размер: 2889 байт]
[Дата изменения: 2025-11-29 22:15:40.480737]

import cv2
import numpy as np
import os

def imread_unicode(image_path):
    """Чтение изображения с поддержкой кириллицы в пути"""
    image_path = os.path.normpath(image_path)
    try:
        with open(image_path, "rb") as f:
            bytes_data = bytearray(f.read())
        numpy_array = np.asarray(bytes_data, dtype=np.uint8)
        image = cv2.imdecode(numpy_array, cv2.IMREAD_COLOR)
        return image
    except Exception as e:
        pass
    try:
        return cv2.imread(image_path)
    except:
        return None


def resize_frame(frame, max_width, max_height, maintain_aspect=True, keep_width_native=False):
    """Изменение размера кадра с сохранением пропорций"""
    height, width = frame.shape[:2]
    
    if keep_width_native:
        if max_height and height > max_height:
            new_height = max_height
            resized = cv2.resize(frame, (width, new_height), interpolation=cv2.INTER_AREA)
            scale = new_height / height
            return resized, scale
        return frame, 1.0
    
    if (max_width == 0 or max_width is None) and (max_height == 0 or max_height is None):
        return frame, 1.0
    
    if max_width == 0: max_width = width
    if max_height == 0: max_height = height
    
    if width <= max_width and height <= max_height:
        return frame, 1.0
    
    if maintain_aspect:
        scale = min(max_width / width, max_height / height)
        new_width = int(width * scale)
        new_height = int(height * scale)
    else:
        new_width = max_width
        new_height = max_height
        scale = min(max_width / width, max_height / height)
    
    resized = cv2.resize(frame, (new_width, new_height), interpolation=cv2.INTER_AREA)
    return resized, scale

def extract_roi(frame, x1, y1, x2, y2, crop_border_ratio=0.0):
    """
    Вырезает область интереса
    
    Args:
        frame: кадр изображения
        x1, y1, x2, y2: координаты области
        crop_border_ratio: коэффициент обрезки краев (0.0-0.5), чтобы исключить края детекции
    """
    h, w = frame.shape[:2]
    
    # Применяем обрезку краев, если указано
    if crop_border_ratio > 0.0 and crop_border_ratio < 0.5:
        width = x2 - x1
        height = y2 - y1
        crop_x = int(width * crop_border_ratio)
        crop_y = int(height * crop_border_ratio)
        x1 = x1 + crop_x
        y1 = y1 + crop_y
        x2 = x2 - crop_x
        y2 = y2 - crop_y
    
    # Проверяем границы
    x1 = max(0, min(w - 1, x1))
    x2 = max(0, min(w, x2))
    y1 = max(0, min(h - 1, y1))
    y2 = max(0, min(h, y2))
    
    if x2 <= x1 or y2 <= y1:
        return None
    return frame[y1:y2, x1:x2]
-----

[Файл: c:\Hahatonn\XakatonAI\src\object_manager.py]
[Размер: 13863 байт]
[Дата изменения: 2025-11-29 22:15:43.437689]

"""
Модуль для управления объектами на экране
"""

from typing import Dict, List, Optional, Tuple
from collections import defaultdict
from src.screen_object import ScreenObject, ObjectStatus
from src.tracker import ReIDTracker, Track
from src.image_utils import extract_roi
import numpy as np


class ObjectManager:
    """Менеджер для управления объектами на экране"""
    
    def __init__(self, class_names: Dict[int, str]):
        """
        Инициализация менеджера объектов
        
        
        Args:
            class_names: словарь соответствия class_id -> имя класса
        """
        self.class_names = class_names
        # Хранилище объектов по типу: {object_type: {object_id: ScreenObject}}
        self.objects_by_type: Dict[str, Dict[int, ScreenObject]] = defaultdict(dict)
        # Счетчики ID для каждого типа объекта
        self.next_id_by_type: Dict[str, int] = defaultdict(int)
        # Связь track_id -> object_id для каждого типа
        self.track_to_object: Dict[int, Tuple[str, int]] = {}  # track_id -> (object_type, object_id)
    
    def get_object_type(self, class_id: int) -> str:
        
        return self.class_names.get(class_id, f"class_{class_id}").lower()
    
    def create_object_from_track(self, track: Track, frame_num: int) -> ScreenObject:
        """
        Создает ScreenObject из Track
        
        Args:
            track: трек объекта
            frame_num: номер кадра
            
        Returns:
            созданный ScreenObject
        """
        object_type = self.get_object_type(track.class_id)
        
        # Получаем следующий ID для этого типа
        object_id = self.next_id_by_type[object_type]
        self.next_id_by_type[object_type] += 1
        
        # Создаем объект
        screen_object = ScreenObject(
            object_id=object_id,
            object_type=object_type,
            class_id=track.class_id,
            bbox=track.bbox,
            confidence=track.confidence,
            frame_num=frame_num,
            features=track.features
        )
        
        # Копируем дополнительную информацию
        if hasattr(track, 'train_number'):
            screen_object.train_number = track.train_number
        
        # Сохраняем объект
        self.objects_by_type[object_type][object_id] = screen_object
        
        # Сохраняем связь track_id -> object_id
        self.track_to_object[track.track_id] = (object_type, object_id)
        
        return screen_object
    
    def update_object_from_track(self, track: Track, frame_num: int, frame: np.ndarray,
                                 attribute_models=None, attr_update_interval=10) -> Optional[ScreenObject]:
        """
        Обновляет существующий объект из трека или создает новый
        
        Args:
            track: трек объекта
            frame_num: номер кадра
            frame: кадр изображения для обновления цветов
            attribute_models: модели для детекции атрибутов (PPE и одежда)
            attr_update_interval: интервал обновления атрибутов (в кадрах)
            
        Returns:
            обновленный или созданный ScreenObject
        """
        # Проверяем, есть ли уже объект для этого трека
        if track.track_id in self.track_to_object:
            object_type, object_id = self.track_to_object[track.track_id]
            screen_object = self.objects_by_type[object_type].get(object_id)
            
            if screen_object:
                # Обновляем существующий объект
                screen_object.update(
                    bbox=track.bbox,
                    confidence=track.confidence,
                    frame_num=frame_num,
                    features=track.features
                )
                
                # Обновляем цвета (каждый N-й кадр для оптимизации)
                if frame_num % 5 == 0:  # Обновляем цвета каждые 5 кадров
                    # Используем компенсацию освещения для лучшего определения цветов
                    lighting_compensation = {"enabled": True, "normalize_brightness": False, "wider_color_ranges": True}
                    screen_object.update_colors(frame, top_n=4, lighting_compensation=lighting_compensation)
                
                # Обновляем атрибуты (PPE и одежда) для людей
                if attribute_models and attribute_models.is_enabled() and object_type == "person":
                    # Обновляем атрибуты периодически
                    if frame_num % attr_update_interval == 0:
                        x1, y1, x2, y2 = track.bbox
                        person_crop = extract_roi(frame, x1, y1, x2, y2, crop_border_ratio=0.0)
                        if person_crop is not None and person_crop.size > 0:
                            try:
                                from src.attribute_models import map_ppe_names, map_clothes_names
                                ppe = attribute_models.run_ppe(person_crop)
                                clothes = attribute_models.run_clothes(person_crop)
                                ppe_names = map_ppe_names(ppe)
                                clothes_names = map_clothes_names(clothes)
                                # Объединяем с существующими атрибутами (уникальные значения)
                                existing_ppe = screen_object.attributes.get("ppe", [])
                                existing_clothes = screen_object.attributes.get("clothes", [])
                                screen_object.attributes = {
                                    "ppe": sorted(set(existing_ppe + ppe_names)),
                                    "clothes": sorted(set(existing_clothes + clothes_names))
                                }
                            except Exception as e:
                                pass  # Игнорируем ошибки при обновлении атрибутов
                
                # Обновляем статус (передаем ширину кадра для определения WORK)
                frame_height, frame_width = frame.shape[:2]
                screen_object.update_status(frame_width=frame_width)
                
                # Обновляем номер поезда, если есть
                if hasattr(track, 'train_number') and track.train_number:
                    screen_object.train_number = track.train_number
                
                return screen_object
        
        # Если объекта нет, создаем новый
        return self.create_object_from_track(track, frame_num)
    
    def get_object_by_track_id(self, track_id: int) -> Optional[ScreenObject]:
        """
        Получает объект по track_id
        
        Args:
            track_id: ID трека
            
        Returns:
            ScreenObject или None
        """
        if track_id not in self.track_to_object:
            return None
        
        object_type, object_id = self.track_to_object[track_id]
        return self.objects_by_type[object_type].get(object_id)
    
    def get_objects_by_type(self, object_type: str) -> List[ScreenObject]:
        """
        Получает все объекты определенного типа
        
        Args:
            object_type: тип объекта
            
        Returns:
            список объектов
        """
        return list(self.objects_by_type[object_type].values())
    
    def get_all_objects(self) -> List[ScreenObject]:
        """
        Получает все объекты
        
        Returns:
            список всех объектов
        """
        all_objects = []
        for objects_dict in self.objects_by_type.values():
            all_objects.extend(objects_dict.values())
        return all_objects
    
    def remove_inactive_objects(self, active_track_ids: set, max_age: int = 150):
        """
        Удаляет неактивные объекты (треки которых больше не существуют)
        
        Args:
            active_track_ids: множество активных track_id
            max_age: максимальный возраст объекта без обновления
        """
        # Находим неактивные треки
        inactive_tracks = set(self.track_to_object.keys()) - active_track_ids
        
        for track_id in list(inactive_tracks):
            if track_id in self.track_to_object:
                object_type, object_id = self.track_to_object[track_id]
                screen_object = self.objects_by_type[object_type].get(object_id)
                
                if screen_object:
                    # Проверяем возраст объекта
                    frames_since_update = screen_object.last_update_frame - screen_object.frame_num
                    if frames_since_update > max_age:
                        # Удаляем объект
                        del self.objects_by_type[object_type][object_id]
                        del self.track_to_object[track_id]
    
    def get_statistics(self, min_frames: int = 20) -> Dict:
        """
        Получает статистику по объектам
        
        Args:
            min_frames: минимальное количество кадров для учета объекта в статистике
        
        Returns:
            словарь со статистикой
        """
        stats = {}
        for object_type, objects_dict in self.objects_by_type.items():
            # Фильтруем объекты по минимальному количеству кадров
            valid_objects = [obj for obj in objects_dict.values() if obj.frame_count >= min_frames]
            
            stats[object_type] = {
                'total': len(valid_objects),
                'by_status': {},
                'by_colors': {},  # Статистика по цветам
                'by_profession': {},  # Статистика по профессиям
                'by_ppe': {},  # Статистика по PPE
                'by_clothes': {}  # Статистика по одежде
            }
            
            # Подсчет по статусам, цветам и профессиям
            for obj in valid_objects:
                # Статусы
                status = obj.status.value
                if status not in stats[object_type]['by_status']:
                    stats[object_type]['by_status'][status] = 0
                stats[object_type]['by_status'][status] += 1
                
                # Цвета
                if obj.color_info and obj.color_info.get('top_colors'):
                    # Берем основной цвет (первый в списке)
                    top_colors = obj.color_info.get('top_colors', [])
                    if top_colors:
                        main_color = top_colors[0].get('name', 'unknown')
                        if main_color not in stats[object_type]['by_colors']:
                            stats[object_type]['by_colors'][main_color] = 0
                        stats[object_type]['by_colors'][main_color] += 1
                else:
                    # Если цвет не определен
                    if 'unknown' not in stats[object_type]['by_colors']:
                        stats[object_type]['by_colors']['unknown'] = 0
                    stats[object_type]['by_colors']['unknown'] += 1
                
                # Профессии (только для людей)
                if obj.profession:
                    profession = obj.profession
                    if profession not in stats[object_type]['by_profession']:
                        stats[object_type]['by_profession'][profession] = 0
                    stats[object_type]['by_profession'][profession] += 1
                
                # Атрибуты (PPE и одежда) - только для людей
                if obj.attributes and object_type == "person":
                    ppe_list = obj.attributes.get("ppe", [])
                    clothes_list = obj.attributes.get("clothes", [])
                    
                    # Статистика по PPE
                    for ppe_item in ppe_list:
                        if ppe_item not in stats[object_type]['by_ppe']:
                            stats[object_type]['by_ppe'][ppe_item] = 0
                        stats[object_type]['by_ppe'][ppe_item] += 1
                    
                    # Статистика по одежде
                    for clothes_item in clothes_list:
                        if clothes_item not in stats[object_type]['by_clothes']:
                            stats[object_type]['by_clothes'][clothes_item] = 0
                        stats[object_type]['by_clothes'][clothes_item] += 1
        
        return stats


-----

[Файл: c:\Hahatonn\XakatonAI\src\ocr_reader.py]
[Размер: 18019 байт]
[Дата изменения: 2025-11-29 22:15:46.466908]

"""
Модуль для распознавания номеров поездов с помощью OCR
"""

import cv2
import numpy as np
from typing import Optional, Tuple, Dict
import re

# Замена похожих символов, которые OCR часто путает
CHAR_REPLACEMENTS = {
    # Латинские буквы, похожие на цифры
    'O': '0',  # Латинская O -> 0
    'o': '0',  # Строчная o -> 0
    'I': '1',  # I -> 1
    'l': '1',  # l -> 1
    '|': '1',  # | -> 1
    'S': '5',  # S -> 5
    's': '5',
    'D': '0',  # D -> 0 (иногда)
    'd': '0',
    'G': '6',  # G -> 6 (иногда)
    'g': '6',
    'Q': '0',  # Q -> 0
    'q': '0',
    # Кириллические буквы, похожие на цифры
    'О': '0',  # Кириллическая О -> 0
    'о': '0',  # Строчная о -> 0
    'З': '3',  # З -> 3
    'з': '3',
    'Б': '6',  # Б -> 6
    'б': '6',
    'В': '8',  # В -> 8 (иногда)
    'в': '8',
}



class TrainNumberOCR:
    """Класс для распознавания номеров поездов"""
    
    def __init__(self, ocr_engine="easyocr", languages=['en', 'ru'], 
                 allowed_chars="0123456789ЭП", expected_length=7):
        """
        Инициализация OCR
        
        Args:
            ocr_engine: движок OCR ('easyocr' или 'tesseract')
            languages: языки для распознавания
            allowed_chars: строка допустимых символов для номера поезда
            expected_length: ожидаемая длина номера поезда
        """
        self.ocr_engine = ocr_engine
        self.languages = languages
        self.allowed_chars = set(allowed_chars)
        self.expected_length = expected_length
        self.reader = None
        
        if ocr_engine == "easyocr":
            try:
                import easyocr
                import warnings
                # Подавляем предупреждение о pin_memory при использовании CPU
                with warnings.catch_warnings():
                    warnings.filterwarnings("ignore", message=".*pin_memory.*")
                    self.reader = easyocr.Reader(languages, gpu=False)
                print(f"EasyOCR инициализирован для языков: {languages}")
            except ImportError:
                print("EasyOCR не установлен. Установите: pip install easyocr")
                self.reader = None
            except Exception as e:
                print(f"Ошибка при инициализации EasyOCR: {e}")
                self.reader = None
        elif ocr_engine == "tesseract":
            try:
                import pytesseract
                self.reader = pytesseract
                print("Tesseract OCR готов к использованию")
            except ImportError:
                print("pytesseract не установлен. Установите: pip install pytesseract")
                self.reader = None
            except Exception as e:
                print(f"Ошибка при инициализации Tesseract: {e}")
                self.reader = None
    
    def preprocess_image(self, roi: np.ndarray) -> np.ndarray:
        """
        Предобработка изображения для улучшения распознавания
        
        Args:
            roi: область интереса
            
        Returns:
            обработанное изображение
        """
        # Конвертируем в grayscale если нужно
        if len(roi.shape) == 3:
            gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        else:
            gray = roi.copy()
        
        # Увеличиваем контраст
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        enhanced = clahe.apply(gray)
        
        # Бинаризация (адаптивная)
        binary = cv2.adaptiveThreshold(
            enhanced, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
            cv2.THRESH_BINARY, 11, 2
        )
        
        # Морфологические операции для очистки
        kernel = np.ones((2, 2), np.uint8)
        cleaned = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
        
        # Увеличение размера для лучшего распознавания
        height, width = cleaned.shape
        scale = max(2.0, 300.0 / max(height, width))
        new_width = int(width * scale)
        new_height = int(height * scale)
        resized = cv2.resize(cleaned, (new_width, new_height), 
                            interpolation=cv2.INTER_CUBIC)
        
        return resized
    
    def filter_train_number(self, text: str) -> Optional[str]:
        """
        Фильтрует текст, оставляя только допустимые символы для номера поезда.
        Также заменяет похожие символы, которые OCR часто путает.
        
        Args:
            text: исходный текст
            
        Returns:
            отфильтрованный текст или None, если не осталось допустимых символов
        """
        if not text:
            return None
        
        # Сначала заменяем похожие символы
        replaced = ''
        for c in text:
            if c in CHAR_REPLACEMENTS:
                replaced += CHAR_REPLACEMENTS[c]
            else:
                replaced += c
        
        # Затем оставляем только допустимые символы (из конфига)
        filtered = ''.join(c for c in replaced if c in self.allowed_chars)
        
        # Убираем пробелы и проверяем, что осталось что-то
        filtered = filtered.strip()
        
        if filtered and len(filtered) > 0:
            # Если длина соответствует ожидаемой, возвращаем как есть
            if len(filtered) == self.expected_length:
                return filtered
            # Если длина больше ожидаемой, берем первые expected_length символов
            elif len(filtered) > self.expected_length:
                return filtered[:self.expected_length]
            # Если длина меньше, но есть символы, возвращаем (может быть неполный номер)
            else:
                return filtered
        
        return None
    
    def recognize_text_easyocr(self, roi: np.ndarray) -> Optional[str]:
        """Распознавание текста с помощью EasyOCR"""
        if self.reader is None:
            return None
        
        try:
            # Пробуем распознавание без предобработки (для лучшего качества)
            results_raw = self.reader.readtext(roi)
            
            # Если не получилось, пробуем с предобработкой
            if not results_raw or len(results_raw) == 0:
                processed = self.preprocess_image(roi)
                results = self.reader.readtext(processed)
            else:
                results = results_raw
            
            if not results:
                return None
            
            # Собираем все распознанные тексты с их позициями
            texts_with_pos = []
            for (bbox, text, confidence) in results:
                if confidence > 0.2:  # Снижаем порог для лучшего распознавания
                    # Вычисляем минимальную x координату (левый край) для сортировки слева направо
                    # bbox - это numpy array формы (4, 2) с точками [x, y]
                    if isinstance(bbox, np.ndarray):
                        x_min = float(np.min(bbox[:, 0]))  # Минимальная x координата
                    else:
                        # Если это список, берем минимальную x из всех точек
                        x_min = min(point[0] for point in bbox)
                    texts_with_pos.append((x_min, text.strip(), confidence))
            
            if texts_with_pos:
                # Сортируем по позиции (x координате) слева направо для правильного порядка
                texts_with_pos.sort(key=lambda x: x[0])
                
                # Объединяем тексты в правильном порядке
                combined_text = ''.join([t[1] for t in texts_with_pos])
                
                # Очищаем текст, но сохраняем пробелы и кириллицу
                # Убираем только специальные символы, оставляем буквы, цифры и пробелы
                cleaned = re.sub(r'[^\w\sА-Яа-яЁё]', '', combined_text)
                cleaned = re.sub(r'\s+', '', cleaned)  # Убираем все пробелы
                result = cleaned.strip()
                
                # Фильтруем только допустимые символы для номера поезда
                filtered_result = self.filter_train_number(result)
                
                if filtered_result:
                    return filtered_result
            
            return None
        except Exception as e:
            print(f"Ошибка EasyOCR: {e}")
            return None
    
    def recognize_text_tesseract(self, roi: np.ndarray) -> Optional[str]:
        """Распознавание текста с помощью Tesseract"""
        if self.reader is None:
            return None
        
        try:
            # Предобработка
            processed = self.preprocess_image(roi)
            
            # Конфигурация для распознавания цифр и букв (используем допустимые символы из конфига)
            allowed_chars_str = ''.join(sorted(self.allowed_chars))
            config = f'--oem 3 --psm 7 -c tessedit_char_whitelist={allowed_chars_str}'
            
            # Распознавание
            text = self.reader.image_to_string(processed, config=config, lang='eng+rus')
            
            if text:
                # Фильтруем только допустимые символы для номера поезда
                filtered_result = self.filter_train_number(text)
                
                if filtered_result:
                    return filtered_result
            
            return None
        except Exception as e:
            print(f"Ошибка Tesseract: {e}")
            return None
    
    def recognize_train_number(self, frame: np.ndarray, 
                              roi_config: Dict) -> Optional[str]:
        """
        Распознавание номера поезда из заданной области кадра
        
        Args:
            frame: кадр изображения
            roi_config: конфигурация области интереса
                {
                    "x": процент от ширины (0.0-1.0),
                    "y": процент от высоты (0.0-1.0),
                    "width": процент ширины (0.0-1.0),
                    "height": процент высоты (0.0-1.0)
                }
            
        Returns:
            распознанный номер поезда или None
        """
        if self.reader is None:
            return None
        
        h, w = frame.shape[:2]
        
        # Вычисляем координаты ROI
        x = int(w * roi_config.get("x", 0.0))
        y = int(h * roi_config.get("y", 0.0))
        width = int(w * roi_config.get("width", 0.2))
        height = int(h * roi_config.get("height", 0.1))
        
        # Проверяем границы
        x = max(0, min(w - 1, x))
        y = max(0, min(h - 1, y))
        width = max(1, min(w - x, width))
        height = max(1, min(h - y, height))
        
        # Извлекаем ROI
        roi = frame[y:y+height, x:x+width]
        
        if roi.size == 0:
            return None
        
        # Распознавание в зависимости от движка
        if self.ocr_engine == "easyocr":
            return self.recognize_text_easyocr(roi)
        elif self.ocr_engine == "tesseract":
            return self.recognize_text_tesseract(roi)
        
        return None
    
    def recognize_from_right_half(self, frame: np.ndarray) -> Optional[str]:
        """
        Распознавание номера поезда из правой половины экрана
        (разделение вертикальной чертой по середине, ищем в правой половине)
        
        Args:
            frame: кадр изображения
            
        Returns:
            распознанный номер поезда или None
        """
        if self.reader is None:
            return None
        
        h, w = frame.shape[:2]
        
        # Центр экрана - вертикальная черта по середине
        center_x = w // 2
        
        # Правая половина экрана: от центра до края по всей высоте
        x = center_x  # Начинаем с центра по X
        y = 0  # Начинаем с верха экрана
        width = w - center_x  # Вся правая половина ширины
        height = h  # Вся высота экрана
        
        # Проверяем границы
        x = max(0, min(w - 1, x))
        y = max(0, min(h - 1, y))
        width = max(1, min(w - x, width))
        height = max(1, min(h - y, height))
        
        # Извлекаем ROI
        roi = frame[y:y+height, x:x+width]
        
        if roi.size == 0:
            return None
        
        # Распознавание в зависимости от движка
        if self.ocr_engine == "easyocr":
            return self.recognize_text_easyocr(roi)
        elif self.ocr_engine == "tesseract":
            return self.recognize_text_tesseract(roi)
        
        return None
    
    def recognize_from_bottom_right_quadrant(self, frame: np.ndarray) -> Optional[str]:
        """
        Распознавание номера поезда из правого нижнего квадранта кадра
        (устаревший метод, используйте recognize_from_right_half)
        """
        return self.recognize_from_right_half(frame)
    def recognize_from_train_bbox(self, frame: np.ndarray, 
                                 bbox: Tuple[int, int, int, int],
                                 roi_offset: Dict = None) -> Optional[str]:
        """
        Распознавание номера поезда из области детектированного поезда
        
        Args:
            frame: кадр изображения
            bbox: координаты поезда (x1, y1, x2, y2)
            roi_offset: смещение и размер ROI относительно bbox
                {
                    "x_offset": смещение по X (процент от ширины bbox),
                    "y_offset": смещение по Y (процент от высоты bbox),
                    "width": ширина ROI (процент от ширины bbox),
                    "height": высота ROI (процент от высоты bbox)
                }
        
        Returns:
            распознанный номер поезда или None
        """
        if self.reader is None:
            return None
        
        x1, y1, x2, y2 = bbox
        train_width = x2 - x1
        train_height = y2 - y1
        
        # Параметры по умолчанию для номера в верхней части поезда
        if roi_offset is None:
            roi_offset = {
                "x_offset": 0.1,  # 10% от левого края
                "y_offset": 0.05,  # 5% от верхнего края
                "width": 0.3,      # 30% ширины поезда
                "height": 0.15     # 15% высоты поезда
            }
        
        # Вычисляем координаты ROI
        roi_x = int(x1 + train_width * roi_offset["x_offset"])
        roi_y = int(y1 + train_height * roi_offset["y_offset"])
        roi_w = int(train_width * roi_offset["width"])
        roi_h = int(train_height * roi_offset["height"])
        
        # Проверяем границы
        h, w = frame.shape[:2]
        roi_x = max(0, min(w - 1, roi_x))
        roi_y = max(0, min(h - 1, roi_y))
        roi_w = max(1, min(w - roi_x, roi_w))
        roi_h = max(1, min(h - roi_y, roi_h))
        
        # Извлекаем ROI
        roi = frame[roi_y:roi_y+roi_h, roi_x:roi_x+roi_w]
        
        if roi.size == 0:
            return None
        
        # Распознавание
        if self.ocr_engine == "easyocr":
            return self.recognize_text_easyocr(roi)
        elif self.ocr_engine == "tesseract":
            return self.recognize_text_tesseract(roi)
        
        return None


-----

[Файл: c:\Hahatonn\XakatonAI\src\ppe_detector.py]
[Размер: 9497 байт]
[Дата изменения: 2025-11-29 22:15:49.148489]

"""
Модуль для детекции средств индивидуальной защиты (PPE)
"""

import sys
import cv2
from ultralytics import YOLO
from typing import List, Tuple, Optional, Dict


class PPEDetector:
    """Класс для детекции средств индивидуальной защиты (PPE)"""
    
    # Классы PPE (из data_custom.yaml и main.py)
    PPE_CLASSES = {
        "hat": "helmet",
        "helmet": "helmet",
        "vest": "safety_vest",
        "safety-vest": "safety_vest",
        "Safety-Vest": "safety_vest",
        "gloves": "gloves",
        "Gloves": "gloves",
        "glass": "safety_glasses",
        "Glass": "safety_glasses"
    }
    
    
    DEFAULT_COLORS = {
        "helmet": (0, 255, 255),        # Желтый для касок (BGR)
        "safety_vest": (255, 165, 0),   # Оранжевый для жилетов (BGR)
        "gloves": (255, 0, 255),        # Пурпурный для перчаток (BGR)
        "safety_glasses": (0, 255, 0),  # Зеленый для очков (BGR)
    }
    
    def __init__(self, model_path=None, 
                 conf_threshold=0.5, device="cpu", custom_colors=None, half_precision=False):
        """
        Инициализация детектора PPE
        
        Args:
            model_path: путь к модели YOLOv8 для PPE
            conf_threshold: порог уверенности
            device: устройство для обработки (cpu, cuda и т.д.)
            custom_colors: словарь пользовательских цветов
            half_precision: использовать половинную точность
        """
        self.conf_threshold = conf_threshold
        self.device = device
        self.half_precision = half_precision
        self.model = None
        self.class_names = {}  # Имена классов из модели
        
        if model_path is None:
            print("Путь к модели PPE не указан, детекция PPE будет отключена")
            self.model = None
            return
        
        print(f"Загрузка модели PPE: {model_path}")
        
        try:
            self.model = YOLO(model_path)
            # Получаем имена классов из модели
            if hasattr(self.model, 'names'):
                self.class_names = self.model.names
                print(f"Модель PPE загружена успешно!")
                print(f"Классы в модели: {list(self.class_names.values())}")
            else:
                print("Предупреждение: не удалось получить имена классов из модели")
                self.class_names = {}
        except Exception as e:
            print(f"Ошибка при загрузке модели PPE: {e}")
            print("Детекция PPE будет отключена")
            self.model = None
        
        # Подготовка цветов
        self.colors = dict(self.DEFAULT_COLORS)
        if custom_colors:
            self._apply_custom_colors(custom_colors)
    
    def _apply_custom_colors(self, colors_config):
        """Применение пользовательских цветов"""
        for key, value in colors_config.items():
            color_tuple = self._parse_color(value)
            if color_tuple is None:
                print(f"Предупреждение: некорректный цвет для '{key}'. Ожидался формат [B,G,R]. Пропущено.")
                continue
            
            # Нормализуем имя класса
            normalized_key = self._normalize_class_name(key)
            if normalized_key:
                self.colors[normalized_key] = color_tuple
    
    @staticmethod
    def _parse_color(value):
        """Преобразование цвета в кортеж BGR"""
        if isinstance(value, (list, tuple)) and len(value) == 3:
            try:
                b, g, r = [int(max(0, min(255, v))) for v in value]
                return (b, g, r)
            except (ValueError, TypeError):
                return None
        return None
    
    def _normalize_class_name(self, class_name: str) -> Optional[str]:
        """
        Нормализует имя класса PPE к стандартному виду
        
        Args:
            class_name: имя класса из модели или конфига
            
        Returns:
            нормализованное имя класса или None
        """
        class_name_lower = class_name.lower().strip()
        
        # Проверяем прямое соответствие
        if class_name_lower in self.PPE_CLASSES:
            return self.PPE_CLASSES[class_name_lower]
        
        # Проверяем через словарь
        for key, normalized in self.PPE_CLASSES.items():
            if key.lower() == class_name_lower:
                return normalized
        
        return None
    
    def detect(self, frame, target_classes=None):
        """
        Детекция PPE на кадре
        
        Args:
            frame: изображение (numpy array)
            target_classes: список классов для детекции (None = все PPE)
                Может быть список строк: ["helmet", "safety_vest", ...]
            
        Returns:
            список детекций: [(class_name, confidence, x1, y1, x2, y2), ...]
            где class_name - нормализованное имя класса (helmet, safety_vest, gloves, safety_glasses)
        """
        if self.model is None:
            return []
        
        # Запускаем детекцию
        try:
            results = self.model(
                frame,
                conf=self.conf_threshold,
                device=self.device,
                verbose=False,
                half=self.half_precision
            )
        except Exception as e:
            print(f"Ошибка при детекции PPE: {e}")
            return []
        
        detections = []
        if len(results) > 0:
            boxes = results[0].boxes
            for box in boxes:
                class_id = int(box.cls[0])
                confidence = float(box.conf[0])
                
                # Получаем имя класса из модели
                if class_id in self.class_names:
                    model_class_name = self.class_names[class_id]
                    # Нормализуем имя класса
                    normalized_name = self._normalize_class_name(model_class_name)
                    
                    if normalized_name:
                        # Фильтруем по целевым классам, если указаны
                        if target_classes is None or normalized_name in target_classes:
                            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                            detections.append((
                                normalized_name,  # Используем нормализованное имя
                                confidence,
                                int(x1),
                                int(y1),
                                int(x2),
                                int(y2)
                            ))
        
        return detections
    
    def draw_detections(self, frame, detections):
        """
        Отрисовка детекций PPE на кадре
        
        Args:
            frame: кадр для отрисовки
            detections: список детекций [(class_name, confidence, x1, y1, x2, y2), ...]
        """
        result_frame = frame.copy()
        
        for det in detections:
            if len(det) != 6:
                continue
            
            class_name, confidence, x1, y1, x2, y2 = det
            
            # Получаем цвет для класса
            color = self.colors.get(class_name, (0, 0, 255))
            
            # Рисуем прямоугольник
            cv2.rectangle(result_frame, (x1, y1), (x2, y2), color, 2)
            
            # Формируем подпись
            label = f"{class_name}: {confidence:.2f}"
            
            label_size, baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)
            label_height = label_size[1] + baseline
            
            # Размещаем текст сверху рамки
            label_y = y1 - 5
            if label_y < label_height:
                label_y = y1 + label_height + 5
            
            # Фон для текста
            bg_y1 = label_y - label_height
            bg_y2 = label_y
            cv2.rectangle(result_frame, (x1, bg_y1),
                         (x1 + label_size[0], bg_y2), color, -1)
            
            # Текст
            cv2.putText(result_frame, label, (x1, label_y - baseline),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
        
        return result_frame
    
    def is_enabled(self) -> bool:
        """Проверяет, включен ли детектор PPE"""
        return self.model is not None


-----

[Файл: c:\Hahatonn\XakatonAI\src\processors.py]
[Размер: 44275 байт]
[Дата изменения: 2025-11-30 00:46:57.773090]

"""
Модуль для обработки изображений и видео
"""

import cv2
import os
from pathlib import Path
from src.image_utils import imread_unicode, resize_frame
from src.filters import apply_color_filters, annotate_rejected
from src.reid import FeatureExtractor
from src.tracker import ReIDTracker
from src.ocr_reader import TrainNumberOCR
from src.object_manager import ObjectManager
from src.ppe_detector import PPEDetector
from src.attribute_models import AttributeModels, map_ppe_names, map_clothes_names


def process_image(image_path, detector, config):
    """Обработка изображения"""
    print(f"Загрузка изображения: {image_path}")
    
    if not os.path.exists(image_path):
        print(f"Ошибка: файл не существует!")
        print(f"Проверьте путь: {image_path}")
        return
    
    image = imread_unicode(image_path)
    if image is None:
        print(f"Ошибка: не удалось загрузить изображение!")
        print(f"Убедитесь, что файл является корректным изображением")
        return
    
    print(f"Изображение загружено: {image.shape[1]}x{image.shape[0]} пикселей")
    
    processing_cfg = config.get("processing", {})
    show_preview = processing_cfg.get("show_preview", True)
    save_results = processing_cfg.get("save_results", False)
    output_dir = processing_cfg.get("output_dir", "results")
    debug_cfg = config.get("debug", {})
    
    # Оптимизация размера если нужно
    opt_config = config.get("video_optimization", {})
    max_width = opt_config.get("max_width", 1920)
    max_height = opt_config.get("max_height", 1080)
    maintain_aspect = opt_config.get("maintain_aspect_ratio", True)
    keep_width_native = opt_config.get("keep_width_native", False)
    
    needs_resize = False
    if keep_width_native and max_height and image.shape[0] > max_height:
        needs_resize = True
    elif (image.shape[1] > max_width or image.shape[0] > max_height):
        needs_resize = True
    
    if needs_resize:
        target_desc = f"до высоты {max_height}px" if keep_width_native else f"до {max_width}x{max_height}"
        print(f"Оптимизация размера {target_desc} ...")
        image, scale = resize_frame(image, max_width, max_height, maintain_aspect, keep_width_native)
        print(f"Размер изменен: {image.shape[1]}x{image.shape[0]} пикселей (масштаб: {scale:.2f})")
    
    print("Выполняется детекция...")
    
    target_classes = config.get("detection", {}).get("target_classes", [0, 6])
    detections = detector.detect(image, target_classes=target_classes)
    
    color_filters = config.get("color_filters", {})
    detections, rejected = apply_color_filters(image, detections, color_filters, detector.CLASSES, debug_cfg)
    
    print(f"Найдено объектов: {len(detections)}")
    if rejected:
        print(f"Отклонено цветовым фильтром: {len(rejected)}")
    for class_id, confidence, x1, y1, x2, y2 in detections:
        class_name = detector.CLASSES.get(class_id, "unknown")
        print(f"  - {class_name}: {confidence:.2f}")
    
    result_image = detector.draw_detections(image, detections)
    
    # Детекция PPE для изображений
    ppe_cfg = config.get("ppe_detection", {})
    use_ppe = ppe_cfg.get("enabled", False)
    if use_ppe:
        try:
            ppe_model_path = ppe_cfg.get("model_path", "PPE_detection_using_YOLOV8-main/yolov8s_custom.pt")
            ppe_conf_threshold = ppe_cfg.get("confidence_threshold", 0.5)
            ppe_detector = PPEDetector(
                model_path=ppe_model_path,
                conf_threshold=ppe_conf_threshold,
                device=config.get("yolo", {}).get("device", "cpu"),
                custom_colors=ppe_cfg.get("colors", {}),
                half_precision=config.get("processing", {}).get("half_precision", False)
            )
            if ppe_detector.is_enabled():
                ppe_target_classes = ppe_cfg.get("target_classes", None)
                ppe_detections = ppe_detector.detect(image, target_classes=ppe_target_classes)
                if ppe_detections:
                    print(f"Найдено PPE: {len(ppe_detections)}")
                    for class_name, confidence, x1, y1, x2, y2 in ppe_detections:
                        print(f"  - {class_name}: {confidence:.2f}")
                    result_image = ppe_detector.draw_detections(result_image, ppe_detections)
        except Exception as e:
            print(f"Ошибка при детекции PPE: {e}")
    
    if debug_cfg.get("show_filtered_objects") and rejected:
        result_image = annotate_rejected(result_image, rejected, detector)
    
    if save_results:
        os.makedirs(output_dir, exist_ok=True)
        output_path = Path(output_dir) / f"{Path(image_path).stem}_detected.png"
        cv2.imwrite(str(output_path), result_image)
        print(f"Результат сохранен: {output_path}")
    
    if show_preview:
        cv2.imshow('Детекция объектов', result_image)
        print("\nНажмите любую клавишу для закрытия окна...")
        cv2.waitKey(0)
        cv2.destroyAllWindows()


def process_video(video_path, detector, config):
    """Обработка видео с оптимизацией"""
    # Для путей с кириллицей
    cap = cv2.VideoCapture(video_path)
    
    if not cap.isOpened():
        try:
            video_path_alt = str(Path(video_path).absolute())
            cap = cv2.VideoCapture(video_path_alt)
        except:
            pass
    
    if not cap.isOpened():
        print(f"Ошибка: не удалось открыть видео {video_path}")
        print("Убедитесь, что путь указан правильно и файл существует")
        return
    
    # Получаем параметры видео
    original_fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    print(f"Оригинальное видео: {width}x{height}, FPS: {original_fps}, Кадров: {total_frames}")
    
    # Параметры оптимизации
    opt_config = config.get("video_optimization", {})
    target_fps = opt_config.get("target_fps", 10)
    max_width = opt_config.get("max_width", 1280)
    max_height = opt_config.get("max_height", 720)
    frame_skip = opt_config.get("frame_skip", 1)
    maintain_aspect = opt_config.get("maintain_aspect_ratio", True)
    keep_width_native = opt_config.get("keep_width_native", False)
    
    processing_cfg = config.get("processing", {})
    show_preview = processing_cfg.get("show_preview", True)
    save_results = processing_cfg.get("save_results", False)
    output_dir = processing_cfg.get("output_dir", "results")
    debug_cfg = config.get("debug", {})
    
    # Вычисляем, через сколько кадров обрабатывать
    # Если frame_skip явно указан > 1, используем его, иначе вычисляем на основе target_fps
    if frame_skip > 1:
        skip_frames = frame_skip
        print(f"Используется явный пропуск кадров: {skip_frames}")
    else:
        # Вычисляем пропуск на основе целевого FPS
        if target_fps <= 0 or original_fps <= 0:
            skip_frames = 1
        elif target_fps < original_fps:
            skip_frames = max(1, int(original_fps / target_fps))
        else:
            skip_frames = 1
        print(f"Автоматический пропуск кадров для FPS {target_fps}: {skip_frames}")
    
    # Предварительная оценка размера вывода
    ret, temp_frame = cap.read()
    if not ret or temp_frame is None:
        print("Не удалось прочитать первый кадр.")
        cap.release()
        return
    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
    resized_sample, _ = resize_frame(temp_frame, max_width, max_height, maintain_aspect, keep_width_native)
    display_width = resized_sample.shape[1]
    display_height = resized_sample.shape[0]
    
    print(f"Обработка: пропуск кадров {skip_frames}, размер {display_width}x{display_height}")
    print(f"Целевой FPS: {target_fps}")
    print("\nНажмите 'q' для выхода\n")
    
    target_classes = config.get("detection", {}).get("target_classes", [0, 6])
    detections_count = {cid: 0 for cid in target_classes}
    color_filters = config.get("color_filters", {})
    rejected_total = 0
    writer = None
    output_path = None
    ppe_total_count = {}  # Общий счетчик детекций PPE по классам
    
    # Инициализация re-identification трекера
    reid_cfg = config.get("re_identification", {})
    use_reid = reid_cfg.get("enabled", False)
    tracker = None
    feature_extractor = None
    last_recognized_train_number = None  # Последний распознанный номер поезда
    
    # Инициализация менеджера объектов
    use_objects = reid_cfg.get("use_objects", False)  # Использовать новый подход с объектами
    object_manager = None
    if use_objects:
        object_manager = ObjectManager(class_names=detector.CLASSES)
        print("Менеджер объектов инициализирован")
    
    # Инициализация детектора PPE
    ppe_cfg = config.get("ppe_detection", {})
    use_ppe = ppe_cfg.get("enabled", False)
    ppe_detector = None
    if use_ppe:
         print("Инициализация детектора PPE...")
         try:
             ppe_model_path = ppe_cfg.get("model_path", None)
             # Если путь не указан или файл не существует, отключаем детекцию PPE
             if ppe_model_path and not os.path.exists(ppe_model_path):
                 print(f"Файл модели PPE не найден: {ppe_model_path}")
                 print("Детекция PPE будет отключена")
                 use_ppe = False
                 ppe_detector = None
             else:
                 ppe_conf_threshold = ppe_cfg.get("confidence_threshold", 0.5)
                 ppe_detector = PPEDetector(
                     model_path=ppe_model_path,
                     conf_threshold=ppe_conf_threshold,
                     device=config.get("yolo", {}).get("device", "cpu"),
                     custom_colors=ppe_cfg.get("colors", {}),
                     half_precision=config.get("processing", {}).get("half_precision", False)
                 )
                 if ppe_detector.is_enabled():
                     print("Детектор PPE инициализирован успешно")
                 else:
                     print("Детектор PPE не инициализирован, продолжаем без детекции PPE...")
                     use_ppe = False
                     ppe_detector = None
         except Exception as e:
             print(f"Ошибка при инициализации детектора PPE: {e}")
             print("Продолжаем без детекции PPE...")
             use_ppe = False
             ppe_detector = None
    
    # Инициализация моделей атрибутов (PPE и одежда)
    attr_cfg = config.get("attributes", {})
    use_attributes = attr_cfg.get("enabled", False)
    attribute_models = None
    attr_update_interval = attr_cfg.get("update_interval", 10)
    
    if use_attributes:
        print("Инициализация моделей атрибутов (PPE и одежда)...")
        try:
            ppe_model_path = attr_cfg.get("ppe_model")
            clothes_model_path = attr_cfg.get("clothes_model")
            attr_device = attr_cfg.get("device", config.get("yolo", {}).get("device", "cpu"))
            attr_conf = attr_cfg.get("confidence", 0.25)
            
            attribute_models = AttributeModels(
                ppe_model_path=ppe_model_path,
                clothes_model_path=clothes_model_path,
                device=attr_device,
                conf=attr_conf
            )
            if attribute_models.is_enabled():
                print("Модели атрибутов инициализированы успешно")
            else:
                print("Модели атрибутов не инициализированы, продолжаем без детекции атрибутов...")
                use_attributes = False
                attribute_models = None
        except Exception as e:
            print(f"Ошибка при инициализации моделей атрибутов: {e}")
            print("Продолжаем без детекции атрибутов...")
            use_attributes = False
            attribute_models = None
    
    if use_reid:
        print("Инициализация re-identification трекера...")
        try:
            device = config.get("yolo", {}).get("device", "cpu")
            feature_extractor = FeatureExtractor(device=device)
            tracker = ReIDTracker(
                feature_extractor=feature_extractor,
                max_distance=reid_cfg.get("max_distance", 0.5),
                max_age=reid_cfg.get("max_age", 30),
                min_hits=reid_cfg.get("min_hits", 1),
                iou_threshold=reid_cfg.get("iou_threshold", 0.3)
            )
            print("Re-identification трекер инициализирован успешно")
        except Exception as e:
            print(f"Ошибка при инициализации re-identification: {e}")
            print("Продолжаем без re-identification...")
            use_reid = False
            tracker = None
    
    # Инициализация OCR для распознавания номеров поездов
    ocr_cfg = config.get("train_number_ocr", {})
    use_ocr = ocr_cfg.get("enabled", False)
    ocr_reader = None
    ocr_frame_skip = ocr_cfg.get("frame_skip", 10)  # Проверять каждый N-й кадр
    ocr_frame_count = 0  # Счетчик кадров для OCR
    use_bottom_right_quadrant = ocr_cfg.get("use_bottom_right_quadrant", True)  # Использовать правый нижний квадрант
    
    if use_ocr:
        print("Инициализация OCR для распознавания номеров поездов...")
        try:
            ocr_engine = ocr_cfg.get("engine", "easyocr")
            allowed_chars = ocr_cfg.get("allowed_chars", "0123456789ЭП")
            expected_length = ocr_cfg.get("expected_length", 7)
            ocr_reader = TrainNumberOCR(
                ocr_engine=ocr_engine,
                allowed_chars=allowed_chars,
                expected_length=expected_length
            )
            if ocr_reader.reader is None:
                print("OCR не инициализирован, продолжаем без распознавания номеров...")
                use_ocr = False
            else:
                print(f"OCR инициализирован успешно (проверка каждые {ocr_frame_skip} кадров)")
                print(f"Допустимые символы: {allowed_chars}, ожидаемая длина: {expected_length}")
        except Exception as e:
            print(f"Ошибка при инициализации OCR: {e}")
            print("Продолжаем без распознавания номеров...")
            use_ocr = False
            ocr_reader = None

    frame_count = 0
    processed_count = 0
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            
            # Пропускаем кадры для снижения FPS
            if frame_count % skip_frames != 0:
                continue
            
            processed_count += 1
            
            resize_needed = False
            if keep_width_native:
                if max_height and frame.shape[0] > max_height:
                    resize_needed = True
            else:
                if frame.shape[1] > max_width or frame.shape[0] > max_height:
                    resize_needed = True
            
            if resize_needed:
                resized_frame, _ = resize_frame(frame, max_width, max_height, maintain_aspect, keep_width_native)
            else:
                resized_frame = frame
            
            # Детекция основных объектов (люди, поезда)
            detections = detector.detect(resized_frame, target_classes=target_classes)
            detections, rejected = apply_color_filters(resized_frame, detections, color_filters, detector.CLASSES, debug_cfg)
            rejected_total += len(rejected)
            
            # Детекция PPE (каски, амуниция)
            ppe_detections = []
            if use_ppe and ppe_detector is not None:
                try:
                    ppe_target_classes = ppe_cfg.get("target_classes", None)  # None = все классы PPE
                    ppe_detections = ppe_detector.detect(resized_frame, target_classes=ppe_target_classes)
                except Exception as e:
                    if debug_cfg.get("log_detection_details"):
                        print(f"Ошибка при детекции PPE: {e}")

            # Re-identification трекинг
            tracks = None
            train_numbers = {}  # Словарь {track_id: train_number}
            
            if use_reid and tracker is not None:
                try:
                    tracks = tracker.update(resized_frame, detections)
                    
                    # Распознавание номеров поездов (только каждый N-й кадр для оптимизации)
                    if use_ocr and ocr_reader is not None:
                        ocr_frame_count += 1
                        
                        # Проверяем только каждый N-й кадр
                        if ocr_frame_count >= ocr_frame_skip:
                            ocr_frame_count = 0
                            
                            train_number = None
                            
                            # Используем правую половину экрана (оптимизированный вариант)
                            if use_bottom_right_quadrant:
                                try:
                                    train_number = ocr_reader.recognize_from_right_half(resized_frame)
                                    if train_number:
                                        last_recognized_train_number = train_number  # Сохраняем последний распознанный номер
                                        
                                        # Присваиваем номер всем поездам в кадре
                                        if tracks:
                                            for track_id, class_id, _, _, _, _, _ in tracks:
                                                if class_id == 6:  # train
                                                    track = tracker.tracks.get(track_id)
                                                    if track:
                                                        track.train_number = train_number
                                                        train_numbers[track_id] = train_number
                                            print(f"[OCR] Распознан номер поезда: {train_number} (из правого нижнего квадранта)")
                                        
                                        # Присваиваем номер всем существующим трекам поездов в tracker
                                        for track_id, track in tracker.tracks.items():
                                            if track.class_id == 6:  # train
                                                if not track.train_number:  # Присваиваем только если номер еще не установлен
                                                    track.train_number = train_number
                                                train_numbers[track_id] = track.train_number or train_number
                                except Exception as e:
                                    if debug_cfg.get("log_detection_details"):
                                        print(f"Ошибка OCR (правый нижний квадрант): {e}")
                            else:
                                # Старый метод с фиксированной областью
                                fixed_roi = ocr_cfg.get("fixed_roi", None)
                                
                                if fixed_roi and fixed_roi.get("enabled", False):
                                    # Используем фиксированную область кадра
                                    train_number = ocr_reader.recognize_train_number(
                                        resized_frame, fixed_roi
                                    )
                                    if train_number:
                                        if tracks:
                                            # Присваиваем номер всем поездам в кадре
                                            for track_id, class_id, _, _, _, _, _ in tracks:
                                                if class_id == 6:  # train
                                                    track = tracker.tracks.get(track_id)
                                                    if track:
                                                        track.train_number = train_number
                                                        train_numbers[track_id] = train_number
                                        else:
                                            # Присваиваем номер всем существующим трекам поездов
                                            for track_id, track in tracker.tracks.items():
                                                if track.class_id == 6:  # train
                                                    track.train_number = train_number
                                                    train_numbers[track_id] = train_number
                                        if processed_count % 30 == 0:
                                            print(f"[OCR] Распознан номер поезда: {train_number} (из фиксированной области)")
                                elif tracks:
                                    # Используем область детектированного поезда
                                    for track_id, class_id, confidence, x1, y1, x2, y2 in tracks:
                                        # Распознаем номер только для поездов (class_id = 6)
                                        if class_id == 6:  # train
                                            track = tracker.tracks.get(track_id)
                                            if track and track.train_number is None:
                                                # Пытаемся распознать номер только один раз для каждого трека
                                                try:
                                                    roi_offset = ocr_cfg.get("roi_offset", None)
                                                    train_number = ocr_reader.recognize_from_train_bbox(
                                                        resized_frame, (x1, y1, x2, y2), roi_offset
                                                    )
                                                    if train_number:
                                                        track.train_number = train_number
                                                        train_numbers[track_id] = train_number
                                                        print(f"[OCR] Распознан номер поезда ID:{track_id}: {train_number}")
                                                except Exception as e:
                                                    if debug_cfg.get("log_detection_details"):
                                                        print(f"Ошибка OCR для трека {track_id}: {e}")
                    
                    # ВСЕГДА извлекаем уже распознанные номера из треков для отрисовки
                    # (как в кадрах с OCR, так и в промежуточных кадрах)
                    if tracks:
                        for track_id, class_id, _, _, _, _, _ in tracks:
                            if class_id == 6:  # train
                                track = tracker.tracks.get(track_id)
                                if track:
                                    # Если у трека нет номера, но есть последний распознанный номер, присваиваем его
                                    if not track.train_number and last_recognized_train_number:
                                        track.train_number = last_recognized_train_number
                                    if track.train_number:
                                        train_numbers[track_id] = track.train_number
                    else:
                        # Если треков еще нет в текущем кадре, но они есть в tracker, используем их
                        for track_id, track in tracker.tracks.items():
                            if track.class_id == 6:  # train
                                # Если у трека нет номера, но есть последний распознанный номер, присваиваем его
                                if not track.train_number and last_recognized_train_number:
                                    track.train_number = last_recognized_train_number
                                if track.train_number:
                                    train_numbers[track_id] = track.train_number
                    
                    # Обновляем объекты из треков
                    if use_objects and tracks:
                        active_track_ids = set()
                        for track_id, class_id, _, _, _, _, _ in tracks:
                            active_track_ids.add(track_id)
                            track = tracker.tracks.get(track_id)
                            if track:
                                object_manager.update_object_from_track(
                                    track, processed_count, resized_frame,
                                    attribute_models=attribute_models,
                                    attr_update_interval=attr_update_interval
                                )
                        
                        # Удаляем неактивные объекты
                        object_manager.remove_inactive_objects(
                            active_track_ids, 
                            max_age=reid_cfg.get("max_age", 150)
                        )
                    
                    if tracks and len(tracks) > 0:
                        if use_objects:
                            # Отрисовка с использованием объектов
                            objects_info = []
                            for track_id, class_id, confidence, x1, y1, x2, y2 in tracks:
                                screen_object = object_manager.get_object_by_track_id(track_id)
                                if screen_object:
                                    objects_info.append({
                                        'track_id': track_id,
                                        'object_id': screen_object.object_id,
                                        'object_type': screen_object.object_type,
                                        'class_id': screen_object.class_id,
                                        'bbox': (x1, y1, x2, y2),
                                        'status': screen_object.status.value,
                                        'profession': screen_object.profession,
                                        'train_number': screen_object.train_number,
                                        'frame_count': screen_object.frame_count,
                                        'attributes': screen_object.attributes
                                    })
                            
                            result_frame = detector.draw_objects(resized_frame, objects_info)
                        else:
                            # Старая отрисовка с ID треков и номерами поездов
                            result_frame = detector.draw_detections(
                                resized_frame, tracks, show_track_ids=True, train_numbers=train_numbers
                            )
                    else:
                        # Если треки еще не созданы, показываем детекции без ID
                        result_frame = detector.draw_detections(resized_frame, detections, show_track_ids=False)
                except Exception as e:
                    print(f"Ошибка в трекинге: {e}")
                    # Fallback: показываем детекции без ID
                    result_frame = detector.draw_detections(resized_frame, detections, show_track_ids=False)
            else:
                # Подсчет
                for class_id, _, _, _, _, _ in detections:
                    if class_id in detections_count:
                        detections_count[class_id] += 1
                
                # Отрисовка без ID
                result_frame = detector.draw_detections(resized_frame, detections, show_track_ids=False)
            
            # Отрисовка детекций PPE поверх основного результата
            if use_ppe and ppe_detector is not None and ppe_detections:
                result_frame = ppe_detector.draw_detections(result_frame, ppe_detections)
            
            if debug_cfg.get("show_filtered_objects") and rejected:
                result_frame = annotate_rejected(result_frame, rejected, detector, color=(0, 0, 255))
            
            # Подсчет детекций PPE для статистики
            ppe_count_by_class = {}
            if use_ppe and ppe_detections:
                for class_name, _, _, _, _, _ in ppe_detections:
                    if class_name not in ppe_count_by_class:
                        ppe_count_by_class[class_name] = 0
                    ppe_count_by_class[class_name] += 1
                    # Обновляем общий счетчик
                    if class_name not in ppe_total_count:
                        ppe_total_count[class_name] = 0
                    ppe_total_count[class_name] += 1
            
            # Информация на кадре
            if use_reid and tracker is not None and tracks:
                if use_objects and object_manager is not None:
                    # Статистика по объектам
                    stats = object_manager.get_statistics()
                    counts_parts = []
                    for object_type in sorted(stats.keys()):
                        total = stats[object_type]['total']
                        counts_parts.append(f"{object_type}: {total}")
                    
                    counts_text = " | ".join(counts_parts)
                    info_text = f"Кадр: {processed_count} | Объектов: {len(tracks)}" + (f" | {counts_text}" if counts_text else "")
                    
                    # Добавляем информацию о PPE
                    if ppe_count_by_class:
                        ppe_parts = [f"{name}:{count}" for name, count in ppe_count_by_class.items()]
                        info_text += " | PPE: " + ", ".join(ppe_parts)
                else:
                    # Подсчет уникальных треков (старый подход)
                    unique_tracks_by_class = {}
                    for track_id, class_id, _, _, _, _, _ in tracks:
                        if class_id not in unique_tracks_by_class:
                            unique_tracks_by_class[class_id] = set()
                        unique_tracks_by_class[class_id].add(track_id)
                    
                    counts_text = " | ".join(
                        f"{detector.CLASSES.get(cid, f'class_{cid}')}: {len(unique_tracks_by_class.get(cid, set()))}"
                        for cid in target_classes
                    )
                    info_text = f"Кадр: {processed_count} | Треков: {len(tracks)}" + (f" | {counts_text}" if counts_text else "")
            else:
                counts_text = " | ".join(
                    f"{detector.CLASSES.get(cid, f'class_{cid}')}: {detections_count[cid]}"
                    for cid in detections_count
                )
                info_text = f"Кадр: {processed_count}" + (f" | {counts_text}" if counts_text else "")
                
                # Добавляем информацию о PPE
                if ppe_count_by_class:
                    ppe_parts = [f"{name}:{count}" for name, count in ppe_count_by_class.items()]
                    info_text += " | PPE: " + ", ".join(ppe_parts)

            # Убрана отрисовка информации на кадре (желтый текст)
            # cv2.putText(result_frame, info_text, (10, 30),
            #            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            
            # Сохранение результата
            if save_results:
                if writer is None:
                    os.makedirs(output_dir, exist_ok=True)
                    output_path = Path(output_dir) / f"{Path(video_path).stem}_detected.mp4"
                    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                    writer = cv2.VideoWriter(str(output_path), fourcc, target_fps,
                                             (result_frame.shape[1], result_frame.shape[0]))
                writer.write(result_frame)
            
            # Показываем результат
            if show_preview:
                cv2.imshow('Детекция объектов', result_frame)
                
                # Задержка для соответствия целевому FPS
                delay = int(1000 / target_fps) if target_fps > 0 else 1
                if cv2.waitKey(delay) & 0xFF == ord('q'):
                    break
            
            # Прогресс
            if processed_count % 30 == 0:
                progress = (frame_count / total_frames) * 100 if total_frames > 0 else 0
                print(f"Обработано кадров: {processed_count}, прогресс: {progress:.1f}%")
    
    except KeyboardInterrupt:
        print("\nОбработка прервана")
    finally:
        cap.release()
        if writer:
            writer.release()
        cv2.destroyAllWindows()
        
        print(f"\nОбработка завершена!")
        print(f"Обработано кадров: {processed_count}")
        
        if use_reid and tracker is not None:
            if use_objects and object_manager is not None:
                # Статистика по объектам
                print("\n=== Статистика объектов ===")
                stats = object_manager.get_statistics()
                for object_type, type_stats in stats.items():
                    print(f"\n{object_type.upper()}:")
                    print(f"  Всего объектов: {type_stats['total']}")
                    if type_stats['by_status']:
                        print("  По статусам:")
                        for status, count in type_stats['by_status'].items():
                            print(f"    {status}: {count}")
                    if type_stats.get('by_colors'):
                        print("  По цветам:")
                        # Сортируем цвета по количеству (от большего к меньшему)
                        sorted_colors = sorted(
                            type_stats['by_colors'].items(), 
                            key=lambda x: x[1], 
                            reverse=True
                        )
                        for color, count in sorted_colors:
                            print(f"    {color}: {count}")
                    if type_stats.get('by_profession'):
                        print("  По профессиям:")
                        # Сортируем профессии по количеству (от большего к меньшему)
                        sorted_professions = sorted(
                            type_stats['by_profession'].items(), 
                            key=lambda x: x[1], 
                            reverse=True
                        )
                        for profession, count in sorted_professions:
                            print(f"    {profession}: {count}")
                    if type_stats.get('by_ppe'):
                        print("  По PPE:")
                        sorted_ppe = sorted(
                            type_stats['by_ppe'].items(),
                            key=lambda x: x[1],
                            reverse=True
                        )
                        for ppe_item, count in sorted_ppe:
                            print(f"    {ppe_item}: {count}")
                    if type_stats.get('by_clothes'):
                        print("  По одежде:")
                        sorted_clothes = sorted(
                            type_stats['by_clothes'].items(),
                            key=lambda x: x[1],
                            reverse=True
                        )
                        for clothes_item, count in sorted_clothes:
                            print(f"    {clothes_item}: {count}")
                
                # Статистика по счетчикам состояний для каждого ID
                print("\n=== Статистика по счетчикам состояний ===")
                all_objects = object_manager.get_all_objects()
                # Фильтруем объекты с менее 20 кадрами
                min_frames_threshold = 20
                valid_objects = [obj for obj in all_objects if obj.frame_count >= min_frames_threshold]
                
                if not valid_objects:
                    print("Нет объектов, находящихся в кадре >= 20 кадров")
                else:
                    # Сортируем по типу и ID
                    valid_objects.sort(key=lambda x: (x.object_type, x.object_id))
                    
                    for obj in valid_objects:
                        print(f"{obj.object_type.upper()}#{obj.object_id}: "
                              f"STAY={obj.stay_frames}, GO={obj.go_frames}, WORK={obj.work_frames}")
                
                # Детальная информация по объектам
                if debug_cfg.get("log_detection_details"):
                    print("\n=== Детальная информация об объектах ===")
                    # Используем отфильтрованные объекты (>= 20 кадров)
                    for obj in valid_objects:
                        info = obj.get_info_dict()
                        color_info_str = "unknown"
                        if obj.color_info and obj.color_info.get('top_colors'):
                            top_colors = obj.color_info.get('top_colors', [])
                            if top_colors:
                                # Формируем строку с цветами и процентами
                                color_parts = []
                                for color_item in top_colors[:3]:  # Берем до 3 основных цветов
                                    name = color_item.get('name', 'unknown')
                                    pct = color_item.get('percentage', 0.0)
                                    color_parts.append(f"{name}({pct:.1%})")
                                color_info_str = ", ".join(color_parts)
                        
                        profession_str = f", профессия={obj.profession}" if obj.profession else ""
                        attrs_str = ""
                        if obj.attributes:
                            ppe_list = obj.attributes.get("ppe", [])
                            clothes_list = obj.attributes.get("clothes", [])
                            all_items = ppe_list + clothes_list
                            if all_items:
                                attrs_str = f", амуниция: {', '.join(all_items)}"
                        
                        print(f"{obj.object_type.upper()}#{obj.object_id}: "
                              f"статус={obj.status.value}, кадров={obj.frame_count}, "
                              f"цвета={color_info_str}{profession_str}{attrs_str}")
                        if obj.train_number:
                            print(f"  Номер поезда: {obj.train_number}")
            else:
                # Старая статистика по трекам
                for cid in target_classes:
                    class_name = detector.CLASSES.get(cid, f"class_{cid}")
                    # Используем all_tracks_by_class для подсчета всех уникальных треков
                    unique_count = len(tracker.all_tracks_by_class.get(cid, set()))
                    active_count = len([t for t in tracker.tracks.values() if t.class_id == cid])
                    print(f"Уникальных треков {class_name}: {unique_count} (активных: {active_count})")
        else:
            for cid, count in detections_count.items():
                class_name = detector.CLASSES.get(cid, f"class_{cid}")
                print(f"Обнаружено {class_name}: {count}")
        
        if rejected_total:
            print(f"Отклонено цветовым фильтром: {rejected_total}")
        
        # Статистика по PPE
        if use_ppe and ppe_total_count:
            print("\n=== Статистика PPE ===")
            for ppe_class, count in sorted(ppe_total_count.items(), key=lambda x: x[1], reverse=True):
                print(f"  {ppe_class}: {count} детекций")
        
        # Вывод текущего времени
        
        if save_results and output_path:
            print(f"Видео сохранено: {output_path}")


-----

[Файл: c:\Hahatonn\XakatonAI\src\reid.py]
[Размер: 6802 байт]
[Дата изменения: 2025-11-29 20:58:31.396895]

"""
Модуль для извлечения признаков (features) для re-identification
"""

import cv2
import numpy as np
from typing import List, Tuple, Optional
import torch
import torch.nn as nn
from torchvision import transforms
from src.image_utils import extract_roi


class FeatureExtractor:
    """Класс для извлечения признаков из изображений объектов"""
    
    def __init__(self, device="cpu", feature_dim=128):
        """
        Инициализация экстрактора признаков
        
        Args:
            device: устройство для обработки (cpu/cuda)
            feature_dim: размерность вектора признаков
        """
        self.device = device
        self.feature_dim = feature_dim
        self.model = self._build_model()
        self.model.eval()
        self.model.to(device)
        
        # Трансформации для предобработки изображений
        self.transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((256, 128)),  # Стандартный размер для person re-id
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
    
    def _build_model(self):
        """Построение модели для извлечения признаков"""
        # Используем упрощенную архитектуру на основе ResNet
        # Можно заменить на предобученную модель для person re-id
        model = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
            
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(256, self.feature_dim),
            nn.BatchNorm1d(self.feature_dim)
        )
        return model
    
    def extract_features(self, frame: np.ndarray, detections: List[Tuple]) -> np.ndarray:
        """
        Извлечение признаков для всех детекций
        
        Args:
            frame: кадр изображения (BGR)
            detections: список детекций [(class_id, confidence, x1, y1, x2, y2), ...]
            
        Returns:
            массив признаков shape=(n_detections, feature_dim)
        """
        if len(detections) == 0:
            return np.array([])
        
        features_list = []
        
        for det in detections:
            class_id, confidence, x1, y1, x2, y2 = det
            
            # Извлекаем ROI
            roi = extract_roi(frame, x1, y1, x2, y2)
            if roi is None or roi.size == 0:
                # Если ROI пустой, используем нулевой вектор
                features_list.append(np.zeros(self.feature_dim))
                continue
            
            # Конвертируем BGR в RGB
            roi_rgb = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)
            
            # Предобработка и извлечение признаков
            try:
                # Преобразуем в тензор
                input_tensor = self.transform(roi_rgb).unsqueeze(0).to(self.device)
                
                # Извлекаем признаки
                with torch.no_grad():
                    features = self.model(input_tensor)
                    # Нормализуем признаки
                    features = torch.nn.functional.normalize(features, p=2, dim=1)
                    features = features.cpu().numpy().flatten()
                
                features_list.append(features)
            except Exception as e:
                # В случае ошибки используем нулевой вектор
                print(f"Ошибка при извлечении признаков: {e}")
                features_list.append(np.zeros(self.feature_dim))
        
        return np.array(features_list)
    
    def compute_distance(self, features1: np.ndarray, features2: np.ndarray) -> float:
        """
        Вычисление расстояния между двумя векторами признаков (cosine distance)
        
        Args:
            features1: первый вектор признаков
            features2: второй вектор признаков
            
        Returns:
            расстояние (0-1, где 0 - идентичны, 1 - максимально разные)
        """
        if features1.shape != features2.shape:
            return 1.0
        
        # Cosine distance
        dot_product = np.dot(features1, features2)
        norm1 = np.linalg.norm(features1)
        norm2 = np.linalg.norm(features2)
        
        if norm1 == 0 or norm2 == 0:
            return 1.0
        
        cosine_similarity = dot_product / (norm1 * norm2)
        # Преобразуем similarity в distance
        distance = 1.0 - cosine_similarity
        
        return max(0.0, min(1.0, distance))
    
    def compute_distance_matrix(self, features1: np.ndarray, features2: np.ndarray) -> np.ndarray:
        """
        Вычисление матрицы расстояний между двумя наборами признаков
        
        Args:
            features1: массив признаков shape=(n1, feature_dim)
            features2: массив признаков shape=(n2, feature_dim)
            
        Returns:
            матрица расстояний shape=(n1, n2)
        """
        if len(features1) == 0 or len(features2) == 0:
            return np.array([])
        
        # Нормализуем признаки
        features1_norm = features1 / (np.linalg.norm(features1, axis=1, keepdims=True) + 1e-8)
        features2_norm = features2 / (np.linalg.norm(features2, axis=1, keepdims=True) + 1e-8)
        
        # Вычисляем cosine distance через dot product
        similarity_matrix = np.dot(features1_norm, features2_norm.T)
        distance_matrix = 1.0 - similarity_matrix
        
        return np.clip(distance_matrix, 0.0, 1.0)


-----

[Файл: c:\Hahatonn\XakatonAI\src\screen_object.py]
[Размер: 15282 байт]
[Дата изменения: 2025-11-29 21:32:49.515770]

"""
Модуль для представления объектов на экране
"""

import numpy as np
from typing import List, Tuple, Dict, Optional
from enum import Enum
import cv2
from src.image_utils import extract_roi
from src.filters import detect_dominant_color


class ObjectStatus(Enum):
    """Статусы объекта"""
    STAY = "stay"      # Остается на месте
    GO = "go"         # Движется
    WORK = "work"     # Работает (для поездов - в движении)
    UNKNOWN = "unknown"  # Неизвестно


class ScreenObject:
    """Класс для представления объекта на экране"""
    
    def __init__(self, object_id: int, object_type: str, class_id: int,
                 bbox: Tuple[int, int, int, int], confidence: float,
                 frame_num: int, features: Optional[np.ndarray] = None):
       
        self.object_id = object_id  # Уникальный ID в группе по типу
        self.object_type = object_type  # 'person', 'train', и т.д.
        self.class_id = class_id
        self.bbox = bbox
        self.confidence = confidence
        self.frame_num = frame_num
        self.features = features
        
        # История позиций для определения статуса
        self.position_history: List[Tuple[int, int, int, int]] = [bbox]
        self.frame_count = 1  # Количество кадров, которое объект фиксировался камерой
        
        # Основные цвета объекта
        self.dominant_colors: List[Tuple[int, int, int]] = []  # BGR формат (для обратной совместимости)
        self.color_info: Optional[Dict] = None  # Полная информация о цветах (названия, проценты, BGR)
        
        # Статус объекта
        self.status = ObjectStatus.UNKNOWN
        
        # Счетчики кадров по состояниям
        self.stay_frames = 0   # Количество кадров в состоянии STAY
        self.go_frames = 0      # Количество кадров в состоянии GO
        self.work_frames = 0   # Количество кадров в состоянии WORK
        
        # Дополнительная информация
        self.train_number: Optional[str] = None  # Номер поезда (для train)
        # Профессия определяется автоматически на основе цветов
        # Для поездов: "поезд", для людей: по умолчанию "работник"
        self.profession: Optional[str] = "поезд" if object_type == "train" else "работник"
        # Атрибуты (PPE и одежда) для людей
        self.attributes: Dict[str, List[str]] = {"ppe": [], "clothes": []}
        self.last_update_frame = frame_num
        
        # Пороги для определения статуса
        self.movement_threshold = 5.0  # Минимальное перемещение для статуса GO (в пикселях)
        self.stay_threshold = 2.0  # Максимальное перемещение для статуса STAY
    
    def update(self, bbox: Tuple[int, int, int, int], confidence: float,
               frame_num: int, features: Optional[np.ndarray] = None):
        """
        Обновление объекта новыми данными
        
        Args:
            bbox: новые координаты
            confidence: новая уверенность
            frame_num: номер кадра
            features: новые признаки
        """
        self.bbox = bbox
        self.confidence = confidence
        self.frame_count += 1
        self.last_update_frame = frame_num
        
        # Обновляем историю позиций
        self.position_history.append(bbox)
        # Ограничиваем историю (храним последние 30 позиций)
        if len(self.position_history) > 30:
            self.position_history.pop(0)
        
        # Обновляем признаки
        if features is not None:
            if self.features is not None:
                alpha = 0.1  # Коэффициент обновления
                self.features = (1 - alpha) * self.features + alpha * features
            else:
                self.features = features
    
    def update_colors(self, frame: np.ndarray, top_n: int = 4, lighting_compensation: Optional[Dict] = None):
        """
        Обновляет основные цвета объекта на основе текущего кадра
        Использует улучшенный метод detect_dominant_color из filters.py
        
        Args:
            frame: кадр изображения (BGR)
            top_n: количество основных цветов для определения (по умолчанию 4)
            lighting_compensation: настройки компенсации освещения (опционально)
        """
        roi = extract_roi(frame, *self.bbox, crop_border_ratio=0.1)  # Обрезаем края на 10%
        if roi is None or roi.size == 0:
            return
        
        # Используем улучшенный метод определения цветов
        try:
            color_info = detect_dominant_color(
                roi, 
                top_n=top_n, 
                lighting_compensation=lighting_compensation
            )
            
            # Сохраняем полную информацию о цветах
            self.color_info = color_info
            
            # Преобразуем результаты в формат BGR для обратной совместимости
            top_colors = color_info.get("top_colors", [])
            avg_bgr = color_info.get("bgr_avg", [0, 0, 0])
            
            if top_colors:
                # Сохраняем средний BGR для каждого доминирующего цвета
                # Используем средний BGR из всего ROI (можно улучшить, вычисляя для каждого цвета отдельно)
                self.dominant_colors = [tuple(avg_bgr)] * min(len(top_colors), top_n)
            else:
                # Если цвета не определены, используем средний цвет ROI
                if not avg_bgr or sum(avg_bgr) == 0:
                    avg_color = np.mean(roi.reshape(-1, 3), axis=0)
                    self.dominant_colors = [tuple(map(int, avg_color))]
                else:
                    self.dominant_colors = [tuple(avg_bgr)]
            
            # Определяем профессию на основе цветов
            self._determine_profession()
        except Exception as e:
            # Fallback на простой метод при ошибке
            avg_color = np.mean(roi.reshape(-1, 3), axis=0)
            self.dominant_colors = [tuple(map(int, avg_color))]
            self.color_info = None
    
    def _determine_profession(self):
        """
        Определяет профессию объекта на основе цветов одежды
        Правила:
        - Для поездов: профессия всегда "поезд" (фильтрация по цветам происходит в filters.py)
        - Белый или серый >20% -> рабочий (приоритет выше красного)
        - Красный цвет -> инженер
        - Остальные -> работник
        """
        # Для поездов профессия всегда "поезд"
        # Фильтрация объектов с неприемлемыми цветами (голубой > 5%) происходит в filters.py
        if self.object_type == "train":
            self.profession = "поезд"
            return
        
        if not self.color_info:
            self.profession = "работник"  # По умолчанию
            return
        
        all_percentages = self.color_info.get("all_percentages", {})
        top_colors = self.color_info.get("top_colors", [])
        
        # СНАЧАЛА проверяем белый и серый цвета (>20%) - приоритет выше красного
        white_percentage = all_percentages.get("white", 0.0)
        gray_percentage = all_percentages.get("gray", 0.0)
        light_gray_percentage = all_percentages.get("light_gray", 0.0)
        dark_gray_percentage = all_percentages.get("dark_gray", 0.0)
        
        total_gray_white = white_percentage + gray_percentage + light_gray_percentage + dark_gray_percentage
        
        if total_gray_white > 0.2:  # 20%
            self.profession = "рабочий"
            return
        
        # Затем проверяем наличие красного цвета
        red_percentage = all_percentages.get("red", 0.0)
        # Также проверяем в top_colors
        for color_item in top_colors:
            if color_item.get("name") == "red":
                red_percentage = max(red_percentage, color_item.get("percentage", 0.0))
        
        if red_percentage > 0.0:
            self.profession = "инженер"
            return
        
        # Остальные случаи
        self.profession = "работник"
    
    def update_status(self, frame_width: Optional[int] = None):
        """
        Обновляет статус объекта на основе истории перемещений и положения на экране
        
        Args:
            frame_width: ширина кадра для определения середины экрана (для статуса WORK)
        """
        # Получаем текущую позицию
        curr_x1, curr_y1, curr_x2, curr_y2 = self.bbox
        curr_center_x = (curr_x1 + curr_x2) / 2
        
        # Проверяем положение относительно середины экрана (для статуса WORK)
        is_on_right_side = False
        if frame_width is not None:
            screen_middle = frame_width / 2
            is_on_right_side = curr_center_x > screen_middle
        
        # Если только одна позиция в истории, определяем статус только по положению
        if len(self.position_history) < 2:
            if is_on_right_side:
                self.status = ObjectStatus.WORK
                self.work_frames += 1
            else:
                self.status = ObjectStatus.UNKNOWN
            return
        
        # Получаем текущую позицию
        curr_x1, curr_y1, curr_x2, curr_y2 = self.bbox
        curr_center_x = (curr_x1 + curr_x2) / 2
        
        # Проверяем положение относительно середины экрана (для статуса WORK)
        is_on_right_side = False
        if frame_width is not None:
            screen_middle = frame_width / 2
            is_on_right_side = curr_center_x > screen_middle
        
        # Вычисляем среднее перемещение за последние кадры
        movements = []
        for i in range(1, min(len(self.position_history), 10)):  # Последние 10 позиций
            prev_x1, prev_y1, prev_x2, prev_y2 = self.position_history[-i-1]
            curr_x1, curr_y1, curr_x2, curr_y2 = self.position_history[-i]
            
            # Центр предыдущей позиции
            prev_center_x = (prev_x1 + prev_x2) / 2
            prev_center_y = (prev_y1 + prev_y2) / 2
            
            # Центр текущей позиции
            curr_center_x = (curr_x1 + curr_x2) / 2
            curr_center_y = (curr_y1 + curr_y2) / 2
            
            # Расстояние перемещения
            distance = np.sqrt((curr_center_x - prev_center_x)**2 + 
                             (curr_center_y - prev_center_y)**2)
            movements.append(distance)
        
        if not movements:
            self.status = ObjectStatus.UNKNOWN
            return
        
        avg_movement = np.mean(movements)
        
        # Сохраняем предыдущий статус для обновления счетчиков
        prev_status = self.status
        
        # Определяем статус
        # WORK определяется по положению справа от середины экрана
        if is_on_right_side:
            self.status = ObjectStatus.WORK
        elif avg_movement < self.stay_threshold:
            self.status = ObjectStatus.STAY
        elif avg_movement >= self.movement_threshold:
            self.status = ObjectStatus.GO
        else:
            # Среднее перемещение - медленное движение
            self.status = ObjectStatus.GO
        
        # Обновляем счетчики кадров (увеличиваем каждый кадр)
        if self.status == ObjectStatus.STAY:
            self.stay_frames += 1
        elif self.status == ObjectStatus.GO:
            self.go_frames += 1
        elif self.status == ObjectStatus.WORK:
            self.work_frames += 1
    
    def get_info_dict(self) -> Dict:
        """
        Возвращает словарь с информацией об объекте
        
        Returns:
            словарь с информацией
        """
        info = {
            'object_id': self.object_id,
            'object_type': self.object_type,
            'class_id': self.class_id,
            'bbox': self.bbox,
            'confidence': self.confidence,
            'frame_count': self.frame_count,
            'status': self.status.value,
            'stay_frames': self.stay_frames,
            'go_frames': self.go_frames,
            'work_frames': self.work_frames,
            'profession': self.profession,
            'attributes': self.attributes,
            'dominant_colors': self.dominant_colors,
            'train_number': self.train_number,
            'first_seen_frame': self.frame_num,
            'last_update_frame': self.last_update_frame
        }
        
        # Добавляем полную информацию о цветах, если доступна
        if self.color_info:
            info['color_info'] = {
                'top_colors': self.color_info.get('top_colors', []),
                'all_percentages': self.color_info.get('all_percentages', {}),
                'bgr_avg': self.color_info.get('bgr_avg', [])
            }
        
        return info
    
    def __repr__(self):
        return (f"ScreenObject(id={self.object_id}, type={self.object_type}, "
                f"status={self.status.value}, frames={self.frame_count})")


-----

[Файл: c:\Hahatonn\XakatonAI\src\tracker.py]
[Размер: 12840 байт]
[Дата изменения: 2025-11-29 20:58:31.411191]

"""
Модуль для отслеживания объектов между кадрами (re-identification)
"""

import numpy as np
from typing import List, Tuple, Dict, Optional
from collections import defaultdict
from src.reid import FeatureExtractor


class Track:
    """Класс для представления трека объекта"""
    
    def __init__(self, track_id: int, class_id: int, bbox: Tuple[int, int, int, int], 
                 confidence: float, features: np.ndarray, frame_num: int):
        """
        Инициализация трека
        
        Args:
            track_id: уникальный ID трека
            class_id: ID класса объекта
            bbox: координаты (x1, y1, x2, y2)
            confidence: уверенность детекции
            features: вектор признаков
            frame_num: номер кадра
        """
        self.track_id = track_id
        self.class_id = class_id
        self.bbox = bbox
        self.confidence = confidence
        self.features = features
        self.frame_num = frame_num
        self.age = 1  # Возраст трека (количество кадров)
        self.time_since_update = 0  # Кадров с последнего обновления
        self.history = [bbox]  # История позиций
        self.train_number = None  # Номер поезда (для класса train)
        
    def update(self, bbox: Tuple[int, int, int, int], confidence: float, 
               features: np.ndarray, frame_num: int):
        """Обновление трека"""
        self.bbox = bbox
        self.confidence = confidence
        # Обновляем признаки как взвешенное среднее
        alpha = 0.1  # Коэффициент обновления
        self.features = (1 - alpha) * self.features + alpha * features
        self.frame_num = frame_num
        self.age += 1
        self.time_since_update = 0
        self.history.append(bbox)
        # Ограничиваем историю
        if len(self.history) > 30:
            self.history.pop(0)
    
    def predict(self):
        """Предсказание следующей позиции (простая линейная экстраполяция)"""
        if len(self.history) < 2:
            return self.bbox
        
        # Простое предсказание на основе последних двух позиций
        x1, y1, x2, y2 = self.bbox
        if len(self.history) >= 2:
            prev_x1, prev_y1, prev_x2, prev_y2 = self.history[-2]
            dx1 = x1 - prev_x1
            dy1 = y1 - prev_y1
            dx2 = x2 - prev_x2
            dy2 = y2 - prev_y2
            
            pred_x1 = int(x1 + dx1)
            pred_y1 = int(y1 + dy1)
            pred_x2 = int(x2 + dx2)
            pred_y2 = int(y2 + dy2)
            
            return (pred_x1, pred_y1, pred_x2, pred_y2)
        
        return self.bbox


class ReIDTracker:
    """Трекер для re-identification объектов"""
    
    def __init__(self, feature_extractor: FeatureExtractor, 
                 max_distance: float = 0.5,
                 max_age: int = 30,
                 min_hits: int = 1,
                 iou_threshold: float = 0.3):
        """
        Инициализация трекера
        
        Args:
            feature_extractor: экстрактор признаков
            max_distance: максимальное расстояние признаков для сопоставления
            max_age: максимальный возраст трека без обновления
            min_hits: минимальное количество попаданий для подтверждения трека
            iou_threshold: порог IoU для сопоставления по позиции
        """
        self.feature_extractor = feature_extractor
        self.max_distance = max_distance
        self.max_age = max_age
        self.min_hits = min_hits
        self.iou_threshold = iou_threshold
        
        self.tracks: Dict[int, Track] = {}
        self.next_id = 1
        self.frame_count = 0
        # Счетчик всех когда-либо созданных треков по классам
        self.all_tracks_by_class: Dict[int, set] = defaultdict(set)
    
    def _compute_iou(self, bbox1: Tuple[int, int, int, int], 
                     bbox2: Tuple[int, int, int, int]) -> float:
        """Вычисление IoU между двумя bbox"""
        x1_1, y1_1, x2_1, y2_1 = bbox1
        x1_2, y1_2, x2_2, y2_2 = bbox2
        
        # Вычисляем пересечение
        x1_i = max(x1_1, x1_2)
        y1_i = max(y1_1, y1_2)
        x2_i = min(x2_1, x2_2)
        y2_i = min(y2_1, y2_2)
        
        if x2_i <= x1_i or y2_i <= y1_i:
            return 0.0
        
        inter_area = (x2_i - x1_i) * (y2_i - y1_i)
        
        # Вычисляем объединение
        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
        union_area = area1 + area2 - inter_area
        
        if union_area == 0:
            return 0.0
        
        return inter_area / union_area
    
    def _associate_detections_to_tracks(self, detections: List[Tuple], 
                                       detection_features: np.ndarray,
                                       tracks: List[Track]) -> Tuple[List[int], List[int], List[int]]:
        """
        Сопоставление детекций с существующими треками
        
        Returns:
            matched: список индексов (det_idx, track_idx)
            unmatched_dets: список индексов несоответствующих детекций
            unmatched_tracks: список индексов несоответствующих треков
        """
        if len(tracks) == 0:
            return [], list(range(len(detections))), []
        
        if len(detections) == 0:
            return [], [], list(range(len(tracks)))
        
        # Вычисляем матрицу расстояний признаков
        track_features = np.array([t.features for t in tracks])
        distance_matrix = self.feature_extractor.compute_distance_matrix(
            detection_features, track_features
        )
        
        # Вычисляем матрицу IoU
        iou_matrix = np.zeros((len(detections), len(tracks)))
        for i, det in enumerate(detections):
            _, _, x1, y1, x2, y2 = det
            det_bbox = (x1, y1, x2, y2)
            for j, track in enumerate(tracks):
                iou_matrix[i, j] = self._compute_iou(det_bbox, track.bbox)
        
        # Комбинированная метрика: расстояние признаков + IoU
        # Нормализуем IoU (чем больше, тем лучше, поэтому инвертируем)
        iou_cost = 1.0 - iou_matrix
        combined_cost = 0.7 * distance_matrix + 0.3 * iou_cost
        
        # Жадное сопоставление (простой алгоритм)
        matched = []
        unmatched_dets = list(range(len(detections)))
        unmatched_tracks = list(range(len(tracks)))
        
        # Сортируем по стоимости и сопоставляем
        cost_pairs = []
        for i in range(len(detections)):
            for j in range(len(tracks)):
                cost = combined_cost[i, j]
                # Проверяем пороги
                if distance_matrix[i, j] <= self.max_distance and iou_matrix[i, j] >= self.iou_threshold:
                    cost_pairs.append((cost, i, j))
        
        cost_pairs.sort(key=lambda x: x[0])
        
        used_dets = set()
        used_tracks = set()
        
        for cost, det_idx, track_idx in cost_pairs:
            if det_idx not in used_dets and track_idx not in used_tracks:
                matched.append((det_idx, track_idx))
                used_dets.add(det_idx)
                used_tracks.add(track_idx)
        
        unmatched_dets = [i for i in range(len(detections)) if i not in used_dets]
        unmatched_tracks = [i for i in range(len(tracks)) if i not in used_tracks]
        
        return matched, unmatched_dets, unmatched_tracks
    
    def update(self, frame: np.ndarray, detections: List[Tuple]) -> List[Tuple]:
        """
        Обновление трекера новыми детекциями
        
        Args:
            frame: текущий кадр
            detections: список детекций [(class_id, confidence, x1, y1, x2, y2), ...]
            
        Returns:
            список треков с ID: [(track_id, class_id, confidence, x1, y1, x2, y2), ...]
        """
        self.frame_count += 1
        
        # Извлекаем признаки для всех детекций
        detection_features = self.feature_extractor.extract_features(frame, detections)
        
        # Получаем активные треки (не слишком старые)
        active_tracks = [t for t in self.tracks.values() 
                        if t.time_since_update < self.max_age]
        
        # Предсказываем позиции для активных треков
        for track in active_tracks:
            track.predict()
            track.time_since_update += 1
        
        # Сопоставляем детекции с треками
        matched, unmatched_dets, unmatched_tracks = self._associate_detections_to_tracks(
            detections, detection_features, active_tracks
        )
        
        # Обновляем сопоставленные треки
        for det_idx, track_idx in matched:
            det = detections[det_idx]
            track = active_tracks[track_idx]
            class_id, confidence, x1, y1, x2, y2 = det
            track.update((x1, y1, x2, y2), confidence, 
                        detection_features[det_idx], self.frame_count)
        
        # Создаем новые треки для несоответствующих детекций
        for det_idx in unmatched_dets:
            det = detections[det_idx]
            class_id, confidence, x1, y1, x2, y2 = det
            track_id = self.next_id
            self.next_id += 1
            
            new_track = Track(
                track_id=track_id,
                class_id=class_id,
                bbox=(x1, y1, x2, y2),
                confidence=confidence,
                features=detection_features[det_idx],
                frame_num=self.frame_count
            )
            self.tracks[track_id] = new_track
            # Сохраняем информацию о всех созданных треках
            self.all_tracks_by_class[class_id].add(track_id)
        
        # Удаляем старые треки
        tracks_to_remove = []
        for track_id, track in self.tracks.items():
            if track.time_since_update >= self.max_age:
                tracks_to_remove.append(track_id)
        
        for track_id in tracks_to_remove:
            del self.tracks[track_id]
        
        # Возвращаем все треки, которые были обновлены в текущем кадре
        # (time_since_update == 0 означает, что трек был обновлен в этом кадре)
        # Также показываем новые треки сразу (age >= min_hits)
        active_tracks_list = []
        for track in self.tracks.values():
            # Показываем треки, которые были обновлены в текущем кадре (включая новые)
            # или которые достаточно старые и еще активны
            if track.time_since_update == 0:
                x1, y1, x2, y2 = track.bbox
                active_tracks_list.append((
                    track.track_id,
                    track.class_id,
                    track.confidence,
                    x1, y1, x2, y2
                ))
            elif track.age >= self.min_hits and track.time_since_update < self.max_age:
                # Показываем старые треки, которые еще активны
                x1, y1, x2, y2 = track.bbox
                active_tracks_list.append((
                    track.track_id,
                    track.class_id,
                    track.confidence,
                    x1, y1, x2, y2
                ))
        
        return active_tracks_list


-----

[Файл: c:\Hahatonn\XakatonAI\src\__init__.py]
[Размер: 0 байт]
[Дата изменения: 2025-11-29 20:58:31.331736]


-----

[Файл: c:\Hahatonn\XakatonAI\config.example.json]
[Размер: 1407 байт]
[Дата изменения: 2025-11-29 22:48:06.423561]

{
  "yolo": {
    "model": "yolo11m.pt",
    "device": "cpu"
  },
  "detection": {
    "confidence_threshold": 0.5,
    "target_classes": [0, 6],
    "class_names": {
      "0": "person",
      "6": "train"
    }
  },
  "video_optimization": {
    "target_fps": 20,
    "max_width": 1280,
    "max_height": 540,
    "frame_skip": 1,
    "maintain_aspect_ratio": true,
    "keep_width_native": true
  },
  "processing": {
    "show_preview": false,
    "half_precision": false,
    "save_results": true,
    "output_dir": "results"
  },
  "debug": {
    "show_filtered_objects": false,
    "log_detection_details": false
  },
  "re_identification": {
    "enabled": true,
    "max_distance": 0.5,
    "max_age": 150,
    "min_hits": 10,
    "iou_threshold": 0.2,
    "use_objects": true
  },
  "train_number_ocr": {
    "enabled": true,
    "engine": "easyocr",
    "frame_skip": 10,
    "use_bottom_right_quadrant": true,
    "allowed_chars": "0123456789ЭП",
    "expected_length": 7,
    "languages": ["en", "ru"]
  },
  "ppe_detection": {
    "enabled": false,
    "model_path": null,
    "confidence_threshold": 0.5,
    "target_classes": null
  },
  "attributes": {
    "enabled": true,
    "ppe_model": "ppe_best.pt",
    "clothes_model": "clothes_best.pt",
    "device": "cpu",
    "confidence": 0.65,
    "update_interval": 10
  }
}


-----

[Файл: c:\Hahatonn\XakatonAI\init_db.sql]
[Размер: 801 байт]
[Дата изменения: 2025-11-30 01:02:48.826616]

-- Создание таблицы для работников
CREATE TABLE IF NOT EXISTS workers (
    id SERIAL PRIMARY KEY,
    track_id INTEGER UNIQUE NOT NULL,
    color VARCHAR(255),
    stand_frames INTEGER DEFAULT 0,
    walk_frames INTEGER DEFAULT 0,
    work_frames INTEGER DEFAULT 0,
    attributes JSONB
);

-- Создание таблицы для поездов
CREATE TABLE IF NOT EXISTS trains (
    id SERIAL PRIMARY KEY,
    track_id INTEGER UNIQUE NOT NULL,
    number VARCHAR(50),
    total_time INTEGER DEFAULT 0
);

-- Создание индексов для улучшения производительности
CREATE INDEX IF NOT EXISTS idx_workers_track_id ON workers(track_id);
CREATE INDEX IF NOT EXISTS idx_trains_track_id ON trains(track_id);


-----

[Файл: c:\Hahatonn\XakatonAI\docker-compose.yml]
[Размер: 1267 байт]
[Дата изменения: 2025-11-29 23:57:14.048709]

services:
  db:
    image: postgres:15-alpine
    container_name: xakaton_db
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-xakaton}
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-12345}
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init_db.sql:/docker-entrypoint-initdb.d/init_db.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - xakaton_network

  app:
    build: .
    container_name: xakaton_app
    depends_on:
      db:
        condition: service_healthy
    environment:
      - POSTGRES_HOST=db
      - POSTGRES_PORT=5432
      - POSTGRES_DB=${POSTGRES_DB:-xakaton}
      - POSTGRES_USER=${POSTGRES_USER:-postgres}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-12345}
    ports:
      - "${APP_PORT:-8000}:8000"
    volumes:
      - ./results:/app/results
      - ./temp:/app/temp
      - ./config.json:/app/config.json:ro
    networks:
      - xakaton_network
    restart: unless-stopped

volumes:
  postgres_data:

networks:
  xakaton_network:
    driver: bridge


-----

[Файл: c:\Hahatonn\XakatonAI\.dockerignore]
[Размер: 502 байт]
[Дата изменения: 2025-11-29 23:52:56.283272]

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
*.egg-info/
dist/
build/

# Virtual environments
venv/
env/
ENV/

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# Git
.git/
.gitignore

# Docker
Dockerfile
docker-compose.yml
.dockerignore

# Documentation
*.md
README*

# Results and temp files
results/*
!results/.gitkeep
temp/*
!temp/.gitkeep

# Logs
*.log

# OS
.DS_Store
Thumbs.db

# Config examples
config.example.json

# Database
*.db
*.sqlite

-----

[Файл: c:\Hahatonn\XakatonAI\.env.example]
[Размер: 186 байт]
[Дата изменения: 2025-11-30 01:40:36.612778]

﻿# PostgreSQL Database Configuration
POSTGRES_DB=xakaton
POSTGRES_USER=postgres
POSTGRES_PASSWORD=12345
POSTGRES_HOST=db
POSTGRES_PORT=5432

# Application Configuration
APP_PORT=8000

-----

